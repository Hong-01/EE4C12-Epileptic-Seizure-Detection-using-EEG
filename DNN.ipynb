{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EE4C12 Epileptic Seizur Detection using-EEG**\n",
    "\n",
    "## *DNN section*\n",
    "\n",
    "    \n",
    "Group 16 members:\n",
    "\n",
    "    1. Zhixuan Ge  \n",
    "    2. Yanqi Hong "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, precision_recall_curve, PrecisionRecallDisplay, roc_auc_score, RocCurveDisplay, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>annotation</th>\n",
       "      <th>min|FP1-F7</th>\n",
       "      <th>min|F7-T3</th>\n",
       "      <th>min|T3-T5</th>\n",
       "      <th>min|T5-O1</th>\n",
       "      <th>min|FP2-F8</th>\n",
       "      <th>min|F8-T4</th>\n",
       "      <th>min|T4-T6</th>\n",
       "      <th>min|T6-O2</th>\n",
       "      <th>...</th>\n",
       "      <th>norm_power_HF|CZ-C4</th>\n",
       "      <th>norm_power_HF|C4-T4</th>\n",
       "      <th>norm_power_HF|FP1-F3</th>\n",
       "      <th>norm_power_HF|F3-C3</th>\n",
       "      <th>norm_power_HF|C3-P3</th>\n",
       "      <th>norm_power_HF|P3-O1</th>\n",
       "      <th>norm_power_HF|FP2-F4</th>\n",
       "      <th>norm_power_HF|F4-C4</th>\n",
       "      <th>norm_power_HF|C4-P4</th>\n",
       "      <th>norm_power_HF|P4-O2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>61</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016087</td>\n",
       "      <td>0.066920</td>\n",
       "      <td>0.102402</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.690787</td>\n",
       "      <td>0.154544</td>\n",
       "      <td>0.062533</td>\n",
       "      <td>0.046460</td>\n",
       "      <td>0.066575</td>\n",
       "      <td>0.086999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>46</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024006</td>\n",
       "      <td>0.064857</td>\n",
       "      <td>0.031791</td>\n",
       "      <td>0.225788</td>\n",
       "      <td>0.409987</td>\n",
       "      <td>0.184671</td>\n",
       "      <td>0.071133</td>\n",
       "      <td>0.022369</td>\n",
       "      <td>0.079494</td>\n",
       "      <td>0.047536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>45</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037326</td>\n",
       "      <td>0.100177</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.622584</td>\n",
       "      <td>0.394504</td>\n",
       "      <td>0.225516</td>\n",
       "      <td>0.050673</td>\n",
       "      <td>0.044906</td>\n",
       "      <td>0.102142</td>\n",
       "      <td>0.068105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027546</td>\n",
       "      <td>0.107883</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.359140</td>\n",
       "      <td>0.276964</td>\n",
       "      <td>0.104977</td>\n",
       "      <td>0.018042</td>\n",
       "      <td>0.079467</td>\n",
       "      <td>0.078255</td>\n",
       "      <td>0.089385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036820</td>\n",
       "      <td>0.182520</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.328354</td>\n",
       "      <td>0.156929</td>\n",
       "      <td>0.151952</td>\n",
       "      <td>0.047532</td>\n",
       "      <td>0.135071</td>\n",
       "      <td>0.098320</td>\n",
       "      <td>0.137701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55451</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244334</td>\n",
       "      <td>0.625396</td>\n",
       "      <td>0.023821</td>\n",
       "      <td>0.058277</td>\n",
       "      <td>0.083594</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>0.119654</td>\n",
       "      <td>0.295364</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>0.199585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55452</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.743060</td>\n",
       "      <td>0.076294</td>\n",
       "      <td>0.332341</td>\n",
       "      <td>0.228458</td>\n",
       "      <td>0.170603</td>\n",
       "      <td>0.351418</td>\n",
       "      <td>0.638666</td>\n",
       "      <td>0.490806</td>\n",
       "      <td>0.307429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55453</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296041</td>\n",
       "      <td>0.770194</td>\n",
       "      <td>0.041190</td>\n",
       "      <td>0.090919</td>\n",
       "      <td>0.186074</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.231053</td>\n",
       "      <td>0.770637</td>\n",
       "      <td>0.285257</td>\n",
       "      <td>0.413382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55454</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>77</td>\n",
       "      <td>82</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440360</td>\n",
       "      <td>0.720855</td>\n",
       "      <td>0.026959</td>\n",
       "      <td>0.026340</td>\n",
       "      <td>0.077674</td>\n",
       "      <td>0.269610</td>\n",
       "      <td>0.186769</td>\n",
       "      <td>0.790173</td>\n",
       "      <td>0.473615</td>\n",
       "      <td>0.415771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55455</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "      <td>80</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019325</td>\n",
       "      <td>0.735140</td>\n",
       "      <td>0.030715</td>\n",
       "      <td>0.077191</td>\n",
       "      <td>0.095298</td>\n",
       "      <td>0.317765</td>\n",
       "      <td>0.271859</td>\n",
       "      <td>0.675646</td>\n",
       "      <td>0.506836</td>\n",
       "      <td>0.561740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55456 rows Ã— 362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Patient  annotation  min|FP1-F7  min|F7-T3  min|T3-T5  min|T5-O1  \\\n",
       "0          258           1          50         61         57         53   \n",
       "1          258           1          48         62         60         46   \n",
       "2          258           1          51         60         59         45   \n",
       "3          258           1          49         58         56         42   \n",
       "4          258           1          45         57         61         41   \n",
       "...        ...         ...         ...        ...        ...        ...   \n",
       "55451    11580          -1          75         73         81         80   \n",
       "55452    11580          -1          74         74         77         71   \n",
       "55453    11580          -1          72         76         72         73   \n",
       "55454    11580          -1          77         82         74         75   \n",
       "55455    11580          -1          71         79         74         78   \n",
       "\n",
       "       min|FP2-F8  min|F8-T4  min|T4-T6  min|T6-O2  ...  norm_power_HF|CZ-C4  \\\n",
       "0              39         35         39         35  ...             0.016087   \n",
       "1              38         35         39         33  ...             0.024006   \n",
       "2              38         36         40         36  ...             0.037326   \n",
       "3              36         36         41         37  ...             0.027546   \n",
       "4              35         37         41         37  ...             0.036820   \n",
       "...           ...        ...        ...        ...  ...                  ...   \n",
       "55451          66         80         77         75  ...             0.244334   \n",
       "55452          79         75         82         77  ...             0.588236   \n",
       "55453          74         76         80         76  ...             0.296041   \n",
       "55454          82         85         80         76  ...             0.440360   \n",
       "55455          80         85         81         75  ...             1.019325   \n",
       "\n",
       "       norm_power_HF|C4-T4  norm_power_HF|FP1-F3  norm_power_HF|F3-C3  \\\n",
       "0                 0.066920              0.102402             0.481384   \n",
       "1                 0.064857              0.031791             0.225788   \n",
       "2                 0.100177              0.050009             0.622584   \n",
       "3                 0.107883              0.014017             0.359140   \n",
       "4                 0.182520              0.031397             0.328354   \n",
       "...                    ...                   ...                  ...   \n",
       "55451             0.625396              0.023821             0.058277   \n",
       "55452             0.743060              0.076294             0.332341   \n",
       "55453             0.770194              0.041190             0.090919   \n",
       "55454             0.720855              0.026959             0.026340   \n",
       "55455             0.735140              0.030715             0.077191   \n",
       "\n",
       "       norm_power_HF|C3-P3  norm_power_HF|P3-O1  norm_power_HF|FP2-F4  \\\n",
       "0                 0.690787             0.154544              0.062533   \n",
       "1                 0.409987             0.184671              0.071133   \n",
       "2                 0.394504             0.225516              0.050673   \n",
       "3                 0.276964             0.104977              0.018042   \n",
       "4                 0.156929             0.151952              0.047532   \n",
       "...                    ...                  ...                   ...   \n",
       "55451             0.083594             0.114426              0.119654   \n",
       "55452             0.228458             0.170603              0.351418   \n",
       "55453             0.186074             0.216797              0.231053   \n",
       "55454             0.077674             0.269610              0.186769   \n",
       "55455             0.095298             0.317765              0.271859   \n",
       "\n",
       "       norm_power_HF|F4-C4  norm_power_HF|C4-P4  norm_power_HF|P4-O2  \n",
       "0                 0.046460             0.066575             0.086999  \n",
       "1                 0.022369             0.079494             0.047536  \n",
       "2                 0.044906             0.102142             0.068105  \n",
       "3                 0.079467             0.078255             0.089385  \n",
       "4                 0.135071             0.098320             0.137701  \n",
       "...                    ...                  ...                  ...  \n",
       "55451             0.295364             0.185930             0.199585  \n",
       "55452             0.638666             0.490806             0.307429  \n",
       "55453             0.770637             0.285257             0.413382  \n",
       "55454             0.790173             0.473615             0.415771  \n",
       "55455             0.675646             0.506836             0.561740  \n",
       "\n",
       "[55456 rows x 362 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data on Yanqi Hong's computer\n",
    "data = pd.read_csv('E:\\DATA\\TUD\\Master\\TUD_Master_Y1\\Q1\\EE4C12 Machine Learning For Electrical Engineering\\CodeLab\\Project\\S&S_SZD (1)\\Data\\Project_Data_EE4C12_S&S_SZD.csv')\n",
    "data\n",
    "\n",
    "# # load data on Zhixuan's computer\n",
    "# data = pd.read_csv('D:\\\\User\\Zhixuan Ge\\Onedrive TUDelft\\OneDrive - Delft University of Technology\\Courses\\ML for EE\\SZD\\S&S_SZD\\Project_Data_EE4C12_S&S_SZD.csv')\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split  \n",
    "\n",
    "X = data.iloc[:, 2:].values\n",
    "y = np.int32(data['annotation'].values)\n",
    "\n",
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original number of training feature is:  360\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current number of training feature after feature selection is:  120\n"
     ]
    }
   ],
   "source": [
    "# Feature selection\n",
    "\n",
    "print(\"The original number of training feature is: \", X_train_scaled.shape[1])\n",
    "clf_etc = ExtraTreesClassifier(random_state=random_state).fit(X_train_scaled, y_train) # fit the model\n",
    "feature_importances = clf_etc.feature_importances_  # get the feature importance\n",
    "\n",
    "plt.bar(range(len(feature_importances)), feature_importances)   # plot the feature importance\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Feature importance\")\n",
    "plt.title(\"Feature importance of all features\")\n",
    "plt.show()\n",
    "\n",
    "important_feature_indices=np.argsort(feature_importances)   # sort the feature importance  \n",
    "important_feature_indices_cut=important_feature_indices[:int(len(important_feature_indices)/1.5)]   # select the most important features  \n",
    "\n",
    "X_train_selected=np.delete(X_train_scaled,important_feature_indices_cut,1)    # delete the least important features\n",
    "X_test_selected=np.delete(X_test_scaled,important_feature_indices_cut,1)      # delete the least important features\n",
    "print(\"The current number of training feature after feature selection is: \", X_train_selected.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sample number of 0 is 1469\n",
      "The original sample number of 1 is 18177\n",
      "The original sample number of -1 is 21946\n",
      "\n",
      "The current sample number of 0 is 21946\n",
      "The current sample number of 1 is 21826\n",
      "The current sample number of -1 is 21826\n"
     ]
    }
   ],
   "source": [
    "# Oversampling and undersampling \n",
    "\n",
    "num_z = np.sum(y_train==0)\n",
    "num_p = np.sum(y_train==1)\n",
    "num_n = np.sum(y_train==-1)\n",
    "print('The original sample number of 0 is', num_z)\n",
    "print('The original sample number of 1 is', num_p)\n",
    "print('The original sample number of -1 is', num_n)\n",
    "\n",
    "# oversampling\n",
    "# sm = SMOTE(random_state=random_state)\n",
    "# X_train_os, y_train_os = sm.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# oversampling & undersampling\n",
    "smt = SMOTETomek(random_state=random_state)\n",
    "# sme = SMOTEENN(random_state=42)\n",
    "X_train_os, y_train_os = smt.fit_resample(X_train_selected ,y_train)\n",
    "\n",
    "num_z = np.sum(y_train_os==0)\n",
    "num_p = np.sum(y_train_os==1)\n",
    "num_n = np.sum(y_train_os==-1)\n",
    "\n",
    "print('')\n",
    "print('The current sample number of 0 is', num_z)\n",
    "print('The current sample number of 1 is', num_p)\n",
    "print('The current sample number of -1 is', num_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DNN model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_tensor(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        X = np.float32(X)\n",
    "        X = torch.from_numpy(X)\n",
    "        # In pytorch, labels start from 0\n",
    "        # shift required\n",
    "        y = np.longlong(y) - y.min()\n",
    "        y = torch.from_numpy(y)\n",
    "        \n",
    "        self.X = X.to(device)\n",
    "        self.y = y.to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN class for random search\n",
    "class DNN_rs(nn.Module):\n",
    "    # Available activation functions: ReLU, Sigmoid, Tanh, LeakyReLU, ELU, SELU, Softplus, Softsign, LogSigmoid, PReLU, Softmin, Softmax, if the input is not in the list, ReLU will be used\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activition_layer=nn.ReLU()):\n",
    "        super(DNN_rs, self).__init__()\n",
    "        depth=len(hidden_sizes)\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i])) \n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            if activition_layer==\"ReLU\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activition_layer==\"Sigmoid\":\n",
    "                layers.append(nn.Sigmoid())\n",
    "            elif activition_layer==\"Tanh\":\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activition_layer==\"LeakyReLU\":\n",
    "                layers.append(nn.LeakyReLU())\n",
    "            elif activition_layer==\"ELU\":\n",
    "                layers.append(nn.ELU())\n",
    "            elif activition_layer==\"SELU\":\n",
    "                layers.append(nn.SELU())\n",
    "            elif activition_layer==\"Softplus\":\n",
    "                layers.append(nn.Softplus())\n",
    "            elif activition_layer==\"Softsign\":\n",
    "                layers.append(nn.Softsign())\n",
    "            elif activition_layer==\"LogSigmoid\":\n",
    "                layers.append(nn.LogSigmoid())\n",
    "            elif activition_layer==\"PReLU\":\n",
    "                layers.append(nn.PReLU())\n",
    "            elif activition_layer==\"Softmin\":\n",
    "                layers.append(nn.Softmin())\n",
    "            elif activition_layer==\"Softmax\":\n",
    "                layers.append(nn.Softmax())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear( hidden_sizes[-1], output_size))\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN_rs(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=360, out_features=100, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test output for DNN_rs\n",
    "input_size = X_train_scaled.shape[1]\n",
    "output_size = 3\n",
    "model_rs = DNN_rs(input_size, hidden_sizes=[100], output_size=output_size, activition_layer=\"PReLU\")\n",
    "print(model_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test functions\n",
    "def train(dataloader, model, loss_fn, optimizer):  \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>5f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # CrossEntropyLoss for multi-classification\n",
    "optimizer_rs = torch.optim.Adam(model_rs.parameters(),weight_decay=0.005)   # Adam optimizer for random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k fold cross validation function\n",
    "def Kfold_split(X_train, y_train, Shuffle_state, k=5):   # Split the data into training and validation sets\n",
    "    #example : X_k_train, y_k_train, X_k_val, y_k_val = Kfold_split(X_train, y_train, Shuffle_state)\n",
    "    kf = KFold(n_splits=k, random_state=Shuffle_state, shuffle=True)    # 5-fold cross validation\n",
    "    kf.get_n_splits(X_train)    \n",
    "    X_k_train = []\n",
    "    y_k_train = []\n",
    "    X_k_val = []\n",
    "    y_k_val = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):  # Split the data into training and validation sets\n",
    "        X_k_train.append(X_train[train_index])\n",
    "        y_k_train.append(y_train[train_index])\n",
    "        X_k_val.append(X_train[val_index])\n",
    "        y_k_val.append(y_train[val_index])\n",
    "    \n",
    "    return X_k_train, y_k_train, X_k_val, y_k_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_eval(true, pred, score_display=True, matrix_display=False, result_return=False):\n",
    "    Accuracy = accuracy_score(true, pred)\n",
    "    F1 = accuracy_score(true, pred)\n",
    "    Precision = accuracy_score(true, pred)\n",
    "    Recall = accuracy_score(true, pred)\n",
    "    \n",
    "    if score_display==True:\n",
    "        print(\"Accuracy: \" + str(Accuracy))\n",
    "        print(\"F1 score: \" + str(F1))\n",
    "        print(\"Recall score: \" + str(Recall))\n",
    "        print(\"Precision score: \" + str(Precision))\n",
    "        \n",
    "    if matrix_display==True:\n",
    "        label = ['Non-seizure', 'Transition','Seizure']\n",
    "        cm = confusion_matrix(true, pred)\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=label).plot()\n",
    "        plt.show(cm_display)\n",
    "    \n",
    "    if result_return:\n",
    "        return Accuracy, F1, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DNN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross validation dataloader\n",
    "batch_size = 1024\n",
    "\n",
    "train_dataloader_list = []\n",
    "val_dataloader_list = []\n",
    "\n",
    "X_k_train_list, y_k_train_list, X_k_val_list, y_k_val_list = Kfold_split(X_train_os, y_train_os, random_state)   # K-fold cross validation for DNN\n",
    "for i in range(len(X_k_train_list)):\n",
    "    trainset_gpu = Data_tensor(X_k_train_list[i], y_k_train_list[i])\n",
    "    valset_gpu = Data_tensor(X_k_val_list[i], y_k_val_list[i])\n",
    "    train_dataloader_list.append(DataLoader(trainset_gpu, batch_size=batch_size, shuffle=True))\n",
    "    val_dataloader_list.append(DataLoader(valset_gpu, batch_size=batch_size, shuffle=True))\n",
    "    \n",
    "valset_gpu_k = Data_tensor(X_train_os, y_train_os)\n",
    "val_dataloader_k = DataLoader(valset_gpu_k, batch_size=batch_size, shuffle=True)\n",
    "testset_gpu = Data_tensor(X_test_selected, y_test)\n",
    "test_dataloader = DataLoader(testset_gpu, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_width=8\n",
    "max_width=640\n",
    "min_hl=1\n",
    "max_hl=3\n",
    "activition_list = [\"ReLU\", \"LogSigmoid\", \"LeakyReLU\",\"Sigmoid\"]\n",
    "\n",
    "optimizer_list = [\"SGD\", \"Adam\"]\n",
    "min_learning_rate=0.0001\n",
    "max_learning_rate=0.5\n",
    "\n",
    "def get_hps():\n",
    "    num_hl = random.randint(min_hl, max_hl)\n",
    "    hl = []\n",
    "    for i in range(num_hl):\n",
    "        hl.append(random.randint(min_width, max_width))\n",
    "    dacay = np.power(10, random.uniform(-4, 1))\n",
    "    activition = random.choice(activition_list)\n",
    "    optimizer = random.choice(optimizer_list)\n",
    "    lr = random.uniform(min_learning_rate, max_learning_rate)\n",
    "    \n",
    "    return {'hl': hl, 'dacay': dacay, 'activition': activition, 'optimizer': optimizer, 'lr': lr}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Search progress: 1/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [122, 33, 289], 'dacay': 0.001676715075046314, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.33838207376271334}\n",
      "loss: 1.126750  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.955976 \n",
      "\n",
      "loss: 0.947808  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.872535 \n",
      "\n",
      "loss: 0.892021  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.807015 \n",
      "\n",
      "loss: 0.806297  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.797323 \n",
      "\n",
      "loss: 0.830730  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.806274 \n",
      "\n",
      "loss: 0.817501  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.763335 \n",
      "\n",
      "loss: 0.752891  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.724004 \n",
      "\n",
      "loss: 0.767633  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.710515 \n",
      "\n",
      "loss: 0.693453  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.725812 \n",
      "\n",
      "loss: 0.686336  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.720782 \n",
      "\n",
      "loss: 0.711181  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.693380 \n",
      "\n",
      "loss: 0.690928  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.666143 \n",
      "\n",
      "loss: 0.675967  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.687232 \n",
      "\n",
      "loss: 0.687996  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.650190 \n",
      "\n",
      "loss: 0.627097  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.642506 \n",
      "\n",
      "loss: 0.661374  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.784322 \n",
      "\n",
      "loss: 0.778860  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.670038 \n",
      "\n",
      "loss: 0.670864  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.626201 \n",
      "\n",
      "loss: 0.613746  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.745360 \n",
      "\n",
      "loss: 0.759505  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.723199 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 2/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [97, 612, 440], 'dacay': 0.00014418278082106587, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.11640718060603072}\n",
      "loss: 1.107010  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.838372 \n",
      "\n",
      "loss: 0.818302  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.741322 \n",
      "\n",
      "loss: 0.746075  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.685137 \n",
      "\n",
      "loss: 0.670304  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.664967 \n",
      "\n",
      "loss: 0.660464  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.622784 \n",
      "\n",
      "loss: 0.632517  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.602834 \n",
      "\n",
      "loss: 0.631378  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.608244 \n",
      "\n",
      "loss: 0.629953  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.550436 \n",
      "\n",
      "loss: 0.590295  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.536547 \n",
      "\n",
      "loss: 0.528722  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.564196 \n",
      "\n",
      "loss: 0.551331  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.507455 \n",
      "\n",
      "loss: 0.523510  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.537799 \n",
      "\n",
      "loss: 0.539262  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.480097 \n",
      "\n",
      "loss: 0.480070  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.633599 \n",
      "\n",
      "loss: 0.661906  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.470411 \n",
      "\n",
      "loss: 0.468221  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.505961 \n",
      "\n",
      "loss: 0.495310  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.426899 \n",
      "\n",
      "loss: 0.466072  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.464387 \n",
      "\n",
      "loss: 0.457118  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.502954 \n",
      "\n",
      "loss: 0.501332  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.478402 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 3/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [35, 582, 211], 'dacay': 0.38027525358289865, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.22465960223729842}\n",
      "loss: 1.115015  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.378788 \n",
      "\n",
      "loss: 1.372497  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.451759 \n",
      "\n",
      "loss: 1.418728  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.140436 \n",
      "\n",
      "loss: 1.137197  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.416857 \n",
      "\n",
      "loss: 1.436333  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.358052 \n",
      "\n",
      "loss: 1.338104  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.173967 \n",
      "\n",
      "loss: 1.177794  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.441084 \n",
      "\n",
      "loss: 1.432790  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.399144 \n",
      "\n",
      "loss: 1.413096  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.327016 \n",
      "\n",
      "loss: 1.321049  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.311248 \n",
      "\n",
      "loss: 1.308372  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.288834 \n",
      "\n",
      "loss: 1.297445  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.259621 \n",
      "\n",
      "loss: 1.274981  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.363875 \n",
      "\n",
      "loss: 1.368789  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.289788 \n",
      "\n",
      "loss: 1.284407  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.471423 \n",
      "\n",
      "loss: 1.454261  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.389918 \n",
      "\n",
      "loss: 1.373350  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.466723 \n",
      "\n",
      "loss: 1.397555  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.472855 \n",
      "\n",
      "loss: 1.474199  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.442622 \n",
      "\n",
      "loss: 1.443044  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.205762 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 4/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [14, 171], 'dacay': 0.3095258856631221, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.0778242019559096}\n",
      "loss: 1.087596  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098761 \n",
      "\n",
      "loss: 1.098679  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098649 \n",
      "\n",
      "loss: 1.098582  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098515 \n",
      "\n",
      "loss: 1.098745  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098593 \n",
      "\n",
      "loss: 1.098234  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098577 \n",
      "\n",
      "loss: 1.098653  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098981 \n",
      "\n",
      "loss: 1.099222  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099389 \n",
      "\n",
      "loss: 1.097055  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098917 \n",
      "\n",
      "loss: 1.098071  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099031 \n",
      "\n",
      "loss: 1.100004  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098520 \n",
      "\n",
      "loss: 1.099072  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098626 \n",
      "\n",
      "loss: 1.098954  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098676 \n",
      "\n",
      "loss: 1.098349  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099198 \n",
      "\n",
      "loss: 1.099467  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099085 \n",
      "\n",
      "loss: 1.098492  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098889 \n",
      "\n",
      "loss: 1.098489  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099086 \n",
      "\n",
      "loss: 1.098316  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098552 \n",
      "\n",
      "loss: 1.098286  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098394 \n",
      "\n",
      "loss: 1.099496  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098791 \n",
      "\n",
      "loss: 1.099147  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.100638 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 5/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [112, 102], 'dacay': 0.007936636741682485, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.30190264308030884}\n",
      "loss: 1.091107  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 3.917942 \n",
      "\n",
      "loss: 3.583569  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.713044 \n",
      "\n",
      "loss: 0.721774  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.643564 \n",
      "\n",
      "loss: 0.607390  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.632543 \n",
      "\n",
      "loss: 0.630260  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.713518 \n",
      "\n",
      "loss: 0.663192  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 2.235453 \n",
      "\n",
      "loss: 2.262201  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 8.878006 \n",
      "\n",
      "loss: 8.831038  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.712243 \n",
      "\n",
      "loss: 0.731909  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.719545 \n",
      "\n",
      "loss: 0.687371  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.811376 \n",
      "\n",
      "loss: 0.764080  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.736117 \n",
      "\n",
      "loss: 0.766774  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 10.800547 \n",
      "\n",
      "loss: 9.830664  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.778503 \n",
      "\n",
      "loss: 0.787411  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.012981 \n",
      "\n",
      "loss: 0.959849  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.723363 \n",
      "\n",
      "loss: 0.726099  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 89.469996 \n",
      "\n",
      "loss: 100.736847  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 95.281622 \n",
      "\n",
      "loss: 95.819664  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.365296 \n",
      "\n",
      "loss: 1.365587  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.749875 \n",
      "\n",
      "loss: 0.699791  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.813911 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 6/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [478], 'dacay': 0.04798886266377097, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.2760651115734862}\n",
      "loss: 1.107824  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 5.998504 \n",
      "\n",
      "loss: 6.047175  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 3.391696 \n",
      "\n",
      "loss: 3.596830  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 47.8%, Avg loss: 5.013236 \n",
      "\n",
      "loss: 5.007102  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 5.016077 \n",
      "\n",
      "loss: 4.949148  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 1.722370 \n",
      "\n",
      "loss: 1.691735  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 3.405518 \n",
      "\n",
      "loss: 3.444300  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 4.024845 \n",
      "\n",
      "loss: 4.116325  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 4.708300 \n",
      "\n",
      "loss: 4.612718  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 4.490863 \n",
      "\n",
      "loss: 4.297170  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 5.478563 \n",
      "\n",
      "loss: 5.712961  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 5.741552 \n",
      "\n",
      "loss: 5.577416  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.2%, Avg loss: 5.531189 \n",
      "\n",
      "loss: 5.937146  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 5.537365 \n",
      "\n",
      "loss: 5.669883  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 3.430801 \n",
      "\n",
      "loss: 3.448976  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 4.533646 \n",
      "\n",
      "loss: 4.215879  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 6.579349 \n",
      "\n",
      "loss: 6.857915  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 6.008752 \n",
      "\n",
      "loss: 5.699079  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 37.3%, Avg loss: 5.592369 \n",
      "\n",
      "loss: 5.533166  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 4.256571 \n",
      "\n",
      "loss: 4.219122  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 2.429859 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 7/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [378, 599, 204], 'dacay': 0.3333183175887366, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.3865568635602671}\n",
      "loss: 1.101472  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098971 \n",
      "\n",
      "loss: 1.098486  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098683 \n",
      "\n",
      "loss: 1.098362  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098941 \n",
      "\n",
      "loss: 1.098874  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098667 \n",
      "\n",
      "loss: 1.098427  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098697 \n",
      "\n",
      "loss: 1.098967  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098620 \n",
      "\n",
      "loss: 1.098122  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098584 \n",
      "\n",
      "loss: 1.098778  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099246 \n",
      "\n",
      "loss: 1.099962  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098780 \n",
      "\n",
      "loss: 1.098432  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098564  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098697 \n",
      "\n",
      "loss: 1.098394  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098841 \n",
      "\n",
      "loss: 1.098950  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098547 \n",
      "\n",
      "loss: 1.098510  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098651 \n",
      "\n",
      "loss: 1.099008  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098583 \n",
      "\n",
      "loss: 1.098492  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098597 \n",
      "\n",
      "loss: 1.098584  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098752 \n",
      "\n",
      "loss: 1.098912  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098609 \n",
      "\n",
      "loss: 1.099053  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098912 \n",
      "\n",
      "loss: 1.099638  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098653 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 8/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [246], 'dacay': 2.149900366689929, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.2267598101539809}\n",
      "loss: 1.090345  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.146079 \n",
      "\n",
      "loss: 1.144331  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.222236 \n",
      "\n",
      "loss: 1.226078  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.118980 \n",
      "\n",
      "loss: 1.120853  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 2.454607 \n",
      "\n",
      "loss: 2.367615  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.452573 \n",
      "\n",
      "loss: 2.452492  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.885285 \n",
      "\n",
      "loss: 1.870413  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 2.086023 \n",
      "\n",
      "loss: 2.082987  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.799416 \n",
      "\n",
      "loss: 1.758698  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.383115 \n",
      "\n",
      "loss: 1.399715  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.422054 \n",
      "\n",
      "loss: 1.434179  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.326397 \n",
      "\n",
      "loss: 1.337115  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.660568 \n",
      "\n",
      "loss: 1.623963  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.231855 \n",
      "\n",
      "loss: 1.216066  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.234436 \n",
      "\n",
      "loss: 1.249287  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.968473 \n",
      "\n",
      "loss: 1.934667  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.148709 \n",
      "\n",
      "loss: 1.144932  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.597662 \n",
      "\n",
      "loss: 1.598018  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.147997 \n",
      "\n",
      "loss: 1.152311  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.385632 \n",
      "\n",
      "loss: 1.365101  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.668097 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 9/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [174, 387], 'dacay': 0.005975214511104342, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.30460458973292737}\n",
      "loss: 1.092652  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.722057 \n",
      "\n",
      "loss: 0.726872  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.624343 \n",
      "\n",
      "loss: 0.630362  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.610908 \n",
      "\n",
      "loss: 0.582153  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.577967 \n",
      "\n",
      "loss: 0.551340  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.695879 \n",
      "\n",
      "loss: 0.726224  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.554543 \n",
      "\n",
      "loss: 0.547312  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.536856 \n",
      "\n",
      "loss: 0.534701  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.543283 \n",
      "\n",
      "loss: 0.542346  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.514641 \n",
      "\n",
      "loss: 0.504057  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.646892 \n",
      "\n",
      "loss: 0.629938  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.534744 \n",
      "\n",
      "loss: 0.525153  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.496932 \n",
      "\n",
      "loss: 0.533707  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.502254 \n",
      "\n",
      "loss: 0.487804  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.541398 \n",
      "\n",
      "loss: 0.519284  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.501501 \n",
      "\n",
      "loss: 0.537216  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.502421 \n",
      "\n",
      "loss: 0.483051  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.464937 \n",
      "\n",
      "loss: 0.476784  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.600666 \n",
      "\n",
      "loss: 0.625194  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.694472 \n",
      "\n",
      "loss: 0.648105  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.478136 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 10/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [554], 'dacay': 0.4422155294069027, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.1897897753346481}\n",
      "loss: 1.122533  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 4.758866 \n",
      "\n",
      "loss: 4.843631  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 8.233434 \n",
      "\n",
      "loss: 7.792462  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 5.640706 \n",
      "\n",
      "loss: 5.656929  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 5.846677 \n",
      "\n",
      "loss: 5.796147  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 6.911502 \n",
      "\n",
      "loss: 6.841492  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.283866 \n",
      "\n",
      "loss: 8.429082  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 5.480700 \n",
      "\n",
      "loss: 5.459713  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.305666 \n",
      "\n",
      "loss: 5.375884  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 42.3%, Avg loss: 15.308303 \n",
      "\n",
      "loss: 15.080904  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 11.674005 \n",
      "\n",
      "loss: 11.298668  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 2.514474 \n",
      "\n",
      "loss: 2.632542  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.635930 \n",
      "\n",
      "loss: 5.671787  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 4.099196 \n",
      "\n",
      "loss: 3.708700  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.898609 \n",
      "\n",
      "loss: 1.821349  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 6.337154 \n",
      "\n",
      "loss: 6.500890  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.466453 \n",
      "\n",
      "loss: 5.553373  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 7.040251 \n",
      "\n",
      "loss: 7.126640  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 5.903072 \n",
      "\n",
      "loss: 5.632249  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 8.063104 \n",
      "\n",
      "loss: 8.390007  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 1.402807 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 11/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [578, 232, 340], 'dacay': 1.6377952263476234, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.4109191167440035}\n",
      "loss: 1.095547  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098757 \n",
      "\n",
      "loss: 1.098305  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098657 \n",
      "\n",
      "loss: 1.098639  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098592 \n",
      "\n",
      "loss: 1.098356  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098958 \n",
      "\n",
      "loss: 1.099482  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098553 \n",
      "\n",
      "loss: 1.098624  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098985 \n",
      "\n",
      "loss: 1.099024  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098717 \n",
      "\n",
      "loss: 1.099151  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098980 \n",
      "\n",
      "loss: 1.098715  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098752 \n",
      "\n",
      "loss: 1.098355  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098761 \n",
      "\n",
      "loss: 1.098912  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098615  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099039 \n",
      "\n",
      "loss: 1.098946  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099037 \n",
      "\n",
      "loss: 1.098912  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098777 \n",
      "\n",
      "loss: 1.099060  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098582 \n",
      "\n",
      "loss: 1.098584  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098778 \n",
      "\n",
      "loss: 1.098911  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098605 \n",
      "\n",
      "loss: 1.098582  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098523  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098654 \n",
      "\n",
      "loss: 1.099010  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098535 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 12/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [418, 282], 'dacay': 0.0002142609499419341, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.32775378878091055}\n",
      "loss: 1.119522  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.684453 \n",
      "\n",
      "loss: 0.679035  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.601521 \n",
      "\n",
      "loss: 0.600814  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.700268 \n",
      "\n",
      "loss: 0.668545  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.596166 \n",
      "\n",
      "loss: 0.604150  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.544796 \n",
      "\n",
      "loss: 0.535117  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.500140 \n",
      "\n",
      "loss: 0.473596  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.489022 \n",
      "\n",
      "loss: 0.503280  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.466571 \n",
      "\n",
      "loss: 0.473687  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.443896 \n",
      "\n",
      "loss: 0.406039  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.509682 \n",
      "\n",
      "loss: 0.509486  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.430938 \n",
      "\n",
      "loss: 0.416571  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.509183 \n",
      "\n",
      "loss: 0.520447  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.534443 \n",
      "\n",
      "loss: 0.533831  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.428964 \n",
      "\n",
      "loss: 0.391529  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.370410 \n",
      "\n",
      "loss: 0.372775  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.376764 \n",
      "\n",
      "loss: 0.376385  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.368320 \n",
      "\n",
      "loss: 0.388371  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.321310 \n",
      "\n",
      "loss: 0.329566  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.321940 \n",
      "\n",
      "loss: 0.333801  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.359169 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 13/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [477, 154], 'dacay': 0.0021105752085991748, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.3735322044449637}\n",
      "loss: 1.115838  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 1.294834 \n",
      "\n",
      "loss: 1.150245  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.101214 \n",
      "\n",
      "loss: 1.099600  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098966 \n",
      "\n",
      "loss: 1.099433  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.099819 \n",
      "\n",
      "loss: 1.100156  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.106413 \n",
      "\n",
      "loss: 1.112962  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 47.1%, Avg loss: 1.903748 \n",
      "\n",
      "loss: 2.101509  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 1.118655 \n",
      "\n",
      "loss: 1.119841  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.134004 \n",
      "\n",
      "loss: 1.117279  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.116039 \n",
      "\n",
      "loss: 1.110745  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 1.085367 \n",
      "\n",
      "loss: 1.072084  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 2.341791 \n",
      "\n",
      "loss: 2.127033  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.097561 \n",
      "\n",
      "loss: 1.097617  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.029738 \n",
      "\n",
      "loss: 1.034477  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.901616 \n",
      "\n",
      "loss: 0.868895  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 1.604946 \n",
      "\n",
      "loss: 1.410980  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.127513 \n",
      "\n",
      "loss: 1.123050  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 124.525592 \n",
      "\n",
      "loss: 128.544403  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 28.2%, Avg loss: 12.774881 \n",
      "\n",
      "loss: 13.176639  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 1.132536 \n",
      "\n",
      "loss: 1.118884  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.094407 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 14/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [605, 416], 'dacay': 0.00645627794386767, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.04554561514567957}\n",
      "loss: 1.140962  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 2.271399 \n",
      "\n",
      "loss: 2.392270  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.810889 \n",
      "\n",
      "loss: 0.787626  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 0.971309 \n",
      "\n",
      "loss: 0.995900  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.945946 \n",
      "\n",
      "loss: 0.969671  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.784554 \n",
      "\n",
      "loss: 0.749759  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 2.413992 \n",
      "\n",
      "loss: 2.421148  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.968599 \n",
      "\n",
      "loss: 0.979615  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 5.509639 \n",
      "\n",
      "loss: 5.099920  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.713568 \n",
      "\n",
      "loss: 1.700746  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.763143 \n",
      "\n",
      "loss: 0.726375  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.9%, Avg loss: 5.773087 \n",
      "\n",
      "loss: 5.753204  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.928416 \n",
      "\n",
      "loss: 0.952961  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.750767 \n",
      "\n",
      "loss: 0.749036  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.038954 \n",
      "\n",
      "loss: 0.934808  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.911200 \n",
      "\n",
      "loss: 0.901182  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.025311 \n",
      "\n",
      "loss: 0.990099  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.827442 \n",
      "\n",
      "loss: 0.794528  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.498463 \n",
      "\n",
      "loss: 1.477010  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 3.203588 \n",
      "\n",
      "loss: 3.425412  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 10.286187 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 15/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [120], 'dacay': 0.0005810407566972637, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.2982448744629137}\n",
      "loss: 1.230182  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 3.024458 \n",
      "\n",
      "loss: 3.143539  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.638107 \n",
      "\n",
      "loss: 0.629084  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.578909 \n",
      "\n",
      "loss: 0.570170  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.597938 \n",
      "\n",
      "loss: 0.585362  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.630602 \n",
      "\n",
      "loss: 0.590799  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.584669 \n",
      "\n",
      "loss: 0.515792  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.578343 \n",
      "\n",
      "loss: 0.546053  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.700753 \n",
      "\n",
      "loss: 0.719745  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.662234 \n",
      "\n",
      "loss: 0.657898  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.660129 \n",
      "\n",
      "loss: 0.666629  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.586720 \n",
      "\n",
      "loss: 0.572748  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.589584 \n",
      "\n",
      "loss: 0.639827  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.574851 \n",
      "\n",
      "loss: 0.587815  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.667673 \n",
      "\n",
      "loss: 0.680063  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.597333 \n",
      "\n",
      "loss: 0.569306  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.627576 \n",
      "\n",
      "loss: 0.640232  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 117.738266 \n",
      "\n",
      "loss: 102.526245  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 7.702581 \n",
      "\n",
      "loss: 6.780336  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 1.377395 \n",
      "\n",
      "loss: 1.511914  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.577341 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 16/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [398, 618], 'dacay': 9.563280653533093, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.3401736767872336}\n",
      "loss: 1.099006  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 17/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [557], 'dacay': 0.5680531833549259, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.1468207143108483}\n",
      "loss: 1.156191  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.025825 \n",
      "\n",
      "loss: 1.018367  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 1.025577 \n",
      "\n",
      "loss: 1.028204  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 1.030126 \n",
      "\n",
      "loss: 1.027250  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.029279 \n",
      "\n",
      "loss: 1.028581  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.027249 \n",
      "\n",
      "loss: 1.025474  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 1.029029 \n",
      "\n",
      "loss: 1.029171  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 1.026154 \n",
      "\n",
      "loss: 1.032365  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 1.027676 \n",
      "\n",
      "loss: 1.021374  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.031000 \n",
      "\n",
      "loss: 1.030609  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.029738 \n",
      "\n",
      "loss: 1.034200  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.029365 \n",
      "\n",
      "loss: 1.026569  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.028511 \n",
      "\n",
      "loss: 1.032992  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.030525 \n",
      "\n",
      "loss: 1.029282  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.028557 \n",
      "\n",
      "loss: 1.029636  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.027470 \n",
      "\n",
      "loss: 1.025673  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.025054 \n",
      "\n",
      "loss: 1.020533  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 1.026181 \n",
      "\n",
      "loss: 1.030384  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 1.031327 \n",
      "\n",
      "loss: 1.032660  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.027684 \n",
      "\n",
      "loss: 1.018102  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.026310 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 18/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [472], 'dacay': 0.00010380698573069099, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.2538900691347022}\n",
      "loss: 1.109643  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.696018 \n",
      "\n",
      "loss: 0.675874  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.654672 \n",
      "\n",
      "loss: 0.648248  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.588691 \n",
      "\n",
      "loss: 0.571408  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.605713 \n",
      "\n",
      "loss: 0.591431  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.547801 \n",
      "\n",
      "loss: 0.593362  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.547376 \n",
      "\n",
      "loss: 0.528922  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.517687 \n",
      "\n",
      "loss: 0.562869  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.497264 \n",
      "\n",
      "loss: 0.527721  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.523688 \n",
      "\n",
      "loss: 0.505357  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.532956 \n",
      "\n",
      "loss: 0.503366  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.462033 \n",
      "\n",
      "loss: 0.477298  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.447541 \n",
      "\n",
      "loss: 0.450428  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.502759 \n",
      "\n",
      "loss: 0.448924  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.444742 \n",
      "\n",
      "loss: 0.477572  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.418744 \n",
      "\n",
      "loss: 0.403822  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.423620 \n",
      "\n",
      "loss: 0.409458  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.408242 \n",
      "\n",
      "loss: 0.396419  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.481157 \n",
      "\n",
      "loss: 0.457428  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.635014 \n",
      "\n",
      "loss: 0.600510  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.379855 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 19/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [313], 'dacay': 1.6156479759161686, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.18701953491031487}\n",
      "loss: 1.245533  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 9.047596 \n",
      "\n",
      "loss: 9.117687  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 10.912397 \n",
      "\n",
      "loss: 11.156997  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 9.506195 \n",
      "\n",
      "loss: 9.304613  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 10.038286 \n",
      "\n",
      "loss: 10.201845  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 11.329049 \n",
      "\n",
      "loss: 11.876696  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 9.056938 \n",
      "\n",
      "loss: 8.992033  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 9.642132 \n",
      "\n",
      "loss: 9.879315  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 9.139273 \n",
      "\n",
      "loss: 9.331928  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 9.229841 \n",
      "\n",
      "loss: 9.278639  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 8.661476 \n",
      "\n",
      "loss: 9.096958  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 10.429873 \n",
      "\n",
      "loss: 10.138029  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 9.214258 \n",
      "\n",
      "loss: 8.979559  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 9.226255 \n",
      "\n",
      "loss: 9.023473  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 10.419993 \n",
      "\n",
      "loss: 9.759925  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 9.409471 \n",
      "\n",
      "loss: 9.164075  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 10.081137 \n",
      "\n",
      "loss: 9.983384  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 9.953282 \n",
      "\n",
      "loss: 9.807025  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 8.195971 \n",
      "\n",
      "loss: 8.664798  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 9.797695 \n",
      "\n",
      "loss: 9.890985  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 9.329099 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 20/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [560], 'dacay': 5.854620354066272, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.24435399344367104}\n",
      "loss: 1.096923  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.091810 \n",
      "\n",
      "loss: 1.090844  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098561 \n",
      "\n",
      "loss: 1.098578  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098597 \n",
      "\n",
      "loss: 1.098715  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098626 \n",
      "\n",
      "loss: 1.098495  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098639 \n",
      "\n",
      "loss: 1.098511  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098679 \n",
      "\n",
      "loss: 1.098978  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098717 \n",
      "\n",
      "loss: 1.099932  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098561 \n",
      "\n",
      "loss: 1.098621  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098610 \n",
      "\n",
      "loss: 1.098562  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098741 \n",
      "\n",
      "loss: 1.098848  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098637 \n",
      "\n",
      "loss: 1.098637  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098618 \n",
      "\n",
      "loss: 1.098154  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098720 \n",
      "\n",
      "loss: 1.098775  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098734 \n",
      "\n",
      "loss: 1.098661  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098798 \n",
      "\n",
      "loss: 1.098788  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098550 \n",
      "\n",
      "loss: 1.099179  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098791 \n",
      "\n",
      "loss: 1.098341  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099002 \n",
      "\n",
      "loss: 1.098609  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098699 \n",
      "\n",
      "loss: 1.098214  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098763 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 21/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [379], 'dacay': 2.4751948293228696, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.029056790730444357}\n",
      "loss: 1.072997  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.097559 \n",
      "\n",
      "loss: 1.097521  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098624 \n",
      "\n",
      "loss: 1.098637  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098640 \n",
      "\n",
      "loss: 1.098672  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098594  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098626 \n",
      "\n",
      "loss: 1.098608  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098620 \n",
      "\n",
      "loss: 1.098605  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098635 \n",
      "\n",
      "loss: 1.098703  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098622 \n",
      "\n",
      "loss: 1.098617  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098614 \n",
      "\n",
      "loss: 1.098625  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098669  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098597 \n",
      "\n",
      "loss: 1.098653  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098618 \n",
      "\n",
      "loss: 1.098622  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098586  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098647  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098625 \n",
      "\n",
      "loss: 1.098633  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098616 \n",
      "\n",
      "loss: 1.098609  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098620  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098625 \n",
      "\n",
      "loss: 1.098591  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098609 \n",
      "\n",
      "loss: 1.098538  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098619 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 22/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [88, 95, 505], 'dacay': 1.2025863417987745, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.32990943629140895}\n",
      "loss: 1.123840  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 12.936377 \n",
      "\n",
      "loss: 13.766363  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 5.548280 \n",
      "\n",
      "loss: 5.661925  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 5.303409 \n",
      "\n",
      "loss: 5.377035  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 4.570398 \n",
      "\n",
      "loss: 4.421873  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 6.599603 \n",
      "\n",
      "loss: 6.715687  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.079898 \n",
      "\n",
      "loss: 8.636158  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 7.080281 \n",
      "\n",
      "loss: 6.789638  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 5.622685 \n",
      "\n",
      "loss: 5.707966  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 6.650781 \n",
      "\n",
      "loss: 6.757397  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 3.841667 \n",
      "\n",
      "loss: 3.879336  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.190295 \n",
      "\n",
      "loss: 8.027354  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.831437 \n",
      "\n",
      "loss: 4.707305  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.987716 \n",
      "\n",
      "loss: 3.016099  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 6.166796 \n",
      "\n",
      "loss: 6.274536  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 4.455134 \n",
      "\n",
      "loss: 4.419390  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.251721 \n",
      "\n",
      "loss: 6.215949  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.663918 \n",
      "\n",
      "loss: 4.886827  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.757491 \n",
      "\n",
      "loss: 5.712070  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 5.505463 \n",
      "\n",
      "loss: 5.297558  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 10.768192 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 23/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [177, 279, 548], 'dacay': 2.3023174513792846, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.46446385515178823}\n",
      "loss: 1.112403  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 13.560634 \n",
      "\n",
      "loss: 15.112225  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 12.875078 \n",
      "\n",
      "loss: 13.153296  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 13.930373 \n",
      "\n",
      "loss: 13.425422  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 14.065128 \n",
      "\n",
      "loss: 13.808300  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 13.347904 \n",
      "\n",
      "loss: 13.254129  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 12.452172 \n",
      "\n",
      "loss: 13.128774  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 13.485382 \n",
      "\n",
      "loss: 13.177384  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 14.083040 \n",
      "\n",
      "loss: 14.527629  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 15.286314 \n",
      "\n",
      "loss: 15.968630  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 13.141294 \n",
      "\n",
      "loss: 13.034474  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 13.374322 \n",
      "\n",
      "loss: 13.663358  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 12.438094 \n",
      "\n",
      "loss: 13.181468  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 13.975110 \n",
      "\n",
      "loss: 13.753120  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 13.286187 \n",
      "\n",
      "loss: 13.816213  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 13.397240 \n",
      "\n",
      "loss: 12.233920  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 13.581204 \n",
      "\n",
      "loss: 13.158243  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 13.156254 \n",
      "\n",
      "loss: 13.125214  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 12.667933 \n",
      "\n",
      "loss: 13.009707  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 14.243160 \n",
      "\n",
      "loss: 14.271309  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 13.602359 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 24/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [213, 327, 416], 'dacay': 9.456856157668861, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.4498170809130084}\n",
      "loss: 1.105931  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 301.149046 \n",
      "\n",
      "loss: 281.593842  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098603 \n",
      "\n",
      "loss: 1.098616  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098586 \n",
      "\n",
      "loss: 1.098656  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098597 \n",
      "\n",
      "loss: 1.098639  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098650 \n",
      "\n",
      "loss: 1.098550  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098601 \n",
      "\n",
      "loss: 1.098624  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098632 \n",
      "\n",
      "loss: 1.098639  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098622 \n",
      "\n",
      "loss: 1.098847  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098623 \n",
      "\n",
      "loss: 1.098732  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098522 \n",
      "\n",
      "loss: 1.098426  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098600 \n",
      "\n",
      "loss: 1.098694  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098630 \n",
      "\n",
      "loss: 1.098418  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098700 \n",
      "\n",
      "loss: 1.098668  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098657 \n",
      "\n",
      "loss: 1.098202  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098664 \n",
      "\n",
      "loss: 1.098630  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098723 \n",
      "\n",
      "loss: 1.098816  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098783 \n",
      "\n",
      "loss: 1.098642  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098711 \n",
      "\n",
      "loss: 1.098612  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098630 \n",
      "\n",
      "loss: 1.098761  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098567 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 25/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [131, 261], 'dacay': 0.0013288828148723235, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.29419552835677093}\n",
      "loss: 1.113036  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.722539 \n",
      "\n",
      "loss: 0.749683  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.648891 \n",
      "\n",
      "loss: 0.644539  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.582637 \n",
      "\n",
      "loss: 0.548651  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.571331 \n",
      "\n",
      "loss: 0.564713  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.543491 \n",
      "\n",
      "loss: 0.540504  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.525969 \n",
      "\n",
      "loss: 0.511791  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.502748 \n",
      "\n",
      "loss: 0.525241  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.511328 \n",
      "\n",
      "loss: 0.451367  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.562084 \n",
      "\n",
      "loss: 0.551597  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.496570 \n",
      "\n",
      "loss: 0.502963  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.477599 \n",
      "\n",
      "loss: 0.476911  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.456237 \n",
      "\n",
      "loss: 0.487772  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.499788 \n",
      "\n",
      "loss: 0.521410  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.509303 \n",
      "\n",
      "loss: 0.485972  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.444079 \n",
      "\n",
      "loss: 0.444137  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.560907 \n",
      "\n",
      "loss: 0.558950  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.441836 \n",
      "\n",
      "loss: 0.477781  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.410580 \n",
      "\n",
      "loss: 0.431160  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.406235 \n",
      "\n",
      "loss: 0.409784  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.444439 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 26/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [610], 'dacay': 0.0012620801094670535, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.11454799772719608}\n",
      "loss: 1.110016  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.743757 \n",
      "\n",
      "loss: 0.749763  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.700539 \n",
      "\n",
      "loss: 0.708271  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.655657 \n",
      "\n",
      "loss: 0.637181  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.634652 \n",
      "\n",
      "loss: 0.650648  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.617417 \n",
      "\n",
      "loss: 0.604276  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.602603 \n",
      "\n",
      "loss: 0.560856  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.574468 \n",
      "\n",
      "loss: 0.585030  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.567091 \n",
      "\n",
      "loss: 0.551719  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.561930 \n",
      "\n",
      "loss: 0.562009  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.549881 \n",
      "\n",
      "loss: 0.564745  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.540869 \n",
      "\n",
      "loss: 0.525428  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.532170 \n",
      "\n",
      "loss: 0.534395  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.507659 \n",
      "\n",
      "loss: 0.523535  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.511623 \n",
      "\n",
      "loss: 0.529778  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.501553 \n",
      "\n",
      "loss: 0.501469  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.491907 \n",
      "\n",
      "loss: 0.490565  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.471050 \n",
      "\n",
      "loss: 0.470246  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.484365 \n",
      "\n",
      "loss: 0.504832  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.472099 \n",
      "\n",
      "loss: 0.489909  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.467228 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 27/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [346], 'dacay': 0.00022609280891738634, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.33452199137031063}\n",
      "loss: 1.138890  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 7.773348 \n",
      "\n",
      "loss: 8.462577  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.832792 \n",
      "\n",
      "loss: 0.902888  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.533374 \n",
      "\n",
      "loss: 0.525224  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.545136 \n",
      "\n",
      "loss: 0.539167  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.611839 \n",
      "\n",
      "loss: 0.574836  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.527873 \n",
      "\n",
      "loss: 0.502975  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.532848 \n",
      "\n",
      "loss: 0.547227  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.511706 \n",
      "\n",
      "loss: 0.496068  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.581173 \n",
      "\n",
      "loss: 0.555985  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.537165 \n",
      "\n",
      "loss: 0.595875  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.647021 \n",
      "\n",
      "loss: 0.594992  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.568293 \n",
      "\n",
      "loss: 0.574692  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.614722 \n",
      "\n",
      "loss: 0.602671  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.607259 \n",
      "\n",
      "loss: 0.611881  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.540805 \n",
      "\n",
      "loss: 0.519269  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.565131 \n",
      "\n",
      "loss: 0.571704  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.624193 \n",
      "\n",
      "loss: 0.661863  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 1186.873164 \n",
      "\n",
      "loss: 1087.922729  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 67.912877 \n",
      "\n",
      "loss: 52.330597  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 5.074440 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 28/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [560], 'dacay': 0.00045873221879225867, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.3923312502029476}\n",
      "loss: 1.125672  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 2.654426 \n",
      "\n",
      "loss: 2.674577  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 2.937902 \n",
      "\n",
      "loss: 2.868404  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 2.895859 \n",
      "\n",
      "loss: 2.738203  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 3.759402 \n",
      "\n",
      "loss: 3.715351  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 2.393507 \n",
      "\n",
      "loss: 2.712457  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 1.239785 \n",
      "\n",
      "loss: 1.191317  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 1.107088 \n",
      "\n",
      "loss: 1.097853  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 2.369276 \n",
      "\n",
      "loss: 2.227961  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 1.066421 \n",
      "\n",
      "loss: 0.931667  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 1.007923 \n",
      "\n",
      "loss: 0.916370  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.847336 \n",
      "\n",
      "loss: 0.902141  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.020407 \n",
      "\n",
      "loss: 0.968763  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 1.562764 \n",
      "\n",
      "loss: 1.459711  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.568992 \n",
      "\n",
      "loss: 1.581215  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.733640 \n",
      "\n",
      "loss: 0.744070  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.907146 \n",
      "\n",
      "loss: 0.951474  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.360639 \n",
      "\n",
      "loss: 1.388776  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.911296 \n",
      "\n",
      "loss: 0.914985  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.732676 \n",
      "\n",
      "loss: 0.742353  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.739822 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 29/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [202, 104], 'dacay': 0.0003052488743843354, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.21184695364765838}\n",
      "loss: 1.140714  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.917194 \n",
      "\n",
      "loss: 0.937622  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.820119 \n",
      "\n",
      "loss: 0.811042  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.829231 \n",
      "\n",
      "loss: 0.808941  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.857432 \n",
      "\n",
      "loss: 0.839633  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.779151 \n",
      "\n",
      "loss: 0.783135  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.879058 \n",
      "\n",
      "loss: 0.872876  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.891847 \n",
      "\n",
      "loss: 0.938219  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.977376 \n",
      "\n",
      "loss: 0.964367  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.817261 \n",
      "\n",
      "loss: 0.791892  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.860396 \n",
      "\n",
      "loss: 0.879604  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.808319 \n",
      "\n",
      "loss: 0.824141  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.860298 \n",
      "\n",
      "loss: 0.925738  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.812852 \n",
      "\n",
      "loss: 0.831821  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.917154 \n",
      "\n",
      "loss: 0.904698  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.834525 \n",
      "\n",
      "loss: 0.823686  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.893597 \n",
      "\n",
      "loss: 0.913741  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 1.411737 \n",
      "\n",
      "loss: 1.440262  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 1.077866 \n",
      "\n",
      "loss: 1.094558  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 0.905086 \n",
      "\n",
      "loss: 0.895447  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.811631 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 30/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [63, 108], 'dacay': 0.00020093859921901044, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.12440330132675079}\n",
      "loss: 1.091023  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.818912 \n",
      "\n",
      "loss: 0.803551  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.733112 \n",
      "\n",
      "loss: 0.761611  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.688222 \n",
      "\n",
      "loss: 0.659988  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.662155 \n",
      "\n",
      "loss: 0.626158  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.634356 \n",
      "\n",
      "loss: 0.661834  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.618210 \n",
      "\n",
      "loss: 0.626669  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.602561 \n",
      "\n",
      "loss: 0.625193  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.571212 \n",
      "\n",
      "loss: 0.588359  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.564877 \n",
      "\n",
      "loss: 0.501676  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.549188 \n",
      "\n",
      "loss: 0.549680  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.593606 \n",
      "\n",
      "loss: 0.640195  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.545243 \n",
      "\n",
      "loss: 0.524632  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.528185 \n",
      "\n",
      "loss: 0.547956  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.566273 \n",
      "\n",
      "loss: 0.578847  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.494205 \n",
      "\n",
      "loss: 0.470590  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.554295 \n",
      "\n",
      "loss: 0.514997  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.475408 \n",
      "\n",
      "loss: 0.519477  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.463185 \n",
      "\n",
      "loss: 0.425585  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.466696 \n",
      "\n",
      "loss: 0.471356  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.501488 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 31/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [557], 'dacay': 0.01750119642586209, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.13934471781900354}\n",
      "loss: 1.111288  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 3.230075 \n",
      "\n",
      "loss: 3.161765  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.9%, Avg loss: 1.518909 \n",
      "\n",
      "loss: 1.551158  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.026157 \n",
      "\n",
      "loss: 0.980410  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 3.344181 \n",
      "\n",
      "loss: 3.454922  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 1.162270 \n",
      "\n",
      "loss: 1.153728  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.502269 \n",
      "\n",
      "loss: 1.477766  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.941186 \n",
      "\n",
      "loss: 0.979245  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 44.7%, Avg loss: 2.036856 \n",
      "\n",
      "loss: 1.997048  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 3.372827 \n",
      "\n",
      "loss: 3.550150  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.749637 \n",
      "\n",
      "loss: 1.780631  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 1.628546 \n",
      "\n",
      "loss: 1.529429  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.3%, Avg loss: 1.751787 \n",
      "\n",
      "loss: 1.680112  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 2.500450 \n",
      "\n",
      "loss: 2.481035  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 1.955027 \n",
      "\n",
      "loss: 1.825028  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.051550 \n",
      "\n",
      "loss: 0.997478  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.504720 \n",
      "\n",
      "loss: 1.541507  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 1.943049 \n",
      "\n",
      "loss: 1.894201  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 1.126572 \n",
      "\n",
      "loss: 1.168311  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 1.159235 \n",
      "\n",
      "loss: 1.202840  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.701643 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 32/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [85], 'dacay': 0.016430611437297585, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.3261072325098612}\n",
      "loss: 1.128461  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.723212 \n",
      "\n",
      "loss: 0.681893  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.664752 \n",
      "\n",
      "loss: 0.687074  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.645697 \n",
      "\n",
      "loss: 0.612436  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.656992 \n",
      "\n",
      "loss: 0.647923  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.646747 \n",
      "\n",
      "loss: 0.661111  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.635099 \n",
      "\n",
      "loss: 0.612663  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.611949 \n",
      "\n",
      "loss: 0.596397  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.629674 \n",
      "\n",
      "loss: 0.616061  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.637264 \n",
      "\n",
      "loss: 0.620368  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.623897 \n",
      "\n",
      "loss: 0.612845  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.621373 \n",
      "\n",
      "loss: 0.626601  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.596473 \n",
      "\n",
      "loss: 0.599269  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.619686 \n",
      "\n",
      "loss: 0.613420  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.615429 \n",
      "\n",
      "loss: 0.618603  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.617713 \n",
      "\n",
      "loss: 0.588891  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.617708 \n",
      "\n",
      "loss: 0.641199  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.608521 \n",
      "\n",
      "loss: 0.644565  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.601143 \n",
      "\n",
      "loss: 0.575098  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.607722 \n",
      "\n",
      "loss: 0.621001  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.609856 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 33/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [23, 103, 250], 'dacay': 0.0006785089067906867, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.10695227486967092}\n",
      "loss: 1.117814  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.748690 \n",
      "\n",
      "loss: 0.748094  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.722072 \n",
      "\n",
      "loss: 0.696104  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.737738 \n",
      "\n",
      "loss: 0.745609  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 1.019329 \n",
      "\n",
      "loss: 1.002939  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.743583 \n",
      "\n",
      "loss: 0.747744  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.716012 \n",
      "\n",
      "loss: 0.698188  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.684192 \n",
      "\n",
      "loss: 0.670957  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.710978 \n",
      "\n",
      "loss: 0.731348  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.693631 \n",
      "\n",
      "loss: 0.680613  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.764592 \n",
      "\n",
      "loss: 0.728135  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.765453 \n",
      "\n",
      "loss: 0.745557  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.796628 \n",
      "\n",
      "loss: 0.833594  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.775396 \n",
      "\n",
      "loss: 0.776983  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.710087 \n",
      "\n",
      "loss: 0.712671  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.716384 \n",
      "\n",
      "loss: 0.666140  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.676516 \n",
      "\n",
      "loss: 0.665589  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.852009 \n",
      "\n",
      "loss: 0.845230  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.709398 \n",
      "\n",
      "loss: 0.695341  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.748650 \n",
      "\n",
      "loss: 0.740766  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.712881 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 34/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [68, 176], 'dacay': 0.007849926581021646, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.46326638497231076}\n",
      "loss: 1.141452  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 2.133411 \n",
      "\n",
      "loss: 2.135948  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.819830 \n",
      "\n",
      "loss: 4.724398  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 2.289446 \n",
      "\n",
      "loss: 2.439834  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 1.611738 \n",
      "\n",
      "loss: 1.642830  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 1.069702 \n",
      "\n",
      "loss: 1.074794  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.838712 \n",
      "\n",
      "loss: 4.927248  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.162023 \n",
      "\n",
      "loss: 3.964283  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 4.705056 \n",
      "\n",
      "loss: 4.902690  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 5.656460 \n",
      "\n",
      "loss: 5.662184  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 4.396442 \n",
      "\n",
      "loss: 4.415159  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.812598 \n",
      "\n",
      "loss: 3.894589  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 1.920649 \n",
      "\n",
      "loss: 1.944039  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.354440 \n",
      "\n",
      "loss: 2.401022  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 36.0%, Avg loss: 1.796899 \n",
      "\n",
      "loss: 1.795718  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.664941 \n",
      "\n",
      "loss: 2.725859  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 11.037868 \n",
      "\n",
      "loss: 11.167640  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.634694 \n",
      "\n",
      "loss: 3.598695  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 4.360133 \n",
      "\n",
      "loss: 4.363033  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 3.173924 \n",
      "\n",
      "loss: 2.988604  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 3.544401 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 35/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [300, 441], 'dacay': 0.3040587585661391, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.0950299438470918}\n",
      "loss: 1.147497  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.434595 \n",
      "\n",
      "loss: 1.474834  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.132083 \n",
      "\n",
      "loss: 1.148453  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.321221 \n",
      "\n",
      "loss: 1.346425  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.428699 \n",
      "\n",
      "loss: 1.402985  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.333313 \n",
      "\n",
      "loss: 1.332182  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.356427 \n",
      "\n",
      "loss: 1.368766  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.349875 \n",
      "\n",
      "loss: 1.383193  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.346879 \n",
      "\n",
      "loss: 1.311573  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.375030 \n",
      "\n",
      "loss: 1.416628  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.412664 \n",
      "\n",
      "loss: 1.432096  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.509619 \n",
      "\n",
      "loss: 1.504668  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.500825 \n",
      "\n",
      "loss: 1.533127  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.415414 \n",
      "\n",
      "loss: 1.411631  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.907937 \n",
      "\n",
      "loss: 1.869683  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.306040 \n",
      "\n",
      "loss: 1.349419  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.419301 \n",
      "\n",
      "loss: 1.420718  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.716155 \n",
      "\n",
      "loss: 1.788104  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.666270 \n",
      "\n",
      "loss: 1.700634  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.749227 \n",
      "\n",
      "loss: 1.739851  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.442213 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 36/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [67], 'dacay': 0.07868672211092306, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.02867691992645079}\n",
      "loss: 1.157172  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.822385 \n",
      "\n",
      "loss: 0.828580  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.774046 \n",
      "\n",
      "loss: 0.746131  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.786385 \n",
      "\n",
      "loss: 0.774725  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.798184 \n",
      "\n",
      "loss: 0.804848  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.794095 \n",
      "\n",
      "loss: 0.792982  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.778331 \n",
      "\n",
      "loss: 0.789692  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.777689 \n",
      "\n",
      "loss: 0.784624  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.778455 \n",
      "\n",
      "loss: 0.752077  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.796293 \n",
      "\n",
      "loss: 0.812270  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.783454 \n",
      "\n",
      "loss: 0.782456  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.788534 \n",
      "\n",
      "loss: 0.785643  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.771640 \n",
      "\n",
      "loss: 0.770147  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.783867 \n",
      "\n",
      "loss: 0.788312  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.776360 \n",
      "\n",
      "loss: 0.796024  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.789222 \n",
      "\n",
      "loss: 0.779110  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.779377 \n",
      "\n",
      "loss: 0.763631  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.775942 \n",
      "\n",
      "loss: 0.776929  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.776234 \n",
      "\n",
      "loss: 0.770057  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.793454 \n",
      "\n",
      "loss: 0.789425  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.775233 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 37/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [496, 522, 551], 'dacay': 0.0006125811653534254, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.0343538142186258}\n",
      "loss: 1.099015  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.994044 \n",
      "\n",
      "loss: 0.996464  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.881329 \n",
      "\n",
      "loss: 0.881130  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.827641 \n",
      "\n",
      "loss: 0.795494  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.800704 \n",
      "\n",
      "loss: 0.772093  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.764718 \n",
      "\n",
      "loss: 0.765314  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.738779 \n",
      "\n",
      "loss: 0.721987  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.705425 \n",
      "\n",
      "loss: 0.686714  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.687021 \n",
      "\n",
      "loss: 0.689514  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.679195 \n",
      "\n",
      "loss: 0.701432  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.661702 \n",
      "\n",
      "loss: 0.668640  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.645361 \n",
      "\n",
      "loss: 0.646481  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.620693 \n",
      "\n",
      "loss: 0.630510  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.613801 \n",
      "\n",
      "loss: 0.616097  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.609349 \n",
      "\n",
      "loss: 0.579513  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.596639 \n",
      "\n",
      "loss: 0.572791  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.586611 \n",
      "\n",
      "loss: 0.600296  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.567742 \n",
      "\n",
      "loss: 0.572601  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.560239 \n",
      "\n",
      "loss: 0.540660  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.558501 \n",
      "\n",
      "loss: 0.592633  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.551374 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 38/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [248], 'dacay': 0.010444261970906834, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.3097288170150183}\n",
      "loss: 1.167225  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 4.185098 \n",
      "\n",
      "loss: 3.861753  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 4.656314 \n",
      "\n",
      "loss: 4.608243  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.719483 \n",
      "\n",
      "loss: 1.716406  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.939535 \n",
      "\n",
      "loss: 1.017684  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.772317 \n",
      "\n",
      "loss: 1.671171  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 3.441178 \n",
      "\n",
      "loss: 3.262540  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 1.578132 \n",
      "\n",
      "loss: 1.616327  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.144566 \n",
      "\n",
      "loss: 1.164975  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 2.351863 \n",
      "\n",
      "loss: 2.290675  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.982419 \n",
      "\n",
      "loss: 0.982136  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 47.3%, Avg loss: 1.601471 \n",
      "\n",
      "loss: 1.605084  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.911322 \n",
      "\n",
      "loss: 1.797599  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.690071 \n",
      "\n",
      "loss: 1.648176  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 1.607895 \n",
      "\n",
      "loss: 1.578279  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 1.493877 \n",
      "\n",
      "loss: 1.462847  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 2.670163 \n",
      "\n",
      "loss: 2.792869  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 1.061626 \n",
      "\n",
      "loss: 1.071329  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 1.003048 \n",
      "\n",
      "loss: 1.088075  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 2.545480 \n",
      "\n",
      "loss: 2.504467  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 1.025149 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 39/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [605, 586], 'dacay': 0.041106791259118765, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.33489601478743763}\n",
      "loss: 1.106530  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.745687 \n",
      "\n",
      "loss: 0.714702  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.703987 \n",
      "\n",
      "loss: 0.752920  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.703090 \n",
      "\n",
      "loss: 0.690929  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.702539 \n",
      "\n",
      "loss: 0.689420  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.704184 \n",
      "\n",
      "loss: 0.698632  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.692427 \n",
      "\n",
      "loss: 0.683868  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.680805 \n",
      "\n",
      "loss: 0.721773  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.691937 \n",
      "\n",
      "loss: 0.679469  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.697710 \n",
      "\n",
      "loss: 0.712935  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.692568 \n",
      "\n",
      "loss: 0.677375  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.686734 \n",
      "\n",
      "loss: 0.723946  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.688272 \n",
      "\n",
      "loss: 0.688563  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.689776 \n",
      "\n",
      "loss: 0.708174  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.694256 \n",
      "\n",
      "loss: 0.700531  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.692399 \n",
      "\n",
      "loss: 0.664522  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.692105 \n",
      "\n",
      "loss: 0.699761  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.677380 \n",
      "\n",
      "loss: 0.691500  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.685899 \n",
      "\n",
      "loss: 0.693764  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.694049 \n",
      "\n",
      "loss: 0.675237  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.697551 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 40/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [252, 279], 'dacay': 0.009526409234558195, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.15815698041629983}\n",
      "loss: 1.085600  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 2.633498 \n",
      "\n",
      "loss: 2.771406  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.770060 \n",
      "\n",
      "loss: 0.795685  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.589289 \n",
      "\n",
      "loss: 0.599328  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.604003 \n",
      "\n",
      "loss: 0.611469  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.610719 \n",
      "\n",
      "loss: 0.617473  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.745720 \n",
      "\n",
      "loss: 0.773400  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.783518 \n",
      "\n",
      "loss: 0.837916  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 6.349280 \n",
      "\n",
      "loss: 5.801569  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 9.747839 \n",
      "\n",
      "loss: 9.513369  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 3.498403 \n",
      "\n",
      "loss: 3.426203  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 14.883895 \n",
      "\n",
      "loss: 14.780265  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 45.271880 \n",
      "\n",
      "loss: 45.643742  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 3.340384 \n",
      "\n",
      "loss: 3.174405  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.698791 \n",
      "\n",
      "loss: 0.685665  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.649327 \n",
      "\n",
      "loss: 0.623564  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.667396 \n",
      "\n",
      "loss: 0.672003  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.773652 \n",
      "\n",
      "loss: 0.777322  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.667763 \n",
      "\n",
      "loss: 0.634104  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.711580 \n",
      "\n",
      "loss: 0.659727  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 21.122929 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 41/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [17], 'dacay': 0.01956264676406525, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.2688634386104035}\n",
      "loss: 1.076354  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.761182 \n",
      "\n",
      "loss: 0.767758  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.707624 \n",
      "\n",
      "loss: 0.730619  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.686711 \n",
      "\n",
      "loss: 0.707104  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.679423 \n",
      "\n",
      "loss: 0.681299  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.678645 \n",
      "\n",
      "loss: 0.668306  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.672192 \n",
      "\n",
      "loss: 0.670137  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.648596 \n",
      "\n",
      "loss: 0.677964  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.651926 \n",
      "\n",
      "loss: 0.643953  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.669419 \n",
      "\n",
      "loss: 0.647147  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.650665 \n",
      "\n",
      "loss: 0.662601  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.648284 \n",
      "\n",
      "loss: 0.642012  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.649489 \n",
      "\n",
      "loss: 0.683964  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.638817 \n",
      "\n",
      "loss: 0.652160  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.653221 \n",
      "\n",
      "loss: 0.620086  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.642553 \n",
      "\n",
      "loss: 0.628792  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.649432 \n",
      "\n",
      "loss: 0.619275  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.629036 \n",
      "\n",
      "loss: 0.618618  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.640587 \n",
      "\n",
      "loss: 0.648631  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.641181 \n",
      "\n",
      "loss: 0.649343  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.639364 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 42/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [279, 143, 365], 'dacay': 2.537002343735568, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.14256519686920668}\n",
      "loss: 1.152656  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.807878 \n",
      "\n",
      "loss: 2.885854  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.163125 \n",
      "\n",
      "loss: 1.154422  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.433233 \n",
      "\n",
      "loss: 1.454435  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.146730 \n",
      "\n",
      "loss: 2.135525  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 31.5%, Avg loss: 152.714914 \n",
      "\n",
      "loss: 155.179459  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 562.299936 \n",
      "\n",
      "loss: 600.969055  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 2.592078 \n",
      "\n",
      "loss: 2.753068  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 1.328290 \n",
      "\n",
      "loss: 1.317043  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 46.1%, Avg loss: 1.154017 \n",
      "\n",
      "loss: 1.146336  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 43.9%, Avg loss: 1.049160 \n",
      "\n",
      "loss: 1.036976  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.074301 \n",
      "\n",
      "loss: 1.065441  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 1.062089 \n",
      "\n",
      "loss: 1.063131  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.9%, Avg loss: 1.102362 \n",
      "\n",
      "loss: 1.113644  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.109290 \n",
      "\n",
      "loss: 1.108006  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098753 \n",
      "\n",
      "loss: 1.097568  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.101664 \n",
      "\n",
      "loss: 1.104356  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.104140 \n",
      "\n",
      "loss: 1.102599  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.101157 \n",
      "\n",
      "loss: 1.100918  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099341 \n",
      "\n",
      "loss: 1.098951  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.099132 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 43/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [564, 317], 'dacay': 0.11439020245340721, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.4658848441140951}\n",
      "loss: 1.122823  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 338.714632 \n",
      "\n",
      "loss: 418.655731  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 989.187650 \n",
      "\n",
      "loss: 1078.595581  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 47.3%, Avg loss: 39.790300 \n",
      "\n",
      "loss: 38.052525  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 112.691380 \n",
      "\n",
      "loss: 103.648354  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 855.370028 \n",
      "\n",
      "loss: 837.317078  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 46.046419 \n",
      "\n",
      "loss: 40.642460  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 7.598361 \n",
      "\n",
      "loss: 8.501846  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 54.622366 \n",
      "\n",
      "loss: 45.987110  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 3.855999 \n",
      "\n",
      "loss: 3.732421  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.826524 \n",
      "\n",
      "loss: 0.831163  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 1.141762 \n",
      "\n",
      "loss: 1.164751  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.806910 \n",
      "\n",
      "loss: 0.814359  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.755755 \n",
      "\n",
      "loss: 0.785707  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.824894 \n",
      "\n",
      "loss: 0.808309  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.881014 \n",
      "\n",
      "loss: 0.871075  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.735540 \n",
      "\n",
      "loss: 0.733824  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.777715 \n",
      "\n",
      "loss: 0.800786  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.758036 \n",
      "\n",
      "loss: 0.711481  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.756300 \n",
      "\n",
      "loss: 0.759270  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.810893 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 44/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [145], 'dacay': 0.002100533476953222, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.13624687133617666}\n",
      "loss: 1.060276  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.756743 \n",
      "\n",
      "loss: 0.783000  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.695846 \n",
      "\n",
      "loss: 0.706718  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.665788 \n",
      "\n",
      "loss: 0.640771  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.647023 \n",
      "\n",
      "loss: 0.655876  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.625567 \n",
      "\n",
      "loss: 0.634061  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.609693 \n",
      "\n",
      "loss: 0.563413  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.587036 \n",
      "\n",
      "loss: 0.579432  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.575395 \n",
      "\n",
      "loss: 0.556075  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.570625 \n",
      "\n",
      "loss: 0.568274  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.565587 \n",
      "\n",
      "loss: 0.548005  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.553377 \n",
      "\n",
      "loss: 0.521845  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.526420 \n",
      "\n",
      "loss: 0.499371  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.524065 \n",
      "\n",
      "loss: 0.511310  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.527851 \n",
      "\n",
      "loss: 0.520001  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.518660 \n",
      "\n",
      "loss: 0.494694  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.514094 \n",
      "\n",
      "loss: 0.529594  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.494077 \n",
      "\n",
      "loss: 0.532775  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.492718 \n",
      "\n",
      "loss: 0.463152  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.504843 \n",
      "\n",
      "loss: 0.476291  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.494368 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 45/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [223, 359, 216], 'dacay': 0.27383291148450767, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.12563551390974007}\n",
      "loss: 1.107906  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 1.627918 \n",
      "\n",
      "loss: 1.525405  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.097820 \n",
      "\n",
      "loss: 1.097540  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098760 \n",
      "\n",
      "loss: 1.098773  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098642 \n",
      "\n",
      "loss: 1.098395  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098514 \n",
      "\n",
      "loss: 1.099016  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098646 \n",
      "\n",
      "loss: 1.098540  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098612 \n",
      "\n",
      "loss: 1.098635  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098456 \n",
      "\n",
      "loss: 1.098746  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098618 \n",
      "\n",
      "loss: 1.098550  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098602 \n",
      "\n",
      "loss: 1.098687  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098568 \n",
      "\n",
      "loss: 1.098603  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098659 \n",
      "\n",
      "loss: 1.098325  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098907 \n",
      "\n",
      "loss: 1.098100  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098762 \n",
      "\n",
      "loss: 1.098657  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098837 \n",
      "\n",
      "loss: 1.099082  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098606 \n",
      "\n",
      "loss: 1.098281  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098873 \n",
      "\n",
      "loss: 1.098195  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098823 \n",
      "\n",
      "loss: 1.098888  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098747 \n",
      "\n",
      "loss: 1.099591  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099137 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 46/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [102], 'dacay': 0.14839601428674118, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.001872489974982372}\n",
      "loss: 1.080087  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 1.055649 \n",
      "\n",
      "loss: 1.050553  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 1.031671 \n",
      "\n",
      "loss: 1.031272  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.016106 \n",
      "\n",
      "loss: 1.020077  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 1.000684 \n",
      "\n",
      "loss: 0.999350  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.984276 \n",
      "\n",
      "loss: 0.981938  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.975071 \n",
      "\n",
      "loss: 0.967436  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.964443 \n",
      "\n",
      "loss: 0.966902  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.959264 \n",
      "\n",
      "loss: 0.968169  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.952349 \n",
      "\n",
      "loss: 0.952364  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.940753 \n",
      "\n",
      "loss: 0.967488  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.936452 \n",
      "\n",
      "loss: 0.939164  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.929782 \n",
      "\n",
      "loss: 0.947369  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.927872 \n",
      "\n",
      "loss: 0.918699  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.924947 \n",
      "\n",
      "loss: 0.919542  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.915625 \n",
      "\n",
      "loss: 0.930600  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.913713 \n",
      "\n",
      "loss: 0.930415  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.908419 \n",
      "\n",
      "loss: 0.916070  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.908274 \n",
      "\n",
      "loss: 0.908168  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.907611 \n",
      "\n",
      "loss: 0.887671  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.899219 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 47/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [276], 'dacay': 0.0006425543697139025, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.2804973417008957}\n",
      "loss: 1.104552  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.462257 \n",
      "\n",
      "loss: 1.427989  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 1.563484 \n",
      "\n",
      "loss: 1.663716  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.460505 \n",
      "\n",
      "loss: 1.505437  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.645021 \n",
      "\n",
      "loss: 1.558888  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1.164748 \n",
      "\n",
      "loss: 1.327257  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.130189 \n",
      "\n",
      "loss: 1.067821  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.642751 \n",
      "\n",
      "loss: 1.673038  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.993541 \n",
      "\n",
      "loss: 0.967664  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.786814 \n",
      "\n",
      "loss: 0.758756  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.304175 \n",
      "\n",
      "loss: 1.368496  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 1.242086 \n",
      "\n",
      "loss: 1.172029  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.187210 \n",
      "\n",
      "loss: 1.242707  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 2.269449 \n",
      "\n",
      "loss: 2.193397  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.978472 \n",
      "\n",
      "loss: 1.062432  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.189261 \n",
      "\n",
      "loss: 1.165370  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 1.440662 \n",
      "\n",
      "loss: 1.412710  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 1.013476 \n",
      "\n",
      "loss: 0.955937  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 1.225636 \n",
      "\n",
      "loss: 1.222288  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.483270 \n",
      "\n",
      "loss: 1.470660  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.403655 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 48/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [85], 'dacay': 5.311958766789297, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.41731405044101977}\n",
      "loss: 1.119186  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 23.0%, Avg loss: 8927864310598001509483145068544.000000 \n",
      "\n",
      "loss: 9309460815616421303202839789568.000000  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 49/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [573, 159, 448], 'dacay': 0.00043373812368277954, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.44950084622242076}\n",
      "loss: 1.102487  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 5414.740948 \n",
      "\n",
      "loss: 5672.748047  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 7098.790602 \n",
      "\n",
      "loss: 6634.328125  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 408.180253 \n",
      "\n",
      "loss: 370.317047  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 202.330300 \n",
      "\n",
      "loss: 175.957428  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 314.379859 \n",
      "\n",
      "loss: 235.406250  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 226.309036 \n",
      "\n",
      "loss: 188.398697  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 84.585443 \n",
      "\n",
      "loss: 81.041275  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 65.867012 \n",
      "\n",
      "loss: 62.422268  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 38.060706 \n",
      "\n",
      "loss: 34.461155  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 64.478199 \n",
      "\n",
      "loss: 82.669693  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 550.655780 \n",
      "\n",
      "loss: 536.253235  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 44.668816 \n",
      "\n",
      "loss: 40.709106  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 40.580154 \n",
      "\n",
      "loss: 31.324026  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 35.708157 \n",
      "\n",
      "loss: 37.174210  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 1005.139221 \n",
      "\n",
      "loss: 932.048523  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 86.922893 \n",
      "\n",
      "loss: 82.334412  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 50.388312 \n",
      "\n",
      "loss: 53.666714  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 25.917429 \n",
      "\n",
      "loss: 27.836569  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 27.403966 \n",
      "\n",
      "loss: 31.906445  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 24.483327 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 50/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [374], 'dacay': 0.0011230075904884356, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.1769039494438477}\n",
      "loss: 1.181788  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 4.421828 \n",
      "\n",
      "loss: 4.322226  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 3.232688 \n",
      "\n",
      "loss: 3.247504  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.084615 \n",
      "\n",
      "loss: 1.051308  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.313912 \n",
      "\n",
      "loss: 1.281747  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.127866 \n",
      "\n",
      "loss: 1.084343  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.523049 \n",
      "\n",
      "loss: 1.580331  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.107418 \n",
      "\n",
      "loss: 1.154554  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.058926 \n",
      "\n",
      "loss: 1.050574  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 1.544814 \n",
      "\n",
      "loss: 1.504927  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.451427 \n",
      "\n",
      "loss: 1.426328  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.921596 \n",
      "\n",
      "loss: 0.869316  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.047947 \n",
      "\n",
      "loss: 1.108794  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.724629 \n",
      "\n",
      "loss: 0.670747  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.863177 \n",
      "\n",
      "loss: 0.873650  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.036146 \n",
      "\n",
      "loss: 0.950215  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.953221 \n",
      "\n",
      "loss: 0.937513  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.935977 \n",
      "\n",
      "loss: 0.983081  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.111490 \n",
      "\n",
      "loss: 1.037233  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.906829 \n",
      "\n",
      "loss: 0.931513  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.718912 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 51/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [424, 166, 250], 'dacay': 2.1039164104085075, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.012490702312904544}\n",
      "loss: 1.105828  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.104966 \n",
      "\n",
      "loss: 1.107428  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.105350 \n",
      "\n",
      "loss: 1.107310  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.110567 \n",
      "\n",
      "loss: 1.118917  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.113542 \n",
      "\n",
      "loss: 1.117882  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098596 \n",
      "\n",
      "loss: 1.098199  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.134431 \n",
      "\n",
      "loss: 1.149550  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.103359 \n",
      "\n",
      "loss: 1.105562  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.139351 \n",
      "\n",
      "loss: 1.138432  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.157056 \n",
      "\n",
      "loss: 1.162384  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.137796 \n",
      "\n",
      "loss: 1.146101  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.104757 \n",
      "\n",
      "loss: 1.105416  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.110705 \n",
      "\n",
      "loss: 1.114558  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.102376 \n",
      "\n",
      "loss: 1.097639  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.100992 \n",
      "\n",
      "loss: 1.099190  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.102048 \n",
      "\n",
      "loss: 1.101920  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.101156 \n",
      "\n",
      "loss: 1.099127  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.108601 \n",
      "\n",
      "loss: 1.115518  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.115821 \n",
      "\n",
      "loss: 1.110133  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.126005 \n",
      "\n",
      "loss: 1.123659  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.100033 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 52/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [348, 429, 262], 'dacay': 0.0021578862512781124, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.43609617477475426}\n",
      "loss: 1.097672  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.627312 \n",
      "\n",
      "loss: 1.382648  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 274.514123 \n",
      "\n",
      "loss: 255.980377  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.199688 \n",
      "\n",
      "loss: 1.198571  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.327408 \n",
      "\n",
      "loss: 1.313489  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 4.842290 \n",
      "\n",
      "loss: 5.026798  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.795347 \n",
      "\n",
      "loss: 3.967976  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 7.032046 \n",
      "\n",
      "loss: 7.372789  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.187500 \n",
      "\n",
      "loss: 2.186233  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 4.005963 \n",
      "\n",
      "loss: 3.949771  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 6.002003 \n",
      "\n",
      "loss: 5.879643  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.879123 \n",
      "\n",
      "loss: 8.901587  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.267577 \n",
      "\n",
      "loss: 1.260512  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.980947 \n",
      "\n",
      "loss: 1.833191  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 2.799258 \n",
      "\n",
      "loss: 2.566370  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 9.637661 \n",
      "\n",
      "loss: 10.751808  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.067429 \n",
      "\n",
      "loss: 2.061305  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.758255 \n",
      "\n",
      "loss: 1.740150  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.440056 \n",
      "\n",
      "loss: 2.552439  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 2.817897 \n",
      "\n",
      "loss: 2.912241  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.780011 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 53/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [235, 212], 'dacay': 1.210411463101838, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.15266491458256642}\n",
      "loss: 1.132816  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.171573 \n",
      "\n",
      "loss: 1.175827  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.108826 \n",
      "\n",
      "loss: 1.108039  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.431317 \n",
      "\n",
      "loss: 1.417921  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.313239 \n",
      "\n",
      "loss: 1.327991  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.157101 \n",
      "\n",
      "loss: 1.156337  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.243063 \n",
      "\n",
      "loss: 1.262020  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.499481 \n",
      "\n",
      "loss: 1.534722  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.203321 \n",
      "\n",
      "loss: 1.201265  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.263867 \n",
      "\n",
      "loss: 1.277886  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.405041 \n",
      "\n",
      "loss: 1.398044  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.239164 \n",
      "\n",
      "loss: 1.222971  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.108480 \n",
      "\n",
      "loss: 1.107335  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.164276 \n",
      "\n",
      "loss: 1.145545  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.193056 \n",
      "\n",
      "loss: 1.192146  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.433294 \n",
      "\n",
      "loss: 1.475904  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.116486 \n",
      "\n",
      "loss: 1.120183  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.579919 \n",
      "\n",
      "loss: 1.585256  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.236483 \n",
      "\n",
      "loss: 1.233736  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.237300 \n",
      "\n",
      "loss: 1.228945  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.714399 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 54/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [236], 'dacay': 0.00013131736919340506, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.16419814940373348}\n",
      "loss: 1.162677  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.465425 \n",
      "\n",
      "loss: 1.440357  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.564310 \n",
      "\n",
      "loss: 0.559310  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.536290 \n",
      "\n",
      "loss: 0.530835  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.515877 \n",
      "\n",
      "loss: 0.491372  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.461512 \n",
      "\n",
      "loss: 0.497899  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.523237 \n",
      "\n",
      "loss: 0.515652  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.546059 \n",
      "\n",
      "loss: 0.568272  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.531236 \n",
      "\n",
      "loss: 0.492447  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.507887 \n",
      "\n",
      "loss: 0.496631  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.484574 \n",
      "\n",
      "loss: 0.480356  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.489017 \n",
      "\n",
      "loss: 0.443284  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.510016 \n",
      "\n",
      "loss: 0.542509  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.532845 \n",
      "\n",
      "loss: 0.575932  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.474799 \n",
      "\n",
      "loss: 0.421691  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.525280 \n",
      "\n",
      "loss: 0.507582  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.584861 \n",
      "\n",
      "loss: 0.595101  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.677575 \n",
      "\n",
      "loss: 0.707562  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.593324 \n",
      "\n",
      "loss: 0.502378  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.611605 \n",
      "\n",
      "loss: 0.629689  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.558361 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 55/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [293], 'dacay': 0.005698107592775896, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.4696246464483254}\n",
      "loss: 1.170156  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 2.167025 \n",
      "\n",
      "loss: 2.169128  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 3.664604 \n",
      "\n",
      "loss: 3.643708  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 3.120880 \n",
      "\n",
      "loss: 3.322824  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.9%, Avg loss: 5.861759 \n",
      "\n",
      "loss: 5.539693  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 1.914825 \n",
      "\n",
      "loss: 1.909770  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 4.717332 \n",
      "\n",
      "loss: 5.077877  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 3.026036 \n",
      "\n",
      "loss: 2.862048  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 6.038505 \n",
      "\n",
      "loss: 6.173450  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 3.521786 \n",
      "\n",
      "loss: 3.645005  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 5.679315 \n",
      "\n",
      "loss: 5.302520  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 2.423865 \n",
      "\n",
      "loss: 2.484414  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.791837 \n",
      "\n",
      "loss: 2.648847  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 2.499520 \n",
      "\n",
      "loss: 2.555448  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 4.184199 \n",
      "\n",
      "loss: 4.202302  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 3.452384 \n",
      "\n",
      "loss: 3.213522  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 3.498311 \n",
      "\n",
      "loss: 3.649855  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 6.054386 \n",
      "\n",
      "loss: 5.859424  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 2.987797 \n",
      "\n",
      "loss: 3.221163  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 3.260097 \n",
      "\n",
      "loss: 3.115936  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 6.196433 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 56/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [275], 'dacay': 0.0007813382416811382, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.05429043335263408}\n",
      "loss: 1.086735  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.816170 \n",
      "\n",
      "loss: 0.810545  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.762977 \n",
      "\n",
      "loss: 0.765169  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.734929 \n",
      "\n",
      "loss: 0.742959  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.718319 \n",
      "\n",
      "loss: 0.712264  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.697999 \n",
      "\n",
      "loss: 0.673358  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.684032 \n",
      "\n",
      "loss: 0.664344  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.656758 \n",
      "\n",
      "loss: 0.649885  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.649513 \n",
      "\n",
      "loss: 0.674049  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.645042 \n",
      "\n",
      "loss: 0.651953  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.634665 \n",
      "\n",
      "loss: 0.645414  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.626822 \n",
      "\n",
      "loss: 0.621894  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.603936 \n",
      "\n",
      "loss: 0.610204  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.600509 \n",
      "\n",
      "loss: 0.586897  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.600802 \n",
      "\n",
      "loss: 0.587270  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.595978 \n",
      "\n",
      "loss: 0.612161  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.588845 \n",
      "\n",
      "loss: 0.591882  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.567655 \n",
      "\n",
      "loss: 0.583090  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.567068 \n",
      "\n",
      "loss: 0.560964  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.569750 \n",
      "\n",
      "loss: 0.560860  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.565692 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 57/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [361, 329], 'dacay': 0.01520440464844374, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.44976241598230865}\n",
      "loss: 1.099491  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 35.5%, Avg loss: 3.031929 \n",
      "\n",
      "loss: 5.153836  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.160947 \n",
      "\n",
      "loss: 1.093672  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 2.038733 \n",
      "\n",
      "loss: 2.067768  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 14.430459 \n",
      "\n",
      "loss: 15.093607  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 5.650985 \n",
      "\n",
      "loss: 6.147455  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 152.369841 \n",
      "\n",
      "loss: 156.832367  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.293481 \n",
      "\n",
      "loss: 1.194349  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 1.071083 \n",
      "\n",
      "loss: 1.072413  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 244.802051 \n",
      "\n",
      "loss: 258.171783  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 1.707613 \n",
      "\n",
      "loss: 1.723557  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 1.502372 \n",
      "\n",
      "loss: 1.661560  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 1.736987 \n",
      "\n",
      "loss: 1.675571  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 5.596647 \n",
      "\n",
      "loss: 5.330060  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 1.288198 \n",
      "\n",
      "loss: 1.218526  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 44.3%, Avg loss: 2.992286 \n",
      "\n",
      "loss: 2.921310  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 9.861627 \n",
      "\n",
      "loss: 10.600857  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.889146 \n",
      "\n",
      "loss: 0.875228  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.770485 \n",
      "\n",
      "loss: 0.767239  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.692043 \n",
      "\n",
      "loss: 0.692945  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 26.526269 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 58/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [268], 'dacay': 0.0001666988748655079, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.260008182248436}\n",
      "loss: 1.133987  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 37.0%, Avg loss: 2.308816 \n",
      "\n",
      "loss: 2.291081  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 2.633785 \n",
      "\n",
      "loss: 2.672654  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.991031 \n",
      "\n",
      "loss: 1.027130  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.995407 \n",
      "\n",
      "loss: 0.971458  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.888637 \n",
      "\n",
      "loss: 0.886517  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.846889 \n",
      "\n",
      "loss: 0.846907  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.783137 \n",
      "\n",
      "loss: 0.714665  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.807601 \n",
      "\n",
      "loss: 0.802749  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.795014 \n",
      "\n",
      "loss: 0.818501  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.732969 \n",
      "\n",
      "loss: 0.739903  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.728459 \n",
      "\n",
      "loss: 0.716949  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.830289 \n",
      "\n",
      "loss: 0.864616  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.743103 \n",
      "\n",
      "loss: 0.774960  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.727953 \n",
      "\n",
      "loss: 0.713807  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.706102 \n",
      "\n",
      "loss: 0.663157  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.699236 \n",
      "\n",
      "loss: 0.697618  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.720526 \n",
      "\n",
      "loss: 0.702122  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.711639 \n",
      "\n",
      "loss: 0.709210  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.733204 \n",
      "\n",
      "loss: 0.708197  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.757962 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 59/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [209, 380, 449], 'dacay': 0.00022380462422740463, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.33172739938514556}\n",
      "loss: 1.097935  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1619.073477 \n",
      "\n",
      "loss: 1795.870728  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 562.401949 \n",
      "\n",
      "loss: 527.327820  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 374.660896 \n",
      "\n",
      "loss: 368.684357  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 189.421268 \n",
      "\n",
      "loss: 177.560791  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 219.720543 \n",
      "\n",
      "loss: 271.987396  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 99.507389 \n",
      "\n",
      "loss: 100.619118  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 87.405408 \n",
      "\n",
      "loss: 89.551895  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 45.045938 \n",
      "\n",
      "loss: 37.937351  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 54.596401 \n",
      "\n",
      "loss: 55.800510  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 23.435659 \n",
      "\n",
      "loss: 24.677996  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 66.141821 \n",
      "\n",
      "loss: 68.592331  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 38.437413 \n",
      "\n",
      "loss: 35.477318  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 129.168787 \n",
      "\n",
      "loss: 108.771355  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 29.812715 \n",
      "\n",
      "loss: 31.256615  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 30.872693 \n",
      "\n",
      "loss: 35.439644  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 38.537015 \n",
      "\n",
      "loss: 39.421402  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 13.854448 \n",
      "\n",
      "loss: 17.622059  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 73.707462 \n",
      "\n",
      "loss: 57.079643  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 11.676485 \n",
      "\n",
      "loss: 11.622000  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 16.985620 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 60/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [315], 'dacay': 0.03432515603902564, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.20125995349015655}\n",
      "loss: 1.141293  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 1.573288 \n",
      "\n",
      "loss: 1.635789  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.718389 \n",
      "\n",
      "loss: 1.618784  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 47.6%, Avg loss: 1.791943 \n",
      "\n",
      "loss: 1.881209  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 2.589394 \n",
      "\n",
      "loss: 2.605101  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 1.632215 \n",
      "\n",
      "loss: 1.611019  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.279677 \n",
      "\n",
      "loss: 4.045744  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 1.499665 \n",
      "\n",
      "loss: 1.460623  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.827636 \n",
      "\n",
      "loss: 1.781177  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.911612 \n",
      "\n",
      "loss: 2.011363  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.218440 \n",
      "\n",
      "loss: 1.215231  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.224915 \n",
      "\n",
      "loss: 1.228494  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 42.2%, Avg loss: 1.520562 \n",
      "\n",
      "loss: 1.553557  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.542492 \n",
      "\n",
      "loss: 1.521072  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.717431 \n",
      "\n",
      "loss: 1.682764  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 2.278689 \n",
      "\n",
      "loss: 2.107334  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 1.097355 \n",
      "\n",
      "loss: 1.113921  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.053832 \n",
      "\n",
      "loss: 1.083101  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 1.747585 \n",
      "\n",
      "loss: 1.742989  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 1.354905 \n",
      "\n",
      "loss: 1.301433  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.186746 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 61/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [575, 138], 'dacay': 0.0009103333742719545, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.3077959064840951}\n",
      "loss: 1.113578  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 36.3%, Avg loss: 1.151934 \n",
      "\n",
      "loss: 1.166951  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.884797 \n",
      "\n",
      "loss: 0.910152  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 0.891287 \n",
      "\n",
      "loss: 0.871506  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.848506 \n",
      "\n",
      "loss: 0.814251  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.839794 \n",
      "\n",
      "loss: 0.878063  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.813773 \n",
      "\n",
      "loss: 0.818314  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.855360 \n",
      "\n",
      "loss: 0.872846  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.787199 \n",
      "\n",
      "loss: 0.783848  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.835551 \n",
      "\n",
      "loss: 0.823704  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.773281 \n",
      "\n",
      "loss: 0.779139  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.783598 \n",
      "\n",
      "loss: 0.766488  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.785473 \n",
      "\n",
      "loss: 0.801837  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.747939 \n",
      "\n",
      "loss: 0.723096  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.776087 \n",
      "\n",
      "loss: 0.754279  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.755353 \n",
      "\n",
      "loss: 0.721465  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.767714 \n",
      "\n",
      "loss: 0.750443  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.772726 \n",
      "\n",
      "loss: 0.800059  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.739395 \n",
      "\n",
      "loss: 0.724182  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.759152 \n",
      "\n",
      "loss: 0.738032  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.772675 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 62/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [423, 569], 'dacay': 1.4744357297415664, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.1051703241728327}\n",
      "loss: 1.138143  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098882 \n",
      "\n",
      "loss: 1.099087  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098619 \n",
      "\n",
      "loss: 1.098725  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098698 \n",
      "\n",
      "loss: 1.098572  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098673 \n",
      "\n",
      "loss: 1.098956  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098735 \n",
      "\n",
      "loss: 1.098573  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098658 \n",
      "\n",
      "loss: 1.098781  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098665 \n",
      "\n",
      "loss: 1.098683  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098465 \n",
      "\n",
      "loss: 1.098585  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098696 \n",
      "\n",
      "loss: 1.098362  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098581 \n",
      "\n",
      "loss: 1.099037  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098938 \n",
      "\n",
      "loss: 1.098437  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098829 \n",
      "\n",
      "loss: 1.099149  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098594 \n",
      "\n",
      "loss: 1.098638  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098616 \n",
      "\n",
      "loss: 1.098272  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098588 \n",
      "\n",
      "loss: 1.097955  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098673 \n",
      "\n",
      "loss: 1.097390  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098551 \n",
      "\n",
      "loss: 1.098948  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098631 \n",
      "\n",
      "loss: 1.098401  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098720 \n",
      "\n",
      "loss: 1.098946  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098535 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 63/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [629, 337, 484], 'dacay': 0.01617819568592975, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.3968883260130904}\n",
      "loss: 1.137791  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 54.445038 \n",
      "\n",
      "loss: 55.883614  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.105544 \n",
      "\n",
      "loss: 1.098144  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.241029 \n",
      "\n",
      "loss: 1.096701  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.522531 \n",
      "\n",
      "loss: 1.096806  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.373809 \n",
      "\n",
      "loss: 1.527622  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.132133 \n",
      "\n",
      "loss: 1.099999  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 1.162616 \n",
      "\n",
      "loss: 1.100957  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098672 \n",
      "\n",
      "loss: 1.099255  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.099277 \n",
      "\n",
      "loss: 1.099059  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099645 \n",
      "\n",
      "loss: 1.098524  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.106955 \n",
      "\n",
      "loss: 1.106150  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099878 \n",
      "\n",
      "loss: 1.101648  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.107071 \n",
      "\n",
      "loss: 1.111730  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098623 \n",
      "\n",
      "loss: 1.098878  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.101413 \n",
      "\n",
      "loss: 1.103299  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.108933 \n",
      "\n",
      "loss: 1.107290  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098561 \n",
      "\n",
      "loss: 1.098663  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.100160 \n",
      "\n",
      "loss: 1.099158  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.108742 \n",
      "\n",
      "loss: 1.106068  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.101048 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 64/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [181, 94, 298], 'dacay': 0.037779834367386626, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.4092298899218785}\n",
      "loss: 1.104272  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.790011 \n",
      "\n",
      "loss: 0.782985  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.748322 \n",
      "\n",
      "loss: 0.751558  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.745350 \n",
      "\n",
      "loss: 0.723903  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.737820 \n",
      "\n",
      "loss: 0.735695  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.728655 \n",
      "\n",
      "loss: 0.734354  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.725184 \n",
      "\n",
      "loss: 0.714789  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.708077 \n",
      "\n",
      "loss: 0.714720  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.714497 \n",
      "\n",
      "loss: 0.704845  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.727647 \n",
      "\n",
      "loss: 0.723185  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.714480 \n",
      "\n",
      "loss: 0.700464  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.718701 \n",
      "\n",
      "loss: 0.725536  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.708965 \n",
      "\n",
      "loss: 0.723494  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.707754 \n",
      "\n",
      "loss: 0.717828  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.724147 \n",
      "\n",
      "loss: 0.719670  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.702193 \n",
      "\n",
      "loss: 0.696769  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.715488 \n",
      "\n",
      "loss: 0.733492  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.704621 \n",
      "\n",
      "loss: 0.706623  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.724955 \n",
      "\n",
      "loss: 0.750687  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.722388 \n",
      "\n",
      "loss: 0.721570  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.711181 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 65/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [325], 'dacay': 0.0013280160688703432, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.012310251324640488}\n",
      "loss: 1.194697  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.943858 \n",
      "\n",
      "loss: 0.953920  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.890719 \n",
      "\n",
      "loss: 0.899763  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.862949 \n",
      "\n",
      "loss: 0.837984  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.851877 \n",
      "\n",
      "loss: 0.812452  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.838964 \n",
      "\n",
      "loss: 0.816841  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.823666 \n",
      "\n",
      "loss: 0.828445  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.811471 \n",
      "\n",
      "loss: 0.817305  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.807046 \n",
      "\n",
      "loss: 0.818368  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.814079 \n",
      "\n",
      "loss: 0.836529  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.800074 \n",
      "\n",
      "loss: 0.787630  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.793422 \n",
      "\n",
      "loss: 0.801892  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.789002 \n",
      "\n",
      "loss: 0.841681  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.782165 \n",
      "\n",
      "loss: 0.766762  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.789104 \n",
      "\n",
      "loss: 0.757096  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.782372 \n",
      "\n",
      "loss: 0.760270  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.780706 \n",
      "\n",
      "loss: 0.792817  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.767480 \n",
      "\n",
      "loss: 0.777878  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.767360 \n",
      "\n",
      "loss: 0.780317  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.774343 \n",
      "\n",
      "loss: 0.792544  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.770127 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 66/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [494], 'dacay': 0.11385965546094101, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.2072790613977599}\n",
      "loss: 1.096086  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 1.212091 \n",
      "\n",
      "loss: 1.134253  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.831216 \n",
      "\n",
      "loss: 0.813210  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.834385 \n",
      "\n",
      "loss: 0.806301  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.840153 \n",
      "\n",
      "loss: 0.823844  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.858750 \n",
      "\n",
      "loss: 0.837415  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.838536 \n",
      "\n",
      "loss: 0.855012  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.829117 \n",
      "\n",
      "loss: 0.855763  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.841734 \n",
      "\n",
      "loss: 0.869501  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.856748 \n",
      "\n",
      "loss: 0.832833  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.882733 \n",
      "\n",
      "loss: 0.892115  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.851988 \n",
      "\n",
      "loss: 0.848474  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.848727 \n",
      "\n",
      "loss: 0.830893  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.830196 \n",
      "\n",
      "loss: 0.835662  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.863336 \n",
      "\n",
      "loss: 0.866520  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.918455 \n",
      "\n",
      "loss: 0.929738  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 38.444037 \n",
      "\n",
      "loss: 36.235474  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 185.316959 \n",
      "\n",
      "loss: 197.596283  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 45.5%, Avg loss: 49.277940 \n",
      "\n",
      "loss: 50.438843  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.504994 \n",
      "\n",
      "loss: 1.455051  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.848570 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 67/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [597, 207, 401], 'dacay': 0.029640521816147642, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.3280633997547809}\n",
      "loss: 1.117115  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.027714 \n",
      "\n",
      "loss: 1.026559  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 0.981429 \n",
      "\n",
      "loss: 0.981871  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.970912 \n",
      "\n",
      "loss: 0.975273  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.956766 \n",
      "\n",
      "loss: 0.960841  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 0.960798 \n",
      "\n",
      "loss: 0.958129  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.956680 \n",
      "\n",
      "loss: 0.959145  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.957031 \n",
      "\n",
      "loss: 0.954412  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.953894 \n",
      "\n",
      "loss: 0.959170  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 1.015071 \n",
      "\n",
      "loss: 1.023681  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 0.961843 \n",
      "\n",
      "loss: 0.975028  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 0.953772 \n",
      "\n",
      "loss: 0.940422  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.943447 \n",
      "\n",
      "loss: 0.937465  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 0.946737 \n",
      "\n",
      "loss: 0.948156  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 0.942389 \n",
      "\n",
      "loss: 0.943922  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 0.957955 \n",
      "\n",
      "loss: 0.961348  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.949510 \n",
      "\n",
      "loss: 0.950977  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.940974 \n",
      "\n",
      "loss: 0.948332  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 0.955445 \n",
      "\n",
      "loss: 0.937194  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 0.948734 \n",
      "\n",
      "loss: 0.925898  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 0.950166 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 68/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [117], 'dacay': 0.7800133465743285, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.4020745171009542}\n",
      "loss: 1.188871  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 7.896566 \n",
      "\n",
      "loss: 8.556713  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 7.167455 \n",
      "\n",
      "loss: 7.247827  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.401689 \n",
      "\n",
      "loss: 8.179262  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 6.367321 \n",
      "\n",
      "loss: 6.203549  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 6.472492 \n",
      "\n",
      "loss: 6.709177  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.678199 \n",
      "\n",
      "loss: 8.870173  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 7.279705 \n",
      "\n",
      "loss: 7.065228  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 6.367353 \n",
      "\n",
      "loss: 6.881809  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 7.744945 \n",
      "\n",
      "loss: 7.757035  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 8.511303 \n",
      "\n",
      "loss: 8.352284  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 6.654222 \n",
      "\n",
      "loss: 6.358366  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 8.497774 \n",
      "\n",
      "loss: 8.335189  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.020014 \n",
      "\n",
      "loss: 8.059561  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 8.174756 \n",
      "\n",
      "loss: 7.986001  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 7.528180 \n",
      "\n",
      "loss: 7.806486  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 6.554059 \n",
      "\n",
      "loss: 6.449155  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 7.303743 \n",
      "\n",
      "loss: 7.121532  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.125007 \n",
      "\n",
      "loss: 8.192404  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 6.335943 \n",
      "\n",
      "loss: 6.047830  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 6.280248 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 69/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [538, 483, 59], 'dacay': 0.06123912407005947, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.06675621886795874}\n",
      "loss: 1.101238  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 5.962735 \n",
      "\n",
      "loss: 5.751370  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.016730 \n",
      "\n",
      "loss: 1.008484  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 25.4%, Avg loss: 1.781629 \n",
      "\n",
      "loss: 1.796314  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.475400 \n",
      "\n",
      "loss: 1.541832  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.109970 \n",
      "\n",
      "loss: 1.108745  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.823203 \n",
      "\n",
      "loss: 0.818261  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.833844 \n",
      "\n",
      "loss: 0.838499  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.829424 \n",
      "\n",
      "loss: 0.803342  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.837333 \n",
      "\n",
      "loss: 0.844566  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.816517 \n",
      "\n",
      "loss: 0.789031  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.821033 \n",
      "\n",
      "loss: 0.796170  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.806977 \n",
      "\n",
      "loss: 0.786310  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.832494 \n",
      "\n",
      "loss: 0.827999  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.823139 \n",
      "\n",
      "loss: 0.853267  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.831644 \n",
      "\n",
      "loss: 0.854815  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.824423 \n",
      "\n",
      "loss: 0.827068  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.835058 \n",
      "\n",
      "loss: 0.852685  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.828158 \n",
      "\n",
      "loss: 0.824309  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.840855 \n",
      "\n",
      "loss: 0.827174  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.810576 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 70/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [551, 580], 'dacay': 0.09489899044704321, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.41530152794373537}\n",
      "loss: 1.176242  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 25.433061 \n",
      "\n",
      "loss: 26.519266  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 19.160124 \n",
      "\n",
      "loss: 17.937981  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 22.350871 \n",
      "\n",
      "loss: 22.351465  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 24.594771 \n",
      "\n",
      "loss: 25.334574  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 9.768789 \n",
      "\n",
      "loss: 9.767312  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 50.690746 \n",
      "\n",
      "loss: 50.988831  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 27.911079 \n",
      "\n",
      "loss: 28.329058  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.130306 \n",
      "\n",
      "loss: 8.172082  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 15.570113 \n",
      "\n",
      "loss: 15.731771  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 47.6%, Avg loss: 18.678537 \n",
      "\n",
      "loss: 21.028044  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 25.360564 \n",
      "\n",
      "loss: 25.096893  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 12.288263 \n",
      "\n",
      "loss: 12.342062  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 24.547022 \n",
      "\n",
      "loss: 24.399624  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 43.741506 \n",
      "\n",
      "loss: 42.273312  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 41.526363 \n",
      "\n",
      "loss: 41.309898  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 76.338175 \n",
      "\n",
      "loss: 74.228607  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 17.107118 \n",
      "\n",
      "loss: 16.972418  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 35.750219 \n",
      "\n",
      "loss: 37.286209  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 16.773996 \n",
      "\n",
      "loss: 16.610258  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 27.865693 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 71/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [464, 170, 494], 'dacay': 0.017797081376238918, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.382930260780153}\n",
      "loss: 1.178503  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 24.025108 \n",
      "\n",
      "loss: 21.403282  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 10.606574 \n",
      "\n",
      "loss: 11.314580  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 19.366386 \n",
      "\n",
      "loss: 20.149969  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 10.649189 \n",
      "\n",
      "loss: 10.260730  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 3.485672 \n",
      "\n",
      "loss: 3.508740  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.501061 \n",
      "\n",
      "loss: 8.677461  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.361810 \n",
      "\n",
      "loss: 3.314065  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 17.717474 \n",
      "\n",
      "loss: 17.803040  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 22.273452 \n",
      "\n",
      "loss: 22.092163  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 11.177217 \n",
      "\n",
      "loss: 11.322800  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 3.938570 \n",
      "\n",
      "loss: 3.774686  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 4.380913 \n",
      "\n",
      "loss: 4.237577  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 2.281494 \n",
      "\n",
      "loss: 2.091836  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 2.171363 \n",
      "\n",
      "loss: 2.295659  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 14.850834 \n",
      "\n",
      "loss: 17.971977  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 9.474816 \n",
      "\n",
      "loss: 8.881170  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 2.306550 \n",
      "\n",
      "loss: 2.432260  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 35.5%, Avg loss: 3.478029 \n",
      "\n",
      "loss: 3.513555  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 3.664784 \n",
      "\n",
      "loss: 3.768460  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 3.062972 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 72/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [504, 252, 289], 'dacay': 0.015825701368744654, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.13593038203038407}\n",
      "loss: 1.099902  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.837084 \n",
      "\n",
      "loss: 0.839293  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.750493 \n",
      "\n",
      "loss: 0.756832  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.700209 \n",
      "\n",
      "loss: 0.683191  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.665636 \n",
      "\n",
      "loss: 0.684222  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.670611 \n",
      "\n",
      "loss: 0.643058  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.620341 \n",
      "\n",
      "loss: 0.630462  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.612756 \n",
      "\n",
      "loss: 0.614150  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.586498 \n",
      "\n",
      "loss: 0.646188  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.586340 \n",
      "\n",
      "loss: 0.579839  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.580773 \n",
      "\n",
      "loss: 0.567992  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.558384 \n",
      "\n",
      "loss: 0.569369  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.572338 \n",
      "\n",
      "loss: 0.604783  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.586621 \n",
      "\n",
      "loss: 0.562080  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.574688 \n",
      "\n",
      "loss: 0.613103  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.537446 \n",
      "\n",
      "loss: 0.535679  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.542041 \n",
      "\n",
      "loss: 0.534968  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.517712 \n",
      "\n",
      "loss: 0.520681  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.547847 \n",
      "\n",
      "loss: 0.515485  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.610892 \n",
      "\n",
      "loss: 0.566184  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.572773 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 73/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [561, 90], 'dacay': 0.0004918926209195686, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.34700551116829625}\n",
      "loss: 1.112866  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.312080 \n",
      "\n",
      "loss: 1.327129  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098998 \n",
      "\n",
      "loss: 1.097859  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099576 \n",
      "\n",
      "loss: 1.100351  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098650 \n",
      "\n",
      "loss: 1.098659  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.106230 \n",
      "\n",
      "loss: 1.101997  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099023 \n",
      "\n",
      "loss: 1.098000  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 16.772635 \n",
      "\n",
      "loss: 17.965134  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098561 \n",
      "\n",
      "loss: 1.098897  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.099429 \n",
      "\n",
      "loss: 1.098547  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098705 \n",
      "\n",
      "loss: 1.098744  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098627 \n",
      "\n",
      "loss: 1.099375  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098883 \n",
      "\n",
      "loss: 1.099276  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098379 \n",
      "\n",
      "loss: 1.099113  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.099159 \n",
      "\n",
      "loss: 1.099267  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098746 \n",
      "\n",
      "loss: 1.097910  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.101058 \n",
      "\n",
      "loss: 1.102352  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098575 \n",
      "\n",
      "loss: 1.098423  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.099229 \n",
      "\n",
      "loss: 1.100994  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098926 \n",
      "\n",
      "loss: 1.099413  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099452 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 74/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [227, 73, 432], 'dacay': 0.010914324159150508, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.0312266287207343}\n",
      "loss: 1.161982  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.155447 \n",
      "\n",
      "loss: 1.155411  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.108234 \n",
      "\n",
      "loss: 1.107745  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.169772 \n",
      "\n",
      "loss: 1.186256  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.122841 \n",
      "\n",
      "loss: 1.130879  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.138418 \n",
      "\n",
      "loss: 1.125629  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.134228 \n",
      "\n",
      "loss: 1.136826  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.182171 \n",
      "\n",
      "loss: 1.180301  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.121427 \n",
      "\n",
      "loss: 1.121870  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.121686 \n",
      "\n",
      "loss: 1.108152  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.151105 \n",
      "\n",
      "loss: 1.172152  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.228120 \n",
      "\n",
      "loss: 1.233830  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.102909 \n",
      "\n",
      "loss: 1.102784  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.120412 \n",
      "\n",
      "loss: 1.118327  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.152818 \n",
      "\n",
      "loss: 1.173030  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.141470 \n",
      "\n",
      "loss: 1.149929  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.204425 \n",
      "\n",
      "loss: 1.214199  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.111778 \n",
      "\n",
      "loss: 1.106150  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.133192 \n",
      "\n",
      "loss: 1.135689  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.109720 \n",
      "\n",
      "loss: 1.113232  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.114120 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 75/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [406, 606], 'dacay': 5.374154750811627, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.2385421683054227}\n",
      "loss: 1.099492  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098129 \n",
      "\n",
      "loss: 1.097531  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098595 \n",
      "\n",
      "loss: 1.098621  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098624 \n",
      "\n",
      "loss: 1.098541  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098608 \n",
      "\n",
      "loss: 1.098652  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098576 \n",
      "\n",
      "loss: 1.098793  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098635 \n",
      "\n",
      "loss: 1.098689  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098595 \n",
      "\n",
      "loss: 1.098687  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098465 \n",
      "\n",
      "loss: 1.098150  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098592 \n",
      "\n",
      "loss: 1.098474  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098634 \n",
      "\n",
      "loss: 1.098694  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098554 \n",
      "\n",
      "loss: 1.098675  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098628 \n",
      "\n",
      "loss: 1.098526  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098673 \n",
      "\n",
      "loss: 1.098889  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098680 \n",
      "\n",
      "loss: 1.098498  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098806 \n",
      "\n",
      "loss: 1.098090  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098667 \n",
      "\n",
      "loss: 1.098894  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098614 \n",
      "\n",
      "loss: 1.098625  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098689 \n",
      "\n",
      "loss: 1.099218  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098633 \n",
      "\n",
      "loss: 1.098439  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098582 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 76/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [313, 407], 'dacay': 1.850289792019238, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.2441659158273702}\n",
      "loss: 1.118279  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.375214 \n",
      "\n",
      "loss: 6.390480  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 6.654671 \n",
      "\n",
      "loss: 6.187811  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 6.830486 \n",
      "\n",
      "loss: 6.801063  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 5.700099 \n",
      "\n",
      "loss: 5.633318  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 5.670571 \n",
      "\n",
      "loss: 5.582626  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 6.287210 \n",
      "\n",
      "loss: 6.164022  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 6.719642 \n",
      "\n",
      "loss: 7.030211  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.091613 \n",
      "\n",
      "loss: 6.124570  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 6.228408 \n",
      "\n",
      "loss: 6.332526  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 6.019772 \n",
      "\n",
      "loss: 6.136238  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 6.167603 \n",
      "\n",
      "loss: 6.251375  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.868713 \n",
      "\n",
      "loss: 5.685678  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.320787 \n",
      "\n",
      "loss: 5.811672  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 7.291243 \n",
      "\n",
      "loss: 7.511590  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 5.913684 \n",
      "\n",
      "loss: 5.996796  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.115121 \n",
      "\n",
      "loss: 6.196876  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 5.086077 \n",
      "\n",
      "loss: 4.998784  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 5.662713 \n",
      "\n",
      "loss: 5.816827  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 6.974801 \n",
      "\n",
      "loss: 6.978034  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 5.846884 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 77/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [454, 505], 'dacay': 0.00013967674306039884, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.36211827067588165}\n",
      "loss: 1.104015  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1014.713994 \n",
      "\n",
      "loss: 846.544556  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 274.800044 \n",
      "\n",
      "loss: 353.499756  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 44.033085 \n",
      "\n",
      "loss: 39.681301  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 34.105370 \n",
      "\n",
      "loss: 30.491566  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 10.161231 \n",
      "\n",
      "loss: 8.840393  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 11.181833 \n",
      "\n",
      "loss: 12.300838  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 8.247565 \n",
      "\n",
      "loss: 9.871725  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 2.766496 \n",
      "\n",
      "loss: 2.311984  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 7.060153 \n",
      "\n",
      "loss: 6.447650  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 1.999145 \n",
      "\n",
      "loss: 1.864297  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 1.162998 \n",
      "\n",
      "loss: 1.521770  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 1.011265 \n",
      "\n",
      "loss: 0.958749  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 1.075573 \n",
      "\n",
      "loss: 1.161347  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 1.039916 \n",
      "\n",
      "loss: 1.084818  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.692666 \n",
      "\n",
      "loss: 0.618124  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.520051 \n",
      "\n",
      "loss: 0.407910  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.743763 \n",
      "\n",
      "loss: 0.659249  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.556835 \n",
      "\n",
      "loss: 0.468306  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.437936 \n",
      "\n",
      "loss: 0.374842  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.477030 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 78/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [138, 554], 'dacay': 0.00013640561862503632, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.04206533633478391}\n",
      "loss: 1.162971  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.095999 \n",
      "\n",
      "loss: 1.094876  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.167762 \n",
      "\n",
      "loss: 1.149608  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.166242 \n",
      "\n",
      "loss: 1.174455  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.036007 \n",
      "\n",
      "loss: 1.036810  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.010245 \n",
      "\n",
      "loss: 1.004149  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.3%, Avg loss: 1.010965 \n",
      "\n",
      "loss: 1.009150  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 0.959839 \n",
      "\n",
      "loss: 0.958887  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 0.935685 \n",
      "\n",
      "loss: 0.953192  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.925004 \n",
      "\n",
      "loss: 0.939124  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.886868 \n",
      "\n",
      "loss: 0.896481  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.879471 \n",
      "\n",
      "loss: 0.897872  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.883694 \n",
      "\n",
      "loss: 0.882132  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.887375 \n",
      "\n",
      "loss: 0.883812  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.862368 \n",
      "\n",
      "loss: 0.863342  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.858787 \n",
      "\n",
      "loss: 0.861425  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.848545 \n",
      "\n",
      "loss: 0.819884  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.830615 \n",
      "\n",
      "loss: 0.822028  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.830908 \n",
      "\n",
      "loss: 0.838095  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.846027 \n",
      "\n",
      "loss: 0.835425  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.831978 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 79/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [146, 480], 'dacay': 0.0008102986821227945, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.16374359820417295}\n",
      "loss: 1.095553  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 14.266094 \n",
      "\n",
      "loss: 14.316703  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 9.375635 \n",
      "\n",
      "loss: 8.707891  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.794794 \n",
      "\n",
      "loss: 0.758142  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.665928 \n",
      "\n",
      "loss: 0.688699  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.599845 \n",
      "\n",
      "loss: 0.619972  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.643874 \n",
      "\n",
      "loss: 0.586221  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.527993 \n",
      "\n",
      "loss: 0.545470  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.502888 \n",
      "\n",
      "loss: 0.492731  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.498163 \n",
      "\n",
      "loss: 0.444976  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.466799 \n",
      "\n",
      "loss: 0.458754  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.448599 \n",
      "\n",
      "loss: 0.442525  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.483483 \n",
      "\n",
      "loss: 0.465133  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.454312 \n",
      "\n",
      "loss: 0.452791  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.426570 \n",
      "\n",
      "loss: 0.430287  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.441445 \n",
      "\n",
      "loss: 0.420793  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.439514 \n",
      "\n",
      "loss: 0.416981  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.429978 \n",
      "\n",
      "loss: 0.400141  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.410141 \n",
      "\n",
      "loss: 0.399887  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.421175 \n",
      "\n",
      "loss: 0.413784  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.435562 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 80/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [342, 353], 'dacay': 0.6399536780650531, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.37602971079503694}\n",
      "loss: 1.105596  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.156450 \n",
      "\n",
      "loss: 1.182858  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 12.117228 \n",
      "\n",
      "loss: 12.619017  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: 2.114196 \n",
      "\n",
      "loss: 2.280815  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 27.043512 \n",
      "\n",
      "loss: 25.682524  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 12.063891 \n",
      "\n",
      "loss: 12.163476  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 9.110403 \n",
      "\n",
      "loss: 9.201996  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 15.100848 \n",
      "\n",
      "loss: 15.653031  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.946905 \n",
      "\n",
      "loss: 6.915582  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 4.533985 \n",
      "\n",
      "loss: 4.629891  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 15.267969 \n",
      "\n",
      "loss: 16.018908  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 16.813661 \n",
      "\n",
      "loss: 16.744846  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 18.841247 \n",
      "\n",
      "loss: 17.661623  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 8.984716 \n",
      "\n",
      "loss: 9.345478  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 3.481347 \n",
      "\n",
      "loss: 3.618986  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 4.993447 \n",
      "\n",
      "loss: 5.055605  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 24.299148 \n",
      "\n",
      "loss: 24.428877  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 10.739503 \n",
      "\n",
      "loss: 10.864896  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 5.203239 \n",
      "\n",
      "loss: 5.076641  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 7.865608 \n",
      "\n",
      "loss: 7.508113  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 11.516609 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 81/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [266, 91], 'dacay': 0.022466435906934916, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.11218866064330212}\n",
      "loss: 1.091671  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 1.237079 \n",
      "\n",
      "loss: 1.266314  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.749216 \n",
      "\n",
      "loss: 0.760442  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.722653 \n",
      "\n",
      "loss: 0.780472  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.740222 \n",
      "\n",
      "loss: 0.716568  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.891297 \n",
      "\n",
      "loss: 0.825653  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.760272 \n",
      "\n",
      "loss: 0.752495  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.757309 \n",
      "\n",
      "loss: 0.751528  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.777977 \n",
      "\n",
      "loss: 0.770979  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.733925 \n",
      "\n",
      "loss: 0.717538  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.756676 \n",
      "\n",
      "loss: 0.799794  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.765936 \n",
      "\n",
      "loss: 0.799494  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.742913 \n",
      "\n",
      "loss: 0.747645  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.762194 \n",
      "\n",
      "loss: 0.757184  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.754757 \n",
      "\n",
      "loss: 0.757795  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.846020 \n",
      "\n",
      "loss: 0.859276  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.309569 \n",
      "\n",
      "loss: 1.313461  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 2.529219 \n",
      "\n",
      "loss: 2.748833  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.313010 \n",
      "\n",
      "loss: 1.355722  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 0.955115 \n",
      "\n",
      "loss: 0.935148  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 4.458124 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 82/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [49], 'dacay': 0.5904267429287562, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.4196305002576706}\n",
      "loss: 1.147835  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.169372 \n",
      "\n",
      "loss: 2.235511  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.355563 \n",
      "\n",
      "loss: 1.398330  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.417546 \n",
      "\n",
      "loss: 2.505193  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 2.662313 \n",
      "\n",
      "loss: 2.735923  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.164320 \n",
      "\n",
      "loss: 2.174348  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.282613 \n",
      "\n",
      "loss: 1.318611  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 2.623570 \n",
      "\n",
      "loss: 2.620251  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.760543 \n",
      "\n",
      "loss: 1.783863  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 2.791645 \n",
      "\n",
      "loss: 2.779167  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.983710 \n",
      "\n",
      "loss: 1.929829  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.827350 \n",
      "\n",
      "loss: 2.684143  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.509896 \n",
      "\n",
      "loss: 1.498915  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.311216 \n",
      "\n",
      "loss: 2.386211  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 2.324718 \n",
      "\n",
      "loss: 2.208665  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 2.684441 \n",
      "\n",
      "loss: 2.586959  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.266922 \n",
      "\n",
      "loss: 1.264139  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.419456 \n",
      "\n",
      "loss: 2.403866  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 42.5%, Avg loss: 2.230993 \n",
      "\n",
      "loss: 2.247330  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.198501 \n",
      "\n",
      "loss: 1.186241  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.620389 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 83/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [164, 252, 137], 'dacay': 0.023324122018216768, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.23257048831926683}\n",
      "loss: 1.102059  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.828770 \n",
      "\n",
      "loss: 0.856194  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.754267 \n",
      "\n",
      "loss: 0.767373  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.704416 \n",
      "\n",
      "loss: 0.676579  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.682164 \n",
      "\n",
      "loss: 0.696036  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.666404 \n",
      "\n",
      "loss: 0.688871  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.646950 \n",
      "\n",
      "loss: 0.633739  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.628000 \n",
      "\n",
      "loss: 0.641773  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.639073 \n",
      "\n",
      "loss: 0.641389  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.633651 \n",
      "\n",
      "loss: 0.607760  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.624032 \n",
      "\n",
      "loss: 0.596311  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.611623 \n",
      "\n",
      "loss: 0.596772  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.628893 \n",
      "\n",
      "loss: 0.678177  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.642143 \n",
      "\n",
      "loss: 0.637780  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.612494 \n",
      "\n",
      "loss: 0.633759  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.626003 \n",
      "\n",
      "loss: 0.624326  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.596384 \n",
      "\n",
      "loss: 0.587355  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.593621 \n",
      "\n",
      "loss: 0.595988  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.604435 \n",
      "\n",
      "loss: 0.602579  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.605045 \n",
      "\n",
      "loss: 0.611207  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.625002 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 84/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [385, 179], 'dacay': 0.10701963434964804, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.4823639144397404}\n",
      "loss: 1.097847  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.872496 \n",
      "\n",
      "loss: 0.881442  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.857226 \n",
      "\n",
      "loss: 0.851798  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.857461 \n",
      "\n",
      "loss: 0.870984  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.864699 \n",
      "\n",
      "loss: 0.854892  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.859236 \n",
      "\n",
      "loss: 0.835877  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.864150 \n",
      "\n",
      "loss: 0.854635  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.850602 \n",
      "\n",
      "loss: 0.835195  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.862469 \n",
      "\n",
      "loss: 0.869455  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.864334 \n",
      "\n",
      "loss: 0.874892  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.853550 \n",
      "\n",
      "loss: 0.845328  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.858712 \n",
      "\n",
      "loss: 0.875487  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.853774 \n",
      "\n",
      "loss: 0.863619  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.858244 \n",
      "\n",
      "loss: 0.859916  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.861529 \n",
      "\n",
      "loss: 0.842003  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.858202 \n",
      "\n",
      "loss: 0.861794  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.849720 \n",
      "\n",
      "loss: 0.850413  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.855878 \n",
      "\n",
      "loss: 0.858577  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.864444 \n",
      "\n",
      "loss: 0.881592  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.859903 \n",
      "\n",
      "loss: 0.873759  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.857496 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 85/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [600], 'dacay': 0.00013439779003988052, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.198387555314162}\n",
      "loss: 1.100062  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 8.836133 \n",
      "\n",
      "loss: 5.905218  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 1.716414 \n",
      "\n",
      "loss: 1.287821  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.663347 \n",
      "\n",
      "loss: 0.673991  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.692460 \n",
      "\n",
      "loss: 0.617253  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.484738 \n",
      "\n",
      "loss: 0.435421  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.585172 \n",
      "\n",
      "loss: 0.556973  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.463629 \n",
      "\n",
      "loss: 0.491677  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.463207 \n",
      "\n",
      "loss: 0.443272  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.461742 \n",
      "\n",
      "loss: 0.459319  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.456223 \n",
      "\n",
      "loss: 0.399879  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.461834 \n",
      "\n",
      "loss: 0.413741  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.623486 \n",
      "\n",
      "loss: 0.635880  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.421243 \n",
      "\n",
      "loss: 0.399334  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.508649 \n",
      "\n",
      "loss: 0.473357  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.502657 \n",
      "\n",
      "loss: 0.497602  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.519734 \n",
      "\n",
      "loss: 0.570756  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.553569 \n",
      "\n",
      "loss: 0.539141  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.479104 \n",
      "\n",
      "loss: 0.519133  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.515234 \n",
      "\n",
      "loss: 0.491584  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.947313 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 86/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [211, 85, 614], 'dacay': 0.2838390075708119, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.3486267819476365}\n",
      "loss: 1.169516  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.440095 \n",
      "\n",
      "loss: 1.406238  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.227212 \n",
      "\n",
      "loss: 3.240877  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.716660 \n",
      "\n",
      "loss: 1.769452  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.559575 \n",
      "\n",
      "loss: 1.514203  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.233829 \n",
      "\n",
      "loss: 1.251822  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.139525 \n",
      "\n",
      "loss: 1.136227  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.396914 \n",
      "\n",
      "loss: 3.466553  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.127651 \n",
      "\n",
      "loss: 1.152706  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.104687 \n",
      "\n",
      "loss: 1.107375  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.471671 \n",
      "\n",
      "loss: 2.437261  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.222069 \n",
      "\n",
      "loss: 1.230888  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.106994 \n",
      "\n",
      "loss: 1.104249  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.097797 \n",
      "\n",
      "loss: 1.096869  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 2.472836 \n",
      "\n",
      "loss: 2.495993  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.446276 \n",
      "\n",
      "loss: 1.447924  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.761161 \n",
      "\n",
      "loss: 1.760312  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.097906 \n",
      "\n",
      "loss: 1.100751  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.143328 \n",
      "\n",
      "loss: 1.141773  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.258617 \n",
      "\n",
      "loss: 1.290460  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.124258 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 87/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [622, 131], 'dacay': 0.9574940480549697, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.2664193481307191}\n",
      "loss: 1.104929  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 1.091242 \n",
      "\n",
      "loss: 1.096178  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098530  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098484 \n",
      "\n",
      "loss: 1.098779  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098632 \n",
      "\n",
      "loss: 1.099031  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098762 \n",
      "\n",
      "loss: 1.098323  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098575 \n",
      "\n",
      "loss: 1.098647  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098963 \n",
      "\n",
      "loss: 1.100188  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099107 \n",
      "\n",
      "loss: 1.099248  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098692 \n",
      "\n",
      "loss: 1.098716  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098737 \n",
      "\n",
      "loss: 1.098383  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099265 \n",
      "\n",
      "loss: 1.099057  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098599 \n",
      "\n",
      "loss: 1.098371  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098436 \n",
      "\n",
      "loss: 1.099169  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098631 \n",
      "\n",
      "loss: 1.098652  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099047 \n",
      "\n",
      "loss: 1.098618  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098639 \n",
      "\n",
      "loss: 1.098667  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098551 \n",
      "\n",
      "loss: 1.099453  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098471 \n",
      "\n",
      "loss: 1.099485  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098954 \n",
      "\n",
      "loss: 1.099501  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099153 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 88/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [387, 78, 526], 'dacay': 0.17286866188273653, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.41118322595322004}\n",
      "loss: 1.096146  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 12.341064 \n",
      "\n",
      "loss: 11.656422  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 5.856385 \n",
      "\n",
      "loss: 5.669243  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.9%, Avg loss: 5.899387 \n",
      "\n",
      "loss: 5.708149  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.004621 \n",
      "\n",
      "loss: 1.008849  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 0.998919 \n",
      "\n",
      "loss: 0.931041  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 0.933354 \n",
      "\n",
      "loss: 0.932037  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.028936 \n",
      "\n",
      "loss: 1.031824  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.915112 \n",
      "\n",
      "loss: 0.944593  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.907026 \n",
      "\n",
      "loss: 0.920715  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.918114 \n",
      "\n",
      "loss: 0.913218  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.927310 \n",
      "\n",
      "loss: 0.942276  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 0.976285 \n",
      "\n",
      "loss: 0.973676  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.916223 \n",
      "\n",
      "loss: 0.914364  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 1.036130 \n",
      "\n",
      "loss: 1.018715  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.921212 \n",
      "\n",
      "loss: 0.919347  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 0.996233 \n",
      "\n",
      "loss: 0.999997  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.934733 \n",
      "\n",
      "loss: 0.940862  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.918768 \n",
      "\n",
      "loss: 0.913236  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 0.977196 \n",
      "\n",
      "loss: 0.992988  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.935011 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 89/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [451], 'dacay': 6.367043779645354, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.21780002378933774}\n",
      "loss: 1.126668  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 16.451615 \n",
      "\n",
      "loss: 16.575251  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 17.797174 \n",
      "\n",
      "loss: 18.318668  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 17.849186 \n",
      "\n",
      "loss: 16.772152  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 18.212101 \n",
      "\n",
      "loss: 18.764242  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 16.822400 \n",
      "\n",
      "loss: 16.792694  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 19.324501 \n",
      "\n",
      "loss: 20.310040  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 16.398108 \n",
      "\n",
      "loss: 16.976904  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 16.989006 \n",
      "\n",
      "loss: 15.845988  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 16.702900 \n",
      "\n",
      "loss: 17.061554  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 19.303208 \n",
      "\n",
      "loss: 20.215582  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 17.487835 \n",
      "\n",
      "loss: 18.333006  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 18.304412 \n",
      "\n",
      "loss: 18.712786  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 16.508975 \n",
      "\n",
      "loss: 16.991005  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 17.221388 \n",
      "\n",
      "loss: 17.348726  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 16.328270 \n",
      "\n",
      "loss: 15.943477  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 17.559843 \n",
      "\n",
      "loss: 17.016935  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 16.801522 \n",
      "\n",
      "loss: 16.618521  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 17.387933 \n",
      "\n",
      "loss: 16.510111  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 17.385308 \n",
      "\n",
      "loss: 17.321138  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 16.639418 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 90/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [542, 284, 638], 'dacay': 1.0989959552167183, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.21784368905264676}\n",
      "loss: 1.102177  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.108045 \n",
      "\n",
      "loss: 2.214684  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 2.494908 \n",
      "\n",
      "loss: 2.539651  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 6.682204 \n",
      "\n",
      "loss: 6.912519  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 6.903312 \n",
      "\n",
      "loss: 6.969739  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 8.008625 \n",
      "\n",
      "loss: 8.231840  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.480090 \n",
      "\n",
      "loss: 4.434471  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 12.368962 \n",
      "\n",
      "loss: 12.404358  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 5.998315 \n",
      "\n",
      "loss: 6.033450  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 8.253017 \n",
      "\n",
      "loss: 8.006959  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.548852 \n",
      "\n",
      "loss: 2.633872  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.238044 \n",
      "\n",
      "loss: 4.497520  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 10.514413 \n",
      "\n",
      "loss: 10.117002  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 4.715405 \n",
      "\n",
      "loss: 4.952591  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 6.064668 \n",
      "\n",
      "loss: 6.041401  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 6.499433 \n",
      "\n",
      "loss: 6.552723  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.517762 \n",
      "\n",
      "loss: 2.424777  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 8.561752 \n",
      "\n",
      "loss: 8.598632  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.886753 \n",
      "\n",
      "loss: 5.123962  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 26.239386 \n",
      "\n",
      "loss: 25.903753  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 7.715392 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 91/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [614, 282, 338], 'dacay': 1.8137014495448143, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.4408274288829857}\n",
      "loss: 1.101138  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.379583 \n",
      "\n",
      "loss: 1.461074  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 1.044347 \n",
      "\n",
      "loss: 1.011557  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 1.114420 \n",
      "\n",
      "loss: 1.096519  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.036472 \n",
      "\n",
      "loss: 1.026013  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.100241 \n",
      "\n",
      "loss: 1.100931  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.103250 \n",
      "\n",
      "loss: 1.102588  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.103275 \n",
      "\n",
      "loss: 1.104927  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099031 \n",
      "\n",
      "loss: 1.099605  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099760 \n",
      "\n",
      "loss: 1.096840  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.100258 \n",
      "\n",
      "loss: 1.100151  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098577 \n",
      "\n",
      "loss: 1.098567  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098780 \n",
      "\n",
      "loss: 1.098646  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098665 \n",
      "\n",
      "loss: 1.098816  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098846 \n",
      "\n",
      "loss: 1.097788  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099049 \n",
      "\n",
      "loss: 1.098381  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099052 \n",
      "\n",
      "loss: 1.100082  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098802 \n",
      "\n",
      "loss: 1.098551  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099078 \n",
      "\n",
      "loss: 1.099676  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099152 \n",
      "\n",
      "loss: 1.099888  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098525 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 92/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [483], 'dacay': 0.0706721369322971, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.014447128885267781}\n",
      "loss: 1.151345  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.900825 \n",
      "\n",
      "loss: 0.904640  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.892637 \n",
      "\n",
      "loss: 0.883687  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.891549 \n",
      "\n",
      "loss: 0.885300  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.886597 \n",
      "\n",
      "loss: 0.889806  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.890824 \n",
      "\n",
      "loss: 0.876030  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.907991 \n",
      "\n",
      "loss: 0.926897  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.871236 \n",
      "\n",
      "loss: 0.859938  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.890475 \n",
      "\n",
      "loss: 0.886765  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.892247 \n",
      "\n",
      "loss: 0.888434  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 0.903317 \n",
      "\n",
      "loss: 0.905005  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 0.916794 \n",
      "\n",
      "loss: 0.923441  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.883086 \n",
      "\n",
      "loss: 0.879793  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 0.927072 \n",
      "\n",
      "loss: 0.922527  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.941207 \n",
      "\n",
      "loss: 0.923820  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.907778 \n",
      "\n",
      "loss: 0.904690  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 0.940744 \n",
      "\n",
      "loss: 0.953830  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.887877 \n",
      "\n",
      "loss: 0.883979  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.907275 \n",
      "\n",
      "loss: 0.922780  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.896435 \n",
      "\n",
      "loss: 0.893767  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.946242 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 93/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [194, 507], 'dacay': 0.0011497389674291023, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.1399012781699892}\n",
      "loss: 1.104806  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 4.227327 \n",
      "\n",
      "loss: 4.018438  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 2.903375 \n",
      "\n",
      "loss: 3.034611  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.646866 \n",
      "\n",
      "loss: 0.690704  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.575556 \n",
      "\n",
      "loss: 0.525157  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.561006 \n",
      "\n",
      "loss: 0.548877  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.584652 \n",
      "\n",
      "loss: 0.553673  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.578704 \n",
      "\n",
      "loss: 0.602875  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.485944 \n",
      "\n",
      "loss: 0.458465  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.449471 \n",
      "\n",
      "loss: 0.396912  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.467432 \n",
      "\n",
      "loss: 0.483570  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.446067 \n",
      "\n",
      "loss: 0.424812  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.465628 \n",
      "\n",
      "loss: 0.447129  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.474865 \n",
      "\n",
      "loss: 0.471206  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.458593 \n",
      "\n",
      "loss: 0.451166  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.462601 \n",
      "\n",
      "loss: 0.469366  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.425316 \n",
      "\n",
      "loss: 0.433373  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.542740 \n",
      "\n",
      "loss: 0.537714  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.428058 \n",
      "\n",
      "loss: 0.379009  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.521518 \n",
      "\n",
      "loss: 0.470856  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.457905 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 94/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [290, 577, 18], 'dacay': 0.038294487226139456, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.1207538895813027}\n",
      "loss: 1.142349  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 1.057639 \n",
      "\n",
      "loss: 1.062351  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.208028 \n",
      "\n",
      "loss: 1.229484  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 0.926760 \n",
      "\n",
      "loss: 0.910167  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.936091 \n",
      "\n",
      "loss: 0.937361  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 1.122869 \n",
      "\n",
      "loss: 1.110819  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.881629 \n",
      "\n",
      "loss: 0.899423  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.882382 \n",
      "\n",
      "loss: 0.883141  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.888725 \n",
      "\n",
      "loss: 0.876009  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.884703 \n",
      "\n",
      "loss: 0.876737  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.919339 \n",
      "\n",
      "loss: 0.935246  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.894520 \n",
      "\n",
      "loss: 0.884032  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.902025 \n",
      "\n",
      "loss: 0.935918  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.863909 \n",
      "\n",
      "loss: 0.855897  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.889473 \n",
      "\n",
      "loss: 0.857346  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.857025 \n",
      "\n",
      "loss: 0.871612  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.863732 \n",
      "\n",
      "loss: 0.857608  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.922411 \n",
      "\n",
      "loss: 0.928825  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.892120 \n",
      "\n",
      "loss: 0.910682  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.912125 \n",
      "\n",
      "loss: 0.912812  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.872859 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 95/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [508, 576], 'dacay': 0.6177649545839605, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.22414245733832672}\n",
      "loss: 1.182194  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.455696 \n",
      "\n",
      "loss: 8.015444  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 12.092243 \n",
      "\n",
      "loss: 11.662010  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 4.835648 \n",
      "\n",
      "loss: 5.249427  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 9.085895 \n",
      "\n",
      "loss: 8.827791  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.544076 \n",
      "\n",
      "loss: 1.561195  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 32.591047 \n",
      "\n",
      "loss: 32.561584  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 15.130884 \n",
      "\n",
      "loss: 14.939334  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 19.662788 \n",
      "\n",
      "loss: 19.865131  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 16.895515 \n",
      "\n",
      "loss: 16.948364  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 10.460166 \n",
      "\n",
      "loss: 9.937978  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 16.414291 \n",
      "\n",
      "loss: 16.464291  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 7.618819 \n",
      "\n",
      "loss: 7.453606  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 8.764230 \n",
      "\n",
      "loss: 8.904571  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 10.409969 \n",
      "\n",
      "loss: 10.484673  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 4.789217 \n",
      "\n",
      "loss: 4.740888  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.197610 \n",
      "\n",
      "loss: 4.077641  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 7.624305 \n",
      "\n",
      "loss: 8.063435  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 3.020722 \n",
      "\n",
      "loss: 3.040580  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 6.309390 \n",
      "\n",
      "loss: 6.534369  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 4.732786 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 96/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [103], 'dacay': 0.0029592244500616827, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.15317239446203118}\n",
      "loss: 1.115583  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.867898 \n",
      "\n",
      "loss: 0.870649  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.827805 \n",
      "\n",
      "loss: 0.841003  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.803295 \n",
      "\n",
      "loss: 0.823157  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.800334 \n",
      "\n",
      "loss: 0.779908  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.782961 \n",
      "\n",
      "loss: 0.790831  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.772100 \n",
      "\n",
      "loss: 0.787120  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.755517 \n",
      "\n",
      "loss: 0.809394  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.754497 \n",
      "\n",
      "loss: 0.757860  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.759102 \n",
      "\n",
      "loss: 0.762198  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.751727 \n",
      "\n",
      "loss: 0.751683  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.749416 \n",
      "\n",
      "loss: 0.725972  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.735092 \n",
      "\n",
      "loss: 0.750728  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.734977 \n",
      "\n",
      "loss: 0.769499  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.742853 \n",
      "\n",
      "loss: 0.710058  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.736472 \n",
      "\n",
      "loss: 0.737652  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.735274 \n",
      "\n",
      "loss: 0.716871  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.723527 \n",
      "\n",
      "loss: 0.744187  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.723086 \n",
      "\n",
      "loss: 0.714196  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.733664 \n",
      "\n",
      "loss: 0.730586  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.729196 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 97/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [385, 492, 574], 'dacay': 0.045144999830516286, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.17597008275432274}\n",
      "loss: 1.109609  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.381436 \n",
      "\n",
      "loss: 6.524984  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 5.400782 \n",
      "\n",
      "loss: 5.591910  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.475880 \n",
      "\n",
      "loss: 6.469838  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 8.350814 \n",
      "\n",
      "loss: 8.177182  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 5.436099 \n",
      "\n",
      "loss: 5.410915  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.019172 \n",
      "\n",
      "loss: 3.817867  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 7.313866 \n",
      "\n",
      "loss: 7.421505  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 6.848305 \n",
      "\n",
      "loss: 7.223243  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 11.300715 \n",
      "\n",
      "loss: 11.201889  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 15.498387 \n",
      "\n",
      "loss: 15.042478  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 5.623217 \n",
      "\n",
      "loss: 5.626678  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.117002 \n",
      "\n",
      "loss: 5.145945  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 11.485940 \n",
      "\n",
      "loss: 12.068063  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 4.670159 \n",
      "\n",
      "loss: 4.789902  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 10.708471 \n",
      "\n",
      "loss: 10.975993  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 13.200757 \n",
      "\n",
      "loss: 13.037170  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 9.833102 \n",
      "\n",
      "loss: 9.979822  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.289755 \n",
      "\n",
      "loss: 2.293231  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 14.264210 \n",
      "\n",
      "loss: 14.520535  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.947761 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 98/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [285, 321], 'dacay': 0.0018072506141554024, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.15784241720859266}\n",
      "loss: 1.120325  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.764552 \n",
      "\n",
      "loss: 0.740963  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.679683 \n",
      "\n",
      "loss: 0.694091  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.649868 \n",
      "\n",
      "loss: 0.696814  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.609845 \n",
      "\n",
      "loss: 0.611801  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.595422 \n",
      "\n",
      "loss: 0.578663  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.561766 \n",
      "\n",
      "loss: 0.563064  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.528911 \n",
      "\n",
      "loss: 0.557923  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.518780 \n",
      "\n",
      "loss: 0.520716  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.520912 \n",
      "\n",
      "loss: 0.474568  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.501774 \n",
      "\n",
      "loss: 0.478125  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.507378 \n",
      "\n",
      "loss: 0.480301  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.455193 \n",
      "\n",
      "loss: 0.456781  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.493577 \n",
      "\n",
      "loss: 0.464837  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.582230 \n",
      "\n",
      "loss: 0.562695  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.471518 \n",
      "\n",
      "loss: 0.495065  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.429709 \n",
      "\n",
      "loss: 0.395148  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.420988 \n",
      "\n",
      "loss: 0.450142  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.417750 \n",
      "\n",
      "loss: 0.451948  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.437209 \n",
      "\n",
      "loss: 0.457726  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.395310 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 99/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [556, 197, 204], 'dacay': 0.0012076002642738932, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.3623200420464165}\n",
      "loss: 1.109795  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.219572 \n",
      "\n",
      "loss: 3.262246  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.100243 \n",
      "\n",
      "loss: 4.194773  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.847721 \n",
      "\n",
      "loss: 1.922194  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 11.061830 \n",
      "\n",
      "loss: 11.639174  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 11.438915 \n",
      "\n",
      "loss: 11.404137  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.378782 \n",
      "\n",
      "loss: 3.497008  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 6.373997 \n",
      "\n",
      "loss: 6.259677  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.504277 \n",
      "\n",
      "loss: 1.508645  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.707464 \n",
      "\n",
      "loss: 1.704275  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 3.025019 \n",
      "\n",
      "loss: 2.845900  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.879934 \n",
      "\n",
      "loss: 3.931186  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.410575 \n",
      "\n",
      "loss: 3.403304  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 6.584248 \n",
      "\n",
      "loss: 6.468412  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.353715 \n",
      "\n",
      "loss: 2.395292  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.500844 \n",
      "\n",
      "loss: 1.504633  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.332450 \n",
      "\n",
      "loss: 2.289535  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 6.514724 \n",
      "\n",
      "loss: 6.427962  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.357903 \n",
      "\n",
      "loss: 1.326094  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.121466 \n",
      "\n",
      "loss: 1.123423  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.911436 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 100/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [619, 297, 110], 'dacay': 1.4559344308862883, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.18050444646777827}\n",
      "loss: 1.101606  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098635  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098562 \n",
      "\n",
      "loss: 1.098812  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098609 \n",
      "\n",
      "loss: 1.098528  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098593 \n",
      "\n",
      "loss: 1.098670  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098727  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098648 \n",
      "\n",
      "loss: 1.098450  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098660 \n",
      "\n",
      "loss: 1.098716  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098720 \n",
      "\n",
      "loss: 1.098646  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098486  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098572 \n",
      "\n",
      "loss: 1.098703  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098580 \n",
      "\n",
      "loss: 1.098591  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098667 \n",
      "\n",
      "loss: 1.098722  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098592 \n",
      "\n",
      "loss: 1.098535  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098635 \n",
      "\n",
      "loss: 1.098705  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098605 \n",
      "\n",
      "loss: 1.098624  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098633 \n",
      "\n",
      "loss: 1.098461  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098593 \n",
      "\n",
      "loss: 1.098540  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098507 \n",
      "\n",
      "loss: 1.098534  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098626 \n",
      "\n",
      "loss: 1.098702  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098682 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 101/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [22, 554], 'dacay': 0.0004294521677148936, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.2767241475026243}\n",
      "loss: 1.101168  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.736627 \n",
      "\n",
      "loss: 0.725978  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.660940 \n",
      "\n",
      "loss: 0.663716  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.603221 \n",
      "\n",
      "loss: 0.562787  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.582122 \n",
      "\n",
      "loss: 0.590904  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.583964 \n",
      "\n",
      "loss: 0.603164  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.579664 \n",
      "\n",
      "loss: 0.561262  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.527599 \n",
      "\n",
      "loss: 0.552101  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.551669 \n",
      "\n",
      "loss: 0.531314  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.583232 \n",
      "\n",
      "loss: 0.589407  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.511758 \n",
      "\n",
      "loss: 0.520105  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.571220 \n",
      "\n",
      "loss: 0.566097  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.484453 \n",
      "\n",
      "loss: 0.479223  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.507779 \n",
      "\n",
      "loss: 0.513089  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.481037 \n",
      "\n",
      "loss: 0.447890  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.492745 \n",
      "\n",
      "loss: 0.473462  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.486439 \n",
      "\n",
      "loss: 0.474278  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.482282 \n",
      "\n",
      "loss: 0.468334  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.456886 \n",
      "\n",
      "loss: 0.437455  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.462543 \n",
      "\n",
      "loss: 0.435798  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.449688 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 102/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [137, 510, 113], 'dacay': 2.3099292327020464, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.23940923288233837}\n",
      "loss: 1.096291  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099031 \n",
      "\n",
      "loss: 1.098668  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098775 \n",
      "\n",
      "loss: 1.098572  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098704 \n",
      "\n",
      "loss: 1.098777  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098843 \n",
      "\n",
      "loss: 1.099936  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098494  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098828 \n",
      "\n",
      "loss: 1.099094  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098807 \n",
      "\n",
      "loss: 1.098213  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098910 \n",
      "\n",
      "loss: 1.098846  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098599 \n",
      "\n",
      "loss: 1.098590  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098516 \n",
      "\n",
      "loss: 1.098447  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098586 \n",
      "\n",
      "loss: 1.098674  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098544 \n",
      "\n",
      "loss: 1.098469  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098846 \n",
      "\n",
      "loss: 1.098818  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098696 \n",
      "\n",
      "loss: 1.098418  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099007 \n",
      "\n",
      "loss: 1.098820  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098592 \n",
      "\n",
      "loss: 1.098823  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099110 \n",
      "\n",
      "loss: 1.098849  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098633 \n",
      "\n",
      "loss: 1.097900  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098822 \n",
      "\n",
      "loss: 1.099345  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098540 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 103/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [196, 60], 'dacay': 0.0018297508284524288, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.41107561108518526}\n",
      "loss: 1.143918  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 0.966145 \n",
      "\n",
      "loss: 0.970902  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.855380 \n",
      "\n",
      "loss: 0.846670  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.915385 \n",
      "\n",
      "loss: 0.921343  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.857076 \n",
      "\n",
      "loss: 0.872022  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.821576 \n",
      "\n",
      "loss: 0.792064  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.817207 \n",
      "\n",
      "loss: 0.822818  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.806653 \n",
      "\n",
      "loss: 0.822738  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.814477 \n",
      "\n",
      "loss: 0.840106  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.825287 \n",
      "\n",
      "loss: 0.807133  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.764587 \n",
      "\n",
      "loss: 0.731700  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.766959 \n",
      "\n",
      "loss: 0.755638  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.805979 \n",
      "\n",
      "loss: 0.814682  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.753742 \n",
      "\n",
      "loss: 0.770432  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.735465 \n",
      "\n",
      "loss: 0.741307  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.796990 \n",
      "\n",
      "loss: 0.807342  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.730871 \n",
      "\n",
      "loss: 0.702857  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.751415 \n",
      "\n",
      "loss: 0.792199  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.741550 \n",
      "\n",
      "loss: 0.731177  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.744301 \n",
      "\n",
      "loss: 0.706808  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.793300 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 104/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [511, 83], 'dacay': 0.07675484205681223, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.07468387247838022}\n",
      "loss: 1.092858  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.872796 \n",
      "\n",
      "loss: 0.876842  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.838711 \n",
      "\n",
      "loss: 0.861456  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.827357 \n",
      "\n",
      "loss: 0.820933  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.825859 \n",
      "\n",
      "loss: 0.801149  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.815596 \n",
      "\n",
      "loss: 0.830038  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.812663 \n",
      "\n",
      "loss: 0.795821  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.807197 \n",
      "\n",
      "loss: 0.803195  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.807228 \n",
      "\n",
      "loss: 0.812665  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.811736 \n",
      "\n",
      "loss: 0.806375  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.806629 \n",
      "\n",
      "loss: 0.796123  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.807313 \n",
      "\n",
      "loss: 0.822774  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.801218 \n",
      "\n",
      "loss: 0.790066  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.806122 \n",
      "\n",
      "loss: 0.814392  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.810583 \n",
      "\n",
      "loss: 0.795896  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.806058 \n",
      "\n",
      "loss: 0.805081  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.809116 \n",
      "\n",
      "loss: 0.795497  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.801521 \n",
      "\n",
      "loss: 0.804801  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.806686 \n",
      "\n",
      "loss: 0.802336  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.811903 \n",
      "\n",
      "loss: 0.810698  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.807486 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 105/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [319, 95, 262], 'dacay': 0.00039107298029450585, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.38786920049513884}\n",
      "loss: 1.128643  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.120053 \n",
      "\n",
      "loss: 1.117056  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.120019 \n",
      "\n",
      "loss: 1.124735  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.099477 \n",
      "\n",
      "loss: 1.103826  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 1.109473 \n",
      "\n",
      "loss: 1.106853  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.108111 \n",
      "\n",
      "loss: 1.108299  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.089056 \n",
      "\n",
      "loss: 1.088609  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.113014 \n",
      "\n",
      "loss: 1.124474  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 0.963564 \n",
      "\n",
      "loss: 0.986910  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.907676 \n",
      "\n",
      "loss: 0.895423  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.864596 \n",
      "\n",
      "loss: 0.847907  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.850236 \n",
      "\n",
      "loss: 0.865175  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.832852 \n",
      "\n",
      "loss: 0.820819  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.842204 \n",
      "\n",
      "loss: 0.827015  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.926102 \n",
      "\n",
      "loss: 0.919893  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.827244 \n",
      "\n",
      "loss: 0.815987  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.827260 \n",
      "\n",
      "loss: 0.840456  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.803675 \n",
      "\n",
      "loss: 0.788007  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.807064 \n",
      "\n",
      "loss: 0.829363  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.807286 \n",
      "\n",
      "loss: 0.820952  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.811417 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 106/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [469, 461], 'dacay': 0.0030673664465682366, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.28435787236505417}\n",
      "loss: 1.107778  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 10.434409 \n",
      "\n",
      "loss: 10.541281  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 2.793899 \n",
      "\n",
      "loss: 2.969844  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.190405 \n",
      "\n",
      "loss: 1.202803  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 7.520535 \n",
      "\n",
      "loss: 7.390801  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 4.499611 \n",
      "\n",
      "loss: 4.634565  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 6.186452 \n",
      "\n",
      "loss: 6.440916  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 15.292866 \n",
      "\n",
      "loss: 14.665666  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 5.562106 \n",
      "\n",
      "loss: 5.473365  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 11.563078 \n",
      "\n",
      "loss: 11.099925  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 7.215888 \n",
      "\n",
      "loss: 7.118373  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 4.637681 \n",
      "\n",
      "loss: 4.580370  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 5.591293 \n",
      "\n",
      "loss: 5.713813  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 10.122061 \n",
      "\n",
      "loss: 9.458652  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 10.763645 \n",
      "\n",
      "loss: 10.636971  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 10.617282 \n",
      "\n",
      "loss: 10.750868  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 17.108417 \n",
      "\n",
      "loss: 15.818602  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.703600 \n",
      "\n",
      "loss: 8.642798  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 8.620437 \n",
      "\n",
      "loss: 8.701639  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 35.7%, Avg loss: 4.584193 \n",
      "\n",
      "loss: 4.785647  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 6.884534 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 107/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [632], 'dacay': 6.317480166924578, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.31279496370026094}\n",
      "loss: 1.164118  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 23.4%, Avg loss: 116971550610552619008.000000 \n",
      "\n",
      "loss: 102009179183941419008.000000  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 108/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [91, 168], 'dacay': 0.0015824350304990762, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.0014375937291825778}\n",
      "loss: 1.098524  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.091553 \n",
      "\n",
      "loss: 1.093869  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 45.5%, Avg loss: 1.085105 \n",
      "\n",
      "loss: 1.086667  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 1.080312 \n",
      "\n",
      "loss: 1.081251  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.074573 \n",
      "\n",
      "loss: 1.072119  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.068024 \n",
      "\n",
      "loss: 1.067271  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.062015 \n",
      "\n",
      "loss: 1.063600  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.056871 \n",
      "\n",
      "loss: 1.061677  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 1.053326 \n",
      "\n",
      "loss: 1.052175  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.047779 \n",
      "\n",
      "loss: 1.045219  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 1.041267 \n",
      "\n",
      "loss: 1.041967  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 1.034946 \n",
      "\n",
      "loss: 1.037771  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 1.030279 \n",
      "\n",
      "loss: 1.035438  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.027302 \n",
      "\n",
      "loss: 1.034484  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.021551 \n",
      "\n",
      "loss: 1.013699  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.014826 \n",
      "\n",
      "loss: 1.013331  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.008304 \n",
      "\n",
      "loss: 1.014145  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.003850 \n",
      "\n",
      "loss: 1.011787  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 1.001342 \n",
      "\n",
      "loss: 1.004146  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.995565 \n",
      "\n",
      "loss: 0.990307  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.988287 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 109/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [616, 489], 'dacay': 0.002859541772530681, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.35350721935652873}\n",
      "loss: 1.103983  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 2.252239 \n",
      "\n",
      "loss: 3.221599  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.244392 \n",
      "\n",
      "loss: 1.240749  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 172.277062 \n",
      "\n",
      "loss: 198.740372  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 233.749968 \n",
      "\n",
      "loss: 234.877411  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.1%, Avg loss: 1.099531 \n",
      "\n",
      "loss: 1.095138  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 42.2%, Avg loss: 21.725813 \n",
      "\n",
      "loss: 23.872425  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.100063 \n",
      "\n",
      "loss: 1.099904  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.099466 \n",
      "\n",
      "loss: 1.099303  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.130559 \n",
      "\n",
      "loss: 1.128940  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.178530 \n",
      "\n",
      "loss: 1.181535  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.109500 \n",
      "\n",
      "loss: 1.106874  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 1.110216 \n",
      "\n",
      "loss: 1.110823  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.103411 \n",
      "\n",
      "loss: 1.098647  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 10.730880 \n",
      "\n",
      "loss: 11.141614  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 1.071474 \n",
      "\n",
      "loss: 1.064207  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 1.055388 \n",
      "\n",
      "loss: 1.128807  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 1.190819 \n",
      "\n",
      "loss: 1.207004  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.793783 \n",
      "\n",
      "loss: 0.752844  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.693022 \n",
      "\n",
      "loss: 0.650745  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 1.788305 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 110/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [472, 80, 247], 'dacay': 4.164426701367751, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.05747667360027678}\n",
      "loss: 1.109872  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.124844 \n",
      "\n",
      "loss: 1.121606  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.106541 \n",
      "\n",
      "loss: 1.107761  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.120131 \n",
      "\n",
      "loss: 1.122388  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.176455 \n",
      "\n",
      "loss: 1.205872  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.158775 \n",
      "\n",
      "loss: 1.168240  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.167991 \n",
      "\n",
      "loss: 1.171421  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.102152 \n",
      "\n",
      "loss: 1.107533  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.191453 \n",
      "\n",
      "loss: 1.186725  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.099682 \n",
      "\n",
      "loss: 1.100619  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.338241 \n",
      "\n",
      "loss: 1.363329  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.122236 \n",
      "\n",
      "loss: 1.115767  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.116320 \n",
      "\n",
      "loss: 1.112065  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.161591 \n",
      "\n",
      "loss: 1.171710  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.197146 \n",
      "\n",
      "loss: 1.195978  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.102365 \n",
      "\n",
      "loss: 1.101255  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.203490 \n",
      "\n",
      "loss: 1.186202  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.303535 \n",
      "\n",
      "loss: 1.269870  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.130035 \n",
      "\n",
      "loss: 1.135797  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.103792 \n",
      "\n",
      "loss: 1.101536  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.124297 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 111/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [160], 'dacay': 3.483832177296059, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.029913055952972143}\n",
      "loss: 1.103552  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.113121 \n",
      "\n",
      "loss: 1.111766  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.101683 \n",
      "\n",
      "loss: 1.103834  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.102632 \n",
      "\n",
      "loss: 1.101400  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.100363 \n",
      "\n",
      "loss: 1.102352  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.105666 \n",
      "\n",
      "loss: 1.109334  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099320 \n",
      "\n",
      "loss: 1.098962  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098868 \n",
      "\n",
      "loss: 1.097500  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098948 \n",
      "\n",
      "loss: 1.097170  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.100554 \n",
      "\n",
      "loss: 1.096377  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099588 \n",
      "\n",
      "loss: 1.099204  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099377 \n",
      "\n",
      "loss: 1.102766  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.102355 \n",
      "\n",
      "loss: 1.102908  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098577 \n",
      "\n",
      "loss: 1.099754  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.101791 \n",
      "\n",
      "loss: 1.097508  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.099025 \n",
      "\n",
      "loss: 1.100589  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.100640 \n",
      "\n",
      "loss: 1.099570  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.100847 \n",
      "\n",
      "loss: 1.098557  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.099985 \n",
      "\n",
      "loss: 1.102420  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.103688 \n",
      "\n",
      "loss: 1.106462  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099167 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 112/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [617, 590], 'dacay': 4.036106755324333, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.23440211534105332}\n",
      "loss: 1.105204  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.161575 \n",
      "\n",
      "loss: 7.929245  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.370015 \n",
      "\n",
      "loss: 8.263271  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 6.997822 \n",
      "\n",
      "loss: 7.083667  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 6.919582 \n",
      "\n",
      "loss: 6.769042  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 7.510000 \n",
      "\n",
      "loss: 7.474801  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 7.554034 \n",
      "\n",
      "loss: 7.099815  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.929559 \n",
      "\n",
      "loss: 6.587843  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.596776 \n",
      "\n",
      "loss: 8.569302  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 8.612955 \n",
      "\n",
      "loss: 8.916010  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 8.609981 \n",
      "\n",
      "loss: 8.624402  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.496247 \n",
      "\n",
      "loss: 7.919059  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 9.075518 \n",
      "\n",
      "loss: 9.262257  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 9.538714 \n",
      "\n",
      "loss: 8.978767  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 8.164562 \n",
      "\n",
      "loss: 8.231017  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 8.025399 \n",
      "\n",
      "loss: 8.127485  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.036496 \n",
      "\n",
      "loss: 7.849619  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 9.007754 \n",
      "\n",
      "loss: 9.109781  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 8.411022 \n",
      "\n",
      "loss: 8.450036  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 8.170457 \n",
      "\n",
      "loss: 8.509373  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 6.929717 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 113/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [420, 286], 'dacay': 0.03179515054130782, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.04031288176446056}\n",
      "loss: 1.101511  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.729179 \n",
      "\n",
      "loss: 1.719117  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.981998 \n",
      "\n",
      "loss: 0.980593  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 0.946950 \n",
      "\n",
      "loss: 0.933763  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 0.984101 \n",
      "\n",
      "loss: 0.983783  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 0.992545 \n",
      "\n",
      "loss: 1.004693  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.963616 \n",
      "\n",
      "loss: 0.973415  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 0.951532 \n",
      "\n",
      "loss: 0.950908  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.074816 \n",
      "\n",
      "loss: 1.057137  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 47.4%, Avg loss: 0.991027 \n",
      "\n",
      "loss: 1.002204  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 0.955859 \n",
      "\n",
      "loss: 0.948529  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 0.976995 \n",
      "\n",
      "loss: 0.975536  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 0.976387 \n",
      "\n",
      "loss: 0.982127  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.960706 \n",
      "\n",
      "loss: 0.952116  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.110006 \n",
      "\n",
      "loss: 1.111650  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 1.234481 \n",
      "\n",
      "loss: 1.221896  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.164356 \n",
      "\n",
      "loss: 2.105129  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.503171 \n",
      "\n",
      "loss: 1.508590  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.123263 \n",
      "\n",
      "loss: 1.119821  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.115055 \n",
      "\n",
      "loss: 1.117119  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.103357 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 114/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [450], 'dacay': 0.47061917310146445, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.045754310626829504}\n",
      "loss: 1.115003  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.949574 \n",
      "\n",
      "loss: 0.952714  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.978956 \n",
      "\n",
      "loss: 0.989163  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.989789 \n",
      "\n",
      "loss: 0.990045  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.993162 \n",
      "\n",
      "loss: 0.987416  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.991291 \n",
      "\n",
      "loss: 0.992645  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.993235 \n",
      "\n",
      "loss: 0.992923  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.993685 \n",
      "\n",
      "loss: 0.991251  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.994344 \n",
      "\n",
      "loss: 0.990491  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.994276 \n",
      "\n",
      "loss: 0.983731  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.992438 \n",
      "\n",
      "loss: 0.993989  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.992189 \n",
      "\n",
      "loss: 0.986256  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.992798 \n",
      "\n",
      "loss: 0.999753  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.992962 \n",
      "\n",
      "loss: 0.995391  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.993309 \n",
      "\n",
      "loss: 0.990153  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.992283 \n",
      "\n",
      "loss: 0.990617  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.993000 \n",
      "\n",
      "loss: 0.989713  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.992505 \n",
      "\n",
      "loss: 1.004717  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.993552 \n",
      "\n",
      "loss: 0.991490  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.992615 \n",
      "\n",
      "loss: 1.001657  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.991307 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 115/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [597, 609, 29], 'dacay': 9.950893641703471, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.38157819118543557}\n",
      "loss: 1.100240  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 116/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [489], 'dacay': 0.03933862762246677, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.09082642469684066}\n",
      "loss: 1.127975  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 36.8%, Avg loss: 1.249898 \n",
      "\n",
      "loss: 1.228673  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 1.163647 \n",
      "\n",
      "loss: 1.193111  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: 1.729924 \n",
      "\n",
      "loss: 1.673789  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 40.6%, Avg loss: 1.208734 \n",
      "\n",
      "loss: 1.241399  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 1.681017 \n",
      "\n",
      "loss: 1.674584  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 47.8%, Avg loss: 1.566468 \n",
      "\n",
      "loss: 1.622421  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 1.713138 \n",
      "\n",
      "loss: 1.788789  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.345354 \n",
      "\n",
      "loss: 1.383059  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 37.1%, Avg loss: 1.826711 \n",
      "\n",
      "loss: 1.835513  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 2.010183 \n",
      "\n",
      "loss: 2.001443  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.914210 \n",
      "\n",
      "loss: 0.911723  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.298143 \n",
      "\n",
      "loss: 1.322169  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.340931 \n",
      "\n",
      "loss: 1.325909  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 42.5%, Avg loss: 1.142726 \n",
      "\n",
      "loss: 1.142431  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.865417 \n",
      "\n",
      "loss: 0.859706  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.100735 \n",
      "\n",
      "loss: 1.101157  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 1.404991 \n",
      "\n",
      "loss: 1.473259  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 1.192988 \n",
      "\n",
      "loss: 1.247932  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 1.188373 \n",
      "\n",
      "loss: 1.179835  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.043915 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 117/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [454, 511, 101], 'dacay': 0.022389890766957972, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.16059702215269012}\n",
      "loss: 1.178766  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.337986 \n",
      "\n",
      "loss: 1.337773  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.121503 \n",
      "\n",
      "loss: 1.128280  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.121712 \n",
      "\n",
      "loss: 1.138616  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098891 \n",
      "\n",
      "loss: 1.098491  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.101483 \n",
      "\n",
      "loss: 1.101627  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.105063 \n",
      "\n",
      "loss: 1.103163  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.101758 \n",
      "\n",
      "loss: 1.100127  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.107690 \n",
      "\n",
      "loss: 1.109565  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.106296 \n",
      "\n",
      "loss: 1.106704  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.107605 \n",
      "\n",
      "loss: 1.110734  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.505060 \n",
      "\n",
      "loss: 1.447742  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 2.497343 \n",
      "\n",
      "loss: 2.481804  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.851549 \n",
      "\n",
      "loss: 2.815000  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.605550 \n",
      "\n",
      "loss: 1.616647  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.256570 \n",
      "\n",
      "loss: 2.197084  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.389570 \n",
      "\n",
      "loss: 1.398176  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.113979 \n",
      "\n",
      "loss: 1.119496  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.103798 \n",
      "\n",
      "loss: 1.103995  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.104242 \n",
      "\n",
      "loss: 1.107843  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.109741 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 118/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [172], 'dacay': 0.004456750459111523, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.3313255101420153}\n",
      "loss: 1.190401  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.484414 \n",
      "\n",
      "loss: 1.532797  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.203488 \n",
      "\n",
      "loss: 1.296471  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.953629 \n",
      "\n",
      "loss: 0.959727  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 1.141195 \n",
      "\n",
      "loss: 1.129427  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 1.341998 \n",
      "\n",
      "loss: 1.397344  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.793826 \n",
      "\n",
      "loss: 1.814384  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.129487 \n",
      "\n",
      "loss: 1.105970  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 47.6%, Avg loss: 2.013838 \n",
      "\n",
      "loss: 2.133261  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 2.129848 \n",
      "\n",
      "loss: 2.121898  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.631431 \n",
      "\n",
      "loss: 1.610548  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 1.404045 \n",
      "\n",
      "loss: 1.505217  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.312696 \n",
      "\n",
      "loss: 1.331881  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.214577 \n",
      "\n",
      "loss: 1.191621  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.804291 \n",
      "\n",
      "loss: 1.684117  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 1.234775 \n",
      "\n",
      "loss: 1.234180  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1.867241 \n",
      "\n",
      "loss: 1.875370  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.440897 \n",
      "\n",
      "loss: 1.392474  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 1.361661 \n",
      "\n",
      "loss: 1.416560  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.438669 \n",
      "\n",
      "loss: 1.402096  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.141315 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 119/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [571, 45], 'dacay': 0.018798774701786928, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.16170456575936973}\n",
      "loss: 1.087257  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.2%, Avg loss: 3.019249 \n",
      "\n",
      "loss: 2.771089  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.930379 \n",
      "\n",
      "loss: 0.937120  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.033465 \n",
      "\n",
      "loss: 1.064766  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.180699 \n",
      "\n",
      "loss: 1.207990  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 43.4%, Avg loss: 1.781915 \n",
      "\n",
      "loss: 1.905686  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 1.008199 \n",
      "\n",
      "loss: 1.034399  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 3.703979 \n",
      "\n",
      "loss: 3.804257  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 13.055035 \n",
      "\n",
      "loss: 12.505486  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.798340 \n",
      "\n",
      "loss: 0.774673  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.741911 \n",
      "\n",
      "loss: 0.711272  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.909078 \n",
      "\n",
      "loss: 1.971861  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.828915 \n",
      "\n",
      "loss: 0.796499  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.117477 \n",
      "\n",
      "loss: 1.070556  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 5.101790 \n",
      "\n",
      "loss: 5.027492  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 21.465178 \n",
      "\n",
      "loss: 20.342869  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 2.558614 \n",
      "\n",
      "loss: 2.765778  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 33.319577 \n",
      "\n",
      "loss: 31.095226  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.080973 \n",
      "\n",
      "loss: 0.908391  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.782540 \n",
      "\n",
      "loss: 0.783044  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.857092 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 120/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [534, 9], 'dacay': 0.19422356992916281, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.02719662227982663}\n",
      "loss: 1.108358  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099148 \n",
      "\n",
      "loss: 1.097804  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.099388 \n",
      "\n",
      "loss: 1.098462  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099128 \n",
      "\n",
      "loss: 1.098536  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.100069 \n",
      "\n",
      "loss: 1.101291  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099808 \n",
      "\n",
      "loss: 1.098939  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.100108 \n",
      "\n",
      "loss: 1.099494  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098636 \n",
      "\n",
      "loss: 1.098397  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098662 \n",
      "\n",
      "loss: 1.098175  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.099003 \n",
      "\n",
      "loss: 1.098671  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099530 \n",
      "\n",
      "loss: 1.098005  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099085 \n",
      "\n",
      "loss: 1.099560  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098868 \n",
      "\n",
      "loss: 1.098911  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099680 \n",
      "\n",
      "loss: 1.099717  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.099966 \n",
      "\n",
      "loss: 1.103129  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.099343 \n",
      "\n",
      "loss: 1.100556  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098568 \n",
      "\n",
      "loss: 1.098826  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098619 \n",
      "\n",
      "loss: 1.098711  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098879 \n",
      "\n",
      "loss: 1.101318  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.100086 \n",
      "\n",
      "loss: 1.099792  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098578 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 121/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [378, 518, 460], 'dacay': 0.6308886006121721, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.2746548975787573}\n",
      "loss: 1.101081  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 161.782555 \n",
      "\n",
      "loss: 163.677704  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 25.5%, Avg loss: 1.111727 \n",
      "\n",
      "loss: 1.107958  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 9.625209 \n",
      "\n",
      "loss: 9.754669  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 169.811176 \n",
      "\n",
      "loss: 166.617798  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 9359.231445 \n",
      "\n",
      "loss: 9371.402344  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 175.655311 \n",
      "\n",
      "loss: 163.502426  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.104068 \n",
      "\n",
      "loss: 1.105575  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 30.7%, Avg loss: 1.400975 \n",
      "\n",
      "loss: 1.377050  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 27543.725060 \n",
      "\n",
      "loss: 28850.033203  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.101685 \n",
      "\n",
      "loss: 1.098619  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.109585 \n",
      "\n",
      "loss: 1.112951  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.216579 \n",
      "\n",
      "loss: 1.099098  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098593 \n",
      "\n",
      "loss: 1.099102  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.092634 \n",
      "\n",
      "loss: 1.092299  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.029978 \n",
      "\n",
      "loss: 1.036477  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.235433 \n",
      "\n",
      "loss: 1.263482  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 1.227465 \n",
      "\n",
      "loss: 1.224675  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.040717 \n",
      "\n",
      "loss: 1.939897  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1167.491502 \n",
      "\n",
      "loss: 1118.399536  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 36.3%, Avg loss: 9.827025 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 122/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [456, 504], 'dacay': 0.00040469740238609464, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.1554439941828282}\n",
      "loss: 1.195920  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.891804 \n",
      "\n",
      "loss: 0.884405  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.935278 \n",
      "\n",
      "loss: 0.936126  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.837735 \n",
      "\n",
      "loss: 0.825447  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.858130 \n",
      "\n",
      "loss: 0.874872  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.776941 \n",
      "\n",
      "loss: 0.780904  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.782809 \n",
      "\n",
      "loss: 0.779176  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.746426 \n",
      "\n",
      "loss: 0.754133  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.771674 \n",
      "\n",
      "loss: 0.733343  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.748152 \n",
      "\n",
      "loss: 0.719589  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.731714 \n",
      "\n",
      "loss: 0.739475  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.728412 \n",
      "\n",
      "loss: 0.734222  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.709594 \n",
      "\n",
      "loss: 0.694348  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.706923 \n",
      "\n",
      "loss: 0.710897  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.733686 \n",
      "\n",
      "loss: 0.698058  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.703440 \n",
      "\n",
      "loss: 0.701335  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.689756 \n",
      "\n",
      "loss: 0.683961  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.675401 \n",
      "\n",
      "loss: 0.659281  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.653865 \n",
      "\n",
      "loss: 0.659715  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.661140 \n",
      "\n",
      "loss: 0.652334  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.657702 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 123/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [573], 'dacay': 0.01096360641849842, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.23077780229997383}\n",
      "loss: 1.175546  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 6.324764 \n",
      "\n",
      "loss: 6.305958  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 2.663664 \n",
      "\n",
      "loss: 3.027457  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 3.994291 \n",
      "\n",
      "loss: 4.307875  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 4.313917 \n",
      "\n",
      "loss: 4.085091  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 4.302481 \n",
      "\n",
      "loss: 4.499136  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 1.305770 \n",
      "\n",
      "loss: 1.269633  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 1.771863 \n",
      "\n",
      "loss: 1.676134  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 2.081298 \n",
      "\n",
      "loss: 2.014212  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 2.367241 \n",
      "\n",
      "loss: 2.431094  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.257096 \n",
      "\n",
      "loss: 1.257405  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 2.756189 \n",
      "\n",
      "loss: 2.704656  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 2.468203 \n",
      "\n",
      "loss: 2.539952  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 2.269716 \n",
      "\n",
      "loss: 2.216570  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.571766 \n",
      "\n",
      "loss: 1.397947  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 1.047890 \n",
      "\n",
      "loss: 1.077327  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 2.356830 \n",
      "\n",
      "loss: 2.319447  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 4.306738 \n",
      "\n",
      "loss: 4.215443  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 2.016524 \n",
      "\n",
      "loss: 2.012619  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.076461 \n",
      "\n",
      "loss: 1.090042  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 3.081438 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 124/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [165], 'dacay': 0.03103684187021479, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.20780362243338296}\n",
      "loss: 1.141862  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.862252 \n",
      "\n",
      "loss: 0.869949  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.725173 \n",
      "\n",
      "loss: 0.729610  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.787794 \n",
      "\n",
      "loss: 0.807588  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.780697 \n",
      "\n",
      "loss: 0.797239  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.782289 \n",
      "\n",
      "loss: 0.768188  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.814600 \n",
      "\n",
      "loss: 0.792060  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.792061 \n",
      "\n",
      "loss: 0.804777  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.755525 \n",
      "\n",
      "loss: 0.801004  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.796832 \n",
      "\n",
      "loss: 0.820698  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.825151 \n",
      "\n",
      "loss: 0.794267  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.783753 \n",
      "\n",
      "loss: 0.757787  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.826126 \n",
      "\n",
      "loss: 0.815334  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.768042 \n",
      "\n",
      "loss: 0.754232  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.793147 \n",
      "\n",
      "loss: 0.810166  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.791733 \n",
      "\n",
      "loss: 0.778911  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 63.593973 \n",
      "\n",
      "loss: 64.185936  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 44.794797 \n",
      "\n",
      "loss: 45.433479  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 4.955901 \n",
      "\n",
      "loss: 4.842380  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.782373 \n",
      "\n",
      "loss: 0.787853  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.724962 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 125/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [491, 257], 'dacay': 0.019227922529544618, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.09538612443686938}\n",
      "loss: 1.112662  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 3.454222 \n",
      "\n",
      "loss: 3.391450  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 2.833896 \n",
      "\n",
      "loss: 2.810801  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 5.173787 \n",
      "\n",
      "loss: 5.145955  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 47.8%, Avg loss: 1.158734 \n",
      "\n",
      "loss: 1.163356  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.287382 \n",
      "\n",
      "loss: 1.305169  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.949312 \n",
      "\n",
      "loss: 0.953189  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 36.2%, Avg loss: 4.541187 \n",
      "\n",
      "loss: 4.419938  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 29.246917 \n",
      "\n",
      "loss: 30.031916  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 2.262311 \n",
      "\n",
      "loss: 2.160307  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.966425 \n",
      "\n",
      "loss: 0.965386  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.074286 \n",
      "\n",
      "loss: 1.075561  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 1.873011 \n",
      "\n",
      "loss: 1.862636  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 5.691950 \n",
      "\n",
      "loss: 5.577045  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.428028 \n",
      "\n",
      "loss: 1.216595  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.882796 \n",
      "\n",
      "loss: 0.868393  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.005128 \n",
      "\n",
      "loss: 1.023088  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 1.118912 \n",
      "\n",
      "loss: 1.137089  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.884733 \n",
      "\n",
      "loss: 0.904284  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 2.182917 \n",
      "\n",
      "loss: 2.049970  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.173093 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 126/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [528, 147, 79], 'dacay': 0.0024056753274808574, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.4671307720018918}\n",
      "loss: 1.097525  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.223050 \n",
      "\n",
      "loss: 2.251757  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.722826 \n",
      "\n",
      "loss: 2.827129  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.145264 \n",
      "\n",
      "loss: 2.028713  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.969478 \n",
      "\n",
      "loss: 1.897003  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 2.473111 \n",
      "\n",
      "loss: 2.528810  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.214426 \n",
      "\n",
      "loss: 1.219193  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.266843 \n",
      "\n",
      "loss: 1.300051  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.185129 \n",
      "\n",
      "loss: 1.178712  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.146963 \n",
      "\n",
      "loss: 1.156643  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.527554 \n",
      "\n",
      "loss: 1.503483  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.107602 \n",
      "\n",
      "loss: 1.110130  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.139577 \n",
      "\n",
      "loss: 1.136545  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.107801 \n",
      "\n",
      "loss: 1.104678  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.668948 \n",
      "\n",
      "loss: 1.690354  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 3.147768 \n",
      "\n",
      "loss: 3.078797  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.475852 \n",
      "\n",
      "loss: 2.519860  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.311966 \n",
      "\n",
      "loss: 4.419160  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.475898 \n",
      "\n",
      "loss: 5.512899  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.816228 \n",
      "\n",
      "loss: 1.764674  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.151972 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 127/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [281, 10, 297], 'dacay': 0.42682997699915937, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.2233290417822929}\n",
      "loss: 1.139513  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 5.269197 \n",
      "\n",
      "loss: 5.121687  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 4.860313 \n",
      "\n",
      "loss: 4.708313  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.335456 \n",
      "\n",
      "loss: 5.225741  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 4.575346 \n",
      "\n",
      "loss: 4.454835  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 4.463199 \n",
      "\n",
      "loss: 4.728961  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.925478 \n",
      "\n",
      "loss: 4.877279  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.345008 \n",
      "\n",
      "loss: 5.227816  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.832990 \n",
      "\n",
      "loss: 4.764610  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 5.077297 \n",
      "\n",
      "loss: 5.039802  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 5.633411 \n",
      "\n",
      "loss: 5.730866  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.141864 \n",
      "\n",
      "loss: 5.041280  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.766037 \n",
      "\n",
      "loss: 4.812270  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 5.255980 \n",
      "\n",
      "loss: 5.211918  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 5.301998 \n",
      "\n",
      "loss: 5.471079  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 4.610149 \n",
      "\n",
      "loss: 4.521651  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 5.439546 \n",
      "\n",
      "loss: 5.449051  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 4.849379 \n",
      "\n",
      "loss: 4.801037  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.620321 \n",
      "\n",
      "loss: 4.790436  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 5.648897 \n",
      "\n",
      "loss: 5.622259  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 4.659240 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 128/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [361, 348], 'dacay': 0.05748930856766734, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.466432842632059}\n",
      "loss: 1.096112  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 24.550810 \n",
      "\n",
      "loss: 24.199730  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 17.938135 \n",
      "\n",
      "loss: 18.657379  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 28.615921 \n",
      "\n",
      "loss: 27.956633  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 44.729162 \n",
      "\n",
      "loss: 44.277950  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 26.313632 \n",
      "\n",
      "loss: 26.752144  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 21.146529 \n",
      "\n",
      "loss: 21.357246  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 30.024412 \n",
      "\n",
      "loss: 30.234991  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 26.274172 \n",
      "\n",
      "loss: 27.480167  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 21.323688 \n",
      "\n",
      "loss: 21.287561  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 24.121508 \n",
      "\n",
      "loss: 24.374460  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 14.484472 \n",
      "\n",
      "loss: 15.361739  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 19.332455 \n",
      "\n",
      "loss: 19.915058  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 19.632919 \n",
      "\n",
      "loss: 19.914122  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 12.691152 \n",
      "\n",
      "loss: 12.545385  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 30.231935 \n",
      "\n",
      "loss: 30.042313  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 17.321222 \n",
      "\n",
      "loss: 17.496675  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 19.694151 \n",
      "\n",
      "loss: 20.127357  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 20.766075 \n",
      "\n",
      "loss: 21.631813  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 18.348452 \n",
      "\n",
      "loss: 17.818792  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 31.1%, Avg loss: 3.297269 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 129/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [252], 'dacay': 0.07227501823653452, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.02191928481874123}\n",
      "loss: 1.140608  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.864992 \n",
      "\n",
      "loss: 0.873972  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.832091 \n",
      "\n",
      "loss: 0.845722  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.822779 \n",
      "\n",
      "loss: 0.807334  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.854424 \n",
      "\n",
      "loss: 0.859660  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.810514 \n",
      "\n",
      "loss: 0.773031  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.849910 \n",
      "\n",
      "loss: 0.877237  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.809424 \n",
      "\n",
      "loss: 0.793137  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.822588 \n",
      "\n",
      "loss: 0.798213  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.901426 \n",
      "\n",
      "loss: 0.913853  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.805538 \n",
      "\n",
      "loss: 0.813643  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.837432 \n",
      "\n",
      "loss: 0.807604  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.806240 \n",
      "\n",
      "loss: 0.812336  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.842279 \n",
      "\n",
      "loss: 0.836748  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.861218 \n",
      "\n",
      "loss: 0.864242  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.896113 \n",
      "\n",
      "loss: 0.855633  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.867325 \n",
      "\n",
      "loss: 0.868800  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.834034 \n",
      "\n",
      "loss: 0.848317  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.808461 \n",
      "\n",
      "loss: 0.799493  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.810868 \n",
      "\n",
      "loss: 0.798337  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.820748 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 130/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [492, 398, 403], 'dacay': 8.751248368551574, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.4824157367416181}\n",
      "loss: 1.101407  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1301.393226 \n",
      "\n",
      "loss: 1338.403809  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 886.120793 \n",
      "\n",
      "loss: 823.098328  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 69.994634 \n",
      "\n",
      "loss: 70.749733  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099685 \n",
      "\n",
      "loss: 1.099840  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099007 \n",
      "\n",
      "loss: 1.097890  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098763 \n",
      "\n",
      "loss: 1.099643  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.099106 \n",
      "\n",
      "loss: 1.099684  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099423 \n",
      "\n",
      "loss: 1.099593  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098664 \n",
      "\n",
      "loss: 1.098432  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099662 \n",
      "\n",
      "loss: 1.098399  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099489 \n",
      "\n",
      "loss: 1.101711  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.100141 \n",
      "\n",
      "loss: 1.100432  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098524 \n",
      "\n",
      "loss: 1.100343  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.100831 \n",
      "\n",
      "loss: 1.103048  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.100961 \n",
      "\n",
      "loss: 1.098895  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098940 \n",
      "\n",
      "loss: 1.099634  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098833 \n",
      "\n",
      "loss: 1.098584  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098397 \n",
      "\n",
      "loss: 1.098852  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.102398 \n",
      "\n",
      "loss: 1.102094  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098859 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 131/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [522], 'dacay': 6.7107019569834145, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.4371077679897487}\n",
      "loss: 1.148474  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 132/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [110, 546], 'dacay': 3.566222112034455, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.20504831201120755}\n",
      "loss: 1.109456  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098612 \n",
      "\n",
      "loss: 1.098420  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098698 \n",
      "\n",
      "loss: 1.098804  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098753 \n",
      "\n",
      "loss: 1.098712  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098601 \n",
      "\n",
      "loss: 1.098553  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098601 \n",
      "\n",
      "loss: 1.098626  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098585 \n",
      "\n",
      "loss: 1.098593  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098614 \n",
      "\n",
      "loss: 1.098259  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098789 \n",
      "\n",
      "loss: 1.098896  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098624 \n",
      "\n",
      "loss: 1.098544  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098700 \n",
      "\n",
      "loss: 1.098706  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098569 \n",
      "\n",
      "loss: 1.098656  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098660 \n",
      "\n",
      "loss: 1.098516  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098727 \n",
      "\n",
      "loss: 1.098537  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098650 \n",
      "\n",
      "loss: 1.098471  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098532 \n",
      "\n",
      "loss: 1.098612  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098660 \n",
      "\n",
      "loss: 1.098707  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098492  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098739 \n",
      "\n",
      "loss: 1.098883  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098603 \n",
      "\n",
      "loss: 1.098540  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098662 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 133/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [166, 84, 488], 'dacay': 0.8084002820443181, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.3116673743209468}\n",
      "loss: 1.098986  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 36.8%, Avg loss: 3.119123 \n",
      "\n",
      "loss: 3.055173  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.3%, Avg loss: 1.065197 \n",
      "\n",
      "loss: 1.064069  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098761 \n",
      "\n",
      "loss: 1.097760  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098666 \n",
      "\n",
      "loss: 1.098589  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098693 \n",
      "\n",
      "loss: 1.098069  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098571 \n",
      "\n",
      "loss: 1.098571  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098593 \n",
      "\n",
      "loss: 1.098796  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098642 \n",
      "\n",
      "loss: 1.098749  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098976 \n",
      "\n",
      "loss: 1.099005  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098555 \n",
      "\n",
      "loss: 1.098469  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098554 \n",
      "\n",
      "loss: 1.099514  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098566 \n",
      "\n",
      "loss: 1.098707  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098430 \n",
      "\n",
      "loss: 1.098257  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098829 \n",
      "\n",
      "loss: 1.098999  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.098563 \n",
      "\n",
      "loss: 1.098690  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099098 \n",
      "\n",
      "loss: 1.098730  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098888 \n",
      "\n",
      "loss: 1.099721  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098688 \n",
      "\n",
      "loss: 1.099190  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.098840 \n",
      "\n",
      "loss: 1.098942  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099080 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 134/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [90, 344], 'dacay': 1.8268659565100702, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.3134369095860931}\n",
      "loss: 1.099304  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 5.269348 \n",
      "\n",
      "loss: 4.821267  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.960635 \n",
      "\n",
      "loss: 2.909287  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 4.530298 \n",
      "\n",
      "loss: 4.529945  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 4.542438 \n",
      "\n",
      "loss: 4.690624  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 7.459152 \n",
      "\n",
      "loss: 7.848609  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.894372 \n",
      "\n",
      "loss: 9.349254  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 9.324806 \n",
      "\n",
      "loss: 9.013018  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 5.016249 \n",
      "\n",
      "loss: 4.888190  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 4.614312 \n",
      "\n",
      "loss: 4.702284  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 6.343115 \n",
      "\n",
      "loss: 6.343569  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 3.497279 \n",
      "\n",
      "loss: 3.456740  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 7.849428 \n",
      "\n",
      "loss: 7.792036  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 6.959327 \n",
      "\n",
      "loss: 6.986633  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 6.246464 \n",
      "\n",
      "loss: 6.092319  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 7.667675 \n",
      "\n",
      "loss: 7.710059  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.563136 \n",
      "\n",
      "loss: 8.471747  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 7.005976 \n",
      "\n",
      "loss: 6.993707  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.658350 \n",
      "\n",
      "loss: 6.401250  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 7.290770 \n",
      "\n",
      "loss: 7.312486  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 7.029911 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 135/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [562, 44], 'dacay': 0.12226363624719493, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.49871321888581205}\n",
      "loss: 1.119181  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 124.400828 \n",
      "\n",
      "loss: 122.114426  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 42.4%, Avg loss: 2.397920 \n",
      "\n",
      "loss: 2.962275  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 42.1%, Avg loss: 2.833424 \n",
      "\n",
      "loss: 2.795890  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 7.004690 \n",
      "\n",
      "loss: 7.279169  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 47.0%, Avg loss: 1103.153982 \n",
      "\n",
      "loss: 1128.204590  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 22.733872 \n",
      "\n",
      "loss: 23.335360  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 29.4%, Avg loss: 145.541328 \n",
      "\n",
      "loss: 139.053497  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 3.505531 \n",
      "\n",
      "loss: 3.709221  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.940577 \n",
      "\n",
      "loss: 1.937530  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 2.008511 \n",
      "\n",
      "loss: 1.749945  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 28.8%, Avg loss: 37.242438 \n",
      "\n",
      "loss: 37.431011  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 34.362734 \n",
      "\n",
      "loss: 33.577560  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 3.129208 \n",
      "\n",
      "loss: 3.125170  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 8.373287 \n",
      "\n",
      "loss: 7.519800  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 4.453774 \n",
      "\n",
      "loss: 4.507638  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 1.539329 \n",
      "\n",
      "loss: 1.446434  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 44.7%, Avg loss: 8.207528 \n",
      "\n",
      "loss: 7.760421  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 1.143353 \n",
      "\n",
      "loss: 1.139439  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 50.2%, Avg loss: 2.425567 \n",
      "\n",
      "loss: 2.420975  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 1.383741 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 136/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [100, 452, 108], 'dacay': 0.6336485785618937, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.08326575688593967}\n",
      "loss: 1.100176  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099542 \n",
      "\n",
      "loss: 1.098711  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098703 \n",
      "\n",
      "loss: 1.098706  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098404 \n",
      "\n",
      "loss: 1.098945  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099145 \n",
      "\n",
      "loss: 1.098091  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098915 \n",
      "\n",
      "loss: 1.099613  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099478 \n",
      "\n",
      "loss: 1.100018  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098709 \n",
      "\n",
      "loss: 1.099133  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.100816 \n",
      "\n",
      "loss: 1.098281  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098599 \n",
      "\n",
      "loss: 1.098823  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098699 \n",
      "\n",
      "loss: 1.098480  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098594 \n",
      "\n",
      "loss: 1.098805  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.098585 \n",
      "\n",
      "loss: 1.099674  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098570 \n",
      "\n",
      "loss: 1.099516  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.099077 \n",
      "\n",
      "loss: 1.100795  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098601 \n",
      "\n",
      "loss: 1.098818  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098816 \n",
      "\n",
      "loss: 1.099233  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.099013 \n",
      "\n",
      "loss: 1.099518  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.098753 \n",
      "\n",
      "loss: 1.098459  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098671 \n",
      "\n",
      "loss: 1.098658  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099082 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 137/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [37, 55], 'dacay': 0.004186714378240838, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.1792958248159517}\n",
      "loss: 1.096409  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.889467 \n",
      "\n",
      "loss: 0.893794  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.672866 \n",
      "\n",
      "loss: 0.639681  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.644638 \n",
      "\n",
      "loss: 0.626247  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.664712 \n",
      "\n",
      "loss: 0.684418  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.684398 \n",
      "\n",
      "loss: 0.685278  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.681223 \n",
      "\n",
      "loss: 0.691501  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.659731 \n",
      "\n",
      "loss: 0.650994  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.772242 \n",
      "\n",
      "loss: 0.757975  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.710914 \n",
      "\n",
      "loss: 0.683422  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.652375 \n",
      "\n",
      "loss: 0.644082  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.665221 \n",
      "\n",
      "loss: 0.656036  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.647437 \n",
      "\n",
      "loss: 0.657276  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.645167 \n",
      "\n",
      "loss: 0.693479  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.678678 \n",
      "\n",
      "loss: 0.672307  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.678389 \n",
      "\n",
      "loss: 0.665266  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.708798 \n",
      "\n",
      "loss: 0.695749  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.699955 \n",
      "\n",
      "loss: 0.677328  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.700951 \n",
      "\n",
      "loss: 0.702854  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.683495 \n",
      "\n",
      "loss: 0.677029  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.703640 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 138/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [157, 258], 'dacay': 0.04527219908473994, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.087617170595946}\n",
      "loss: 1.100097  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 0.971353 \n",
      "\n",
      "loss: 0.942429  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.909235 \n",
      "\n",
      "loss: 0.899926  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 46.2%, Avg loss: 0.995647 \n",
      "\n",
      "loss: 1.016965  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.913708 \n",
      "\n",
      "loss: 0.928587  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 0.877186 \n",
      "\n",
      "loss: 0.863006  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 0.932576 \n",
      "\n",
      "loss: 0.920628  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.901880 \n",
      "\n",
      "loss: 0.880469  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 48.3%, Avg loss: 1.031620 \n",
      "\n",
      "loss: 1.032910  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 0.922645 \n",
      "\n",
      "loss: 0.933385  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 47.1%, Avg loss: 1.042395 \n",
      "\n",
      "loss: 1.008316  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.905162 \n",
      "\n",
      "loss: 0.906938  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.885021 \n",
      "\n",
      "loss: 0.863647  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 0.962586 \n",
      "\n",
      "loss: 0.952589  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.886855 \n",
      "\n",
      "loss: 0.882588  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.831007 \n",
      "\n",
      "loss: 0.811535  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.879025 \n",
      "\n",
      "loss: 0.883261  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.929905 \n",
      "\n",
      "loss: 0.956244  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.868785 \n",
      "\n",
      "loss: 0.854085  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.899036 \n",
      "\n",
      "loss: 0.859637  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 0.868768 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 139/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [399, 254, 517], 'dacay': 3.6656000698224345, 'activition': 'LogSigmoid', 'optimizer': 'SGD', 'lr': 0.23062884168616327}\n",
      "loss: 1.144092  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 5.894993 \n",
      "\n",
      "loss: 6.026310  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 7.154390 \n",
      "\n",
      "loss: 6.883420  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 7.852268 \n",
      "\n",
      "loss: 8.261850  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 5.785871 \n",
      "\n",
      "loss: 5.867857  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 5.384309 \n",
      "\n",
      "loss: 5.491009  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 10.178953 \n",
      "\n",
      "loss: 10.129974  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 8.127323 \n",
      "\n",
      "loss: 8.100762  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 6.622605 \n",
      "\n",
      "loss: 6.589611  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 5.068150 \n",
      "\n",
      "loss: 4.947050  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 5.126511 \n",
      "\n",
      "loss: 5.026477  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.388384 \n",
      "\n",
      "loss: 8.685890  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 3.859301 \n",
      "\n",
      "loss: 3.685457  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.621464 \n",
      "\n",
      "loss: 6.893385  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 5.449950 \n",
      "\n",
      "loss: 5.555264  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 8.903535 \n",
      "\n",
      "loss: 9.071986  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.396649 \n",
      "\n",
      "loss: 6.309853  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 6.570872 \n",
      "\n",
      "loss: 6.459573  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 6.389869 \n",
      "\n",
      "loss: 6.444485  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 9.137091 \n",
      "\n",
      "loss: 9.604617  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 4.654064 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 140/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [478, 269], 'dacay': 0.21587561425923868, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.33883768176096984}\n",
      "loss: 1.245514  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 8.233092 \n",
      "\n",
      "loss: 8.046741  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 10.040740 \n",
      "\n",
      "loss: 9.803843  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 12.863543 \n",
      "\n",
      "loss: 12.453918  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 11.788299 \n",
      "\n",
      "loss: 12.013966  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 10.093575 \n",
      "\n",
      "loss: 9.783880  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 25.886614 \n",
      "\n",
      "loss: 26.378931  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 12.357704 \n",
      "\n",
      "loss: 12.169957  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 18.157364 \n",
      "\n",
      "loss: 17.921558  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 28.072137 \n",
      "\n",
      "loss: 28.071186  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 9.545459 \n",
      "\n",
      "loss: 9.790136  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 22.529280 \n",
      "\n",
      "loss: 22.027811  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 15.811801 \n",
      "\n",
      "loss: 15.142573  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 13.441552 \n",
      "\n",
      "loss: 13.213182  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 4.117421 \n",
      "\n",
      "loss: 4.127581  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 8.866603 \n",
      "\n",
      "loss: 8.257759  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 19.352872 \n",
      "\n",
      "loss: 18.880152  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 20.046484 \n",
      "\n",
      "loss: 20.703154  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.106983 \n",
      "\n",
      "loss: 2.150242  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 11.826017 \n",
      "\n",
      "loss: 12.587569  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 9.595620 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 141/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [83], 'dacay': 0.016167725165620768, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.3195054573097701}\n",
      "loss: 1.084915  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1.479212 \n",
      "\n",
      "loss: 1.632490  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.728463 \n",
      "\n",
      "loss: 0.702688  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.760744 \n",
      "\n",
      "loss: 0.729832  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.702067 \n",
      "\n",
      "loss: 0.697271  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.730272 \n",
      "\n",
      "loss: 0.744426  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.730708 \n",
      "\n",
      "loss: 0.727594  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.774826 \n",
      "\n",
      "loss: 0.761685  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.809076 \n",
      "\n",
      "loss: 0.847875  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.822515 \n",
      "\n",
      "loss: 0.812964  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.740868 \n",
      "\n",
      "loss: 0.727539  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.764519 \n",
      "\n",
      "loss: 0.769081  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.775409 \n",
      "\n",
      "loss: 0.780385  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.757661 \n",
      "\n",
      "loss: 0.747136  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.816678 \n",
      "\n",
      "loss: 0.870865  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 23.352932 \n",
      "\n",
      "loss: 20.047716  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 11.312653 \n",
      "\n",
      "loss: 12.463214  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 28.221673 \n",
      "\n",
      "loss: 29.894712  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 3.080895 \n",
      "\n",
      "loss: 2.720241  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.012275 \n",
      "\n",
      "loss: 1.018292  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.721957 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 142/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [264, 475], 'dacay': 1.6846885559177744, 'activition': 'LogSigmoid', 'optimizer': 'Adam', 'lr': 0.4266823957842269}\n",
      "loss: 1.156761  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 290.327853 \n",
      "\n",
      "loss: 271.320557  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 9.024392 \n",
      "\n",
      "loss: 9.465975  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 3.307326 \n",
      "\n",
      "loss: 3.498959  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.558219 \n",
      "\n",
      "loss: 2.539891  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 16.595351 \n",
      "\n",
      "loss: 17.128475  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 12095.293870 \n",
      "\n",
      "loss: 12483.397461  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 2584.559345 \n",
      "\n",
      "loss: 2516.255371  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 21.098181 \n",
      "\n",
      "loss: 20.739937  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 3.347534 \n",
      "\n",
      "loss: 3.241200  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.103749 \n",
      "\n",
      "loss: 1.102871  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.118251 \n",
      "\n",
      "loss: 1.121874  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099576 \n",
      "\n",
      "loss: 1.099984  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098472 \n",
      "\n",
      "loss: 1.098730  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.099044 \n",
      "\n",
      "loss: 1.098217  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.101912 \n",
      "\n",
      "loss: 1.101825  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099483 \n",
      "\n",
      "loss: 1.099297  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.100107 \n",
      "\n",
      "loss: 1.100560  [ 1024/52478]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098524 \n",
      "\n",
      "loss: 1.099238  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 1.099208 \n",
      "\n",
      "loss: 1.097370  [ 1024/52479]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 1.100069 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "Search progress: 143/400\n",
      "--------------------\n",
      "Parameter set:  {'hl': [250], 'dacay': 0.008069032938310938, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.49634675260908606}\n",
      "loss: 1.092885  [ 1024/52478]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mHong-01\\EE4C12-Epileptic-Seizure-Detection-using-EEG\\DNN.ipynb å•å…ƒæ ¼ 23\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=31'>32</a>\u001b[0m validation_dataloader \u001b[39m=\u001b[39m val_dataloader_list[i\u001b[39m%\u001b[39m\u001b[39m5\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=32'>33</a>\u001b[0m model_rs\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=33'>34</a>\u001b[0m train(train_dataloader, model_rs, loss_fn, optimizer_rs)\n\u001b[0;32m     <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=34'>35</a>\u001b[0m test(validation_dataloader, model_rs, loss_fn)\n\u001b[0;32m     <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39m5\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n",
      "\u001b[1;32mHong-01\\EE4C12-Epileptic-Seizure-Detection-using-EEG\\DNN.ipynb å•å…ƒæ ¼ 23\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=2'>3</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m      <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=5'>6</a>\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell://github/Hong-01/EE4C12-Epileptic-Seizure-Detection-using-EEG/DNN.ipynb#X22sdnNjb2RlLXZmcw%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Compute prediction error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:139\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(elem) \u001b[39m==\u001b[39m elem_size \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m it):\n\u001b[0;32m    138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(random_state)\n",
    "\n",
    "input_size=X_test_selected.shape[-1]\n",
    "output_size=3\n",
    "epochs = 20\n",
    "num_search = 400\n",
    "\n",
    "accuracy_list = []\n",
    "hp_list = []\n",
    "pm_list = []\n",
    "\n",
    "for search in range(num_search):\n",
    "    print('-'*20)\n",
    "    print(\"Search progress: \"+ str(search+1)+\"/\"+str(num_search))\n",
    "    print('-'*20)\n",
    "    hp = get_hps()\n",
    "    print(\"Parameter set: \",hp)\n",
    "    model_rs = DNN_rs(input_size=input_size, hidden_sizes=hp['hl'], output_size=output_size, activition_layer=hp['activition']).to(device)\n",
    "    \n",
    "    if hp['optimizer'] == 'SGD':\n",
    "        optimizer_rs = torch.optim.SGD(model_rs.parameters(), lr=hp['lr'], weight_decay=hp['dacay'])\n",
    "    elif hp['optimizer'] == 'Adam':\n",
    "        optimizer_rs = torch.optim.Adam(model_rs.parameters(), lr=hp['lr'], weight_decay=hp['dacay'])\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        break\n",
    "        \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        train_dataloader = train_dataloader_list[i%5]\n",
    "        validation_dataloader = val_dataloader_list[i%5]\n",
    "        model_rs.train()\n",
    "        train(train_dataloader, model_rs, loss_fn, optimizer_rs)\n",
    "        test(validation_dataloader, model_rs, loss_fn)\n",
    "        \n",
    "        if (i+1)%5==0:\n",
    "            model_rs.eval()\n",
    "            pred = model_rs(val_dataloader_k.dataset[:][0]).detach().cpu().max(axis=1).indices.numpy()\n",
    "            true = val_dataloader_k.dataset[:][1].cpu().numpy()\n",
    "            accuracy_list.append(accuracy_score(true, pred))\n",
    "            hp.update({'epoch': i+1})\n",
    "            hp_list.append(hp.copy())\n",
    "            pm_list.append(model_rs.state_dict())\n",
    "    print('-'*20)\n",
    "    \n",
    "accuracy_list = np.array(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8387189844200807\n",
      "F1 score: 0.8387189844200807\n",
      "Recall score: 0.8387189844200807\n",
      "Precision score: 0.8387189844200807\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_idx = np.argmax(accuracy_list)\n",
    "hp = hp_list[best_idx]\n",
    "model_best = DNN_rs(input_size=input_size, hidden_sizes=hp['hl'], output_size=output_size, activition_layer=hp['activition']).to(device)\n",
    "model_best.load_state_dict(pm_list[best_idx])   \n",
    "\n",
    "model_best.eval()\n",
    "pred = model_best(test_dataloader.dataset[:][0]).detach().cpu().max(axis=1).indices.numpy()\n",
    "true = test_dataloader.dataset[:][1].cpu().numpy()\n",
    "\n",
    "performance_eval(true, pred, matrix_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hl': [137, 435],\n",
       " 'alpha': 0.00025676224472019756,\n",
       " 'activition': 'ReLU',\n",
       " 'optimizer': 'SGD',\n",
       " 'lr': 0.4999539234717537,\n",
       " 'epoch': 20}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
