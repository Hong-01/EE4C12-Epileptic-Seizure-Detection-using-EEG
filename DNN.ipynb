{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, precision_recall_curve, PrecisionRecallDisplay, roc_auc_score, RocCurveDisplay, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>annotation</th>\n",
       "      <th>min|FP1-F7</th>\n",
       "      <th>min|F7-T3</th>\n",
       "      <th>min|T3-T5</th>\n",
       "      <th>min|T5-O1</th>\n",
       "      <th>min|FP2-F8</th>\n",
       "      <th>min|F8-T4</th>\n",
       "      <th>min|T4-T6</th>\n",
       "      <th>min|T6-O2</th>\n",
       "      <th>...</th>\n",
       "      <th>norm_power_HF|CZ-C4</th>\n",
       "      <th>norm_power_HF|C4-T4</th>\n",
       "      <th>norm_power_HF|FP1-F3</th>\n",
       "      <th>norm_power_HF|F3-C3</th>\n",
       "      <th>norm_power_HF|C3-P3</th>\n",
       "      <th>norm_power_HF|P3-O1</th>\n",
       "      <th>norm_power_HF|FP2-F4</th>\n",
       "      <th>norm_power_HF|F4-C4</th>\n",
       "      <th>norm_power_HF|C4-P4</th>\n",
       "      <th>norm_power_HF|P4-O2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>61</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016087</td>\n",
       "      <td>0.066920</td>\n",
       "      <td>0.102402</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.690787</td>\n",
       "      <td>0.154544</td>\n",
       "      <td>0.062533</td>\n",
       "      <td>0.046460</td>\n",
       "      <td>0.066575</td>\n",
       "      <td>0.086999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>46</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024006</td>\n",
       "      <td>0.064857</td>\n",
       "      <td>0.031791</td>\n",
       "      <td>0.225788</td>\n",
       "      <td>0.409987</td>\n",
       "      <td>0.184671</td>\n",
       "      <td>0.071133</td>\n",
       "      <td>0.022369</td>\n",
       "      <td>0.079494</td>\n",
       "      <td>0.047536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>45</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037326</td>\n",
       "      <td>0.100177</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.622584</td>\n",
       "      <td>0.394504</td>\n",
       "      <td>0.225516</td>\n",
       "      <td>0.050673</td>\n",
       "      <td>0.044906</td>\n",
       "      <td>0.102142</td>\n",
       "      <td>0.068105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027546</td>\n",
       "      <td>0.107883</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.359140</td>\n",
       "      <td>0.276964</td>\n",
       "      <td>0.104977</td>\n",
       "      <td>0.018042</td>\n",
       "      <td>0.079467</td>\n",
       "      <td>0.078255</td>\n",
       "      <td>0.089385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036820</td>\n",
       "      <td>0.182520</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.328354</td>\n",
       "      <td>0.156929</td>\n",
       "      <td>0.151952</td>\n",
       "      <td>0.047532</td>\n",
       "      <td>0.135071</td>\n",
       "      <td>0.098320</td>\n",
       "      <td>0.137701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55451</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244334</td>\n",
       "      <td>0.625396</td>\n",
       "      <td>0.023821</td>\n",
       "      <td>0.058277</td>\n",
       "      <td>0.083594</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>0.119654</td>\n",
       "      <td>0.295364</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>0.199585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55452</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.743060</td>\n",
       "      <td>0.076294</td>\n",
       "      <td>0.332341</td>\n",
       "      <td>0.228458</td>\n",
       "      <td>0.170603</td>\n",
       "      <td>0.351418</td>\n",
       "      <td>0.638666</td>\n",
       "      <td>0.490806</td>\n",
       "      <td>0.307429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55453</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296041</td>\n",
       "      <td>0.770194</td>\n",
       "      <td>0.041190</td>\n",
       "      <td>0.090919</td>\n",
       "      <td>0.186074</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.231053</td>\n",
       "      <td>0.770637</td>\n",
       "      <td>0.285257</td>\n",
       "      <td>0.413382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55454</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>77</td>\n",
       "      <td>82</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440360</td>\n",
       "      <td>0.720855</td>\n",
       "      <td>0.026959</td>\n",
       "      <td>0.026340</td>\n",
       "      <td>0.077674</td>\n",
       "      <td>0.269610</td>\n",
       "      <td>0.186769</td>\n",
       "      <td>0.790173</td>\n",
       "      <td>0.473615</td>\n",
       "      <td>0.415771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55455</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "      <td>80</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019325</td>\n",
       "      <td>0.735140</td>\n",
       "      <td>0.030715</td>\n",
       "      <td>0.077191</td>\n",
       "      <td>0.095298</td>\n",
       "      <td>0.317765</td>\n",
       "      <td>0.271859</td>\n",
       "      <td>0.675646</td>\n",
       "      <td>0.506836</td>\n",
       "      <td>0.561740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55456 rows Ã— 362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Patient  annotation  min|FP1-F7  min|F7-T3  min|T3-T5  min|T5-O1  \\\n",
       "0          258           1          50         61         57         53   \n",
       "1          258           1          48         62         60         46   \n",
       "2          258           1          51         60         59         45   \n",
       "3          258           1          49         58         56         42   \n",
       "4          258           1          45         57         61         41   \n",
       "...        ...         ...         ...        ...        ...        ...   \n",
       "55451    11580          -1          75         73         81         80   \n",
       "55452    11580          -1          74         74         77         71   \n",
       "55453    11580          -1          72         76         72         73   \n",
       "55454    11580          -1          77         82         74         75   \n",
       "55455    11580          -1          71         79         74         78   \n",
       "\n",
       "       min|FP2-F8  min|F8-T4  min|T4-T6  min|T6-O2  ...  norm_power_HF|CZ-C4  \\\n",
       "0              39         35         39         35  ...             0.016087   \n",
       "1              38         35         39         33  ...             0.024006   \n",
       "2              38         36         40         36  ...             0.037326   \n",
       "3              36         36         41         37  ...             0.027546   \n",
       "4              35         37         41         37  ...             0.036820   \n",
       "...           ...        ...        ...        ...  ...                  ...   \n",
       "55451          66         80         77         75  ...             0.244334   \n",
       "55452          79         75         82         77  ...             0.588236   \n",
       "55453          74         76         80         76  ...             0.296041   \n",
       "55454          82         85         80         76  ...             0.440360   \n",
       "55455          80         85         81         75  ...             1.019325   \n",
       "\n",
       "       norm_power_HF|C4-T4  norm_power_HF|FP1-F3  norm_power_HF|F3-C3  \\\n",
       "0                 0.066920              0.102402             0.481384   \n",
       "1                 0.064857              0.031791             0.225788   \n",
       "2                 0.100177              0.050009             0.622584   \n",
       "3                 0.107883              0.014017             0.359140   \n",
       "4                 0.182520              0.031397             0.328354   \n",
       "...                    ...                   ...                  ...   \n",
       "55451             0.625396              0.023821             0.058277   \n",
       "55452             0.743060              0.076294             0.332341   \n",
       "55453             0.770194              0.041190             0.090919   \n",
       "55454             0.720855              0.026959             0.026340   \n",
       "55455             0.735140              0.030715             0.077191   \n",
       "\n",
       "       norm_power_HF|C3-P3  norm_power_HF|P3-O1  norm_power_HF|FP2-F4  \\\n",
       "0                 0.690787             0.154544              0.062533   \n",
       "1                 0.409987             0.184671              0.071133   \n",
       "2                 0.394504             0.225516              0.050673   \n",
       "3                 0.276964             0.104977              0.018042   \n",
       "4                 0.156929             0.151952              0.047532   \n",
       "...                    ...                  ...                   ...   \n",
       "55451             0.083594             0.114426              0.119654   \n",
       "55452             0.228458             0.170603              0.351418   \n",
       "55453             0.186074             0.216797              0.231053   \n",
       "55454             0.077674             0.269610              0.186769   \n",
       "55455             0.095298             0.317765              0.271859   \n",
       "\n",
       "       norm_power_HF|F4-C4  norm_power_HF|C4-P4  norm_power_HF|P4-O2  \n",
       "0                 0.046460             0.066575             0.086999  \n",
       "1                 0.022369             0.079494             0.047536  \n",
       "2                 0.044906             0.102142             0.068105  \n",
       "3                 0.079467             0.078255             0.089385  \n",
       "4                 0.135071             0.098320             0.137701  \n",
       "...                    ...                  ...                  ...  \n",
       "55451             0.295364             0.185930             0.199585  \n",
       "55452             0.638666             0.490806             0.307429  \n",
       "55453             0.770637             0.285257             0.413382  \n",
       "55454             0.790173             0.473615             0.415771  \n",
       "55455             0.675646             0.506836             0.561740  \n",
       "\n",
       "[55456 rows x 362 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #load data on Yanqi Hong's computer\n",
    "# data = pd.read_csv('E:\\DATA\\TUD\\Master\\TUD_Master_Y1\\Q1\\EE4C12 Machine Learning For Electrical Engineering\\CodeLab\\Project\\S&S_SZD (1)\\Data\\Project_Data_EE4C12_S&S_SZD.csv')\n",
    "# data\n",
    "\n",
    "# load data on Zhixuan's computer\n",
    "data = pd.read_csv('D:\\\\User\\Zhixuan Ge\\Onedrive TUDelft\\OneDrive - Delft University of Technology\\Courses\\ML for EE\\SZD\\S&S_SZD\\Project_Data_EE4C12_S&S_SZD.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:].values\n",
    "y = np.int32(data['annotation'].values)\n",
    "\n",
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original number of training feature is:  360\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current number of training feature after feature selection is:  360\n"
     ]
    }
   ],
   "source": [
    "print(\"The original number of training feature is: \", X_train_scaled.shape[1])\n",
    "clf_etc = ExtraTreesClassifier(random_state=random_state).fit(X_train_scaled, y_train) # fit the model\n",
    "feature_importances = clf_etc.feature_importances_  # get the feature importance\n",
    "\n",
    "plt.bar(range(len(feature_importances)), feature_importances)   # plot the feature importance\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Feature importance\")\n",
    "plt.title(\"Feature importance of all features\")\n",
    "plt.show()\n",
    "\n",
    "important_feature_indices=np.argsort(feature_importances)   # sort the feature importance  \n",
    "important_feature_indices_cut=important_feature_indices[:int(len(important_feature_indices)/1.5)]   # select the most important features  \n",
    "\n",
    "X_train_selected=np.delete(X_train_scaled,important_feature_indices_cut,1)    # delete the least important features\n",
    "X_test_selected=np.delete(X_test_scaled,important_feature_indices_cut,1)      # delete the least important features\n",
    "print(\"The current number of training feature after feature selection is: \", X_train_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x199229cdc50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGwCAYAAAB7MGXBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB3RElEQVR4nO3dd1yV5f/H8ddhDxmiyFAEFFFcuBVNza2VIyvNzJFl+dXK1bIyR8OsLDNH9su06ajUljly74FbnAiCCuICRPY51++PI0ePgAICN+PzfDzO45xzn/vc9+c+N3DeXPd1X7dOKaUQQgghhNCIhdYFCCGEEKJ8kzAihBBCCE1JGBFCCCGEpiSMCCGEEEJTEkaEEEIIoSkJI0IIIYTQlIQRIYQQQmjKSusC8sJgMHDx4kWcnJzQ6XRalyOEEEKIPFBKcePGDby9vbGwyL39o1SEkYsXL+Lj46N1GUIIIYQogOjoaKpVq5br66UijDg5OQHGjXF2dta4GiGEEELkRWJiIj4+Pqbv8dyUijCSdWjG2dlZwogQQghRytyvi4V0YBVCCCGEpiSMCCGEEEJTEkaEEEIIoSkJI0IIIYTQlIQRIYQQQmhKwogQQgghNCVhRAghhBCakjAihBBCCE1JGBFCCCGEpiSMCCGEEEJT+Q4jW7ZsoWfPnnh7e6PT6Vi5cuV937N582aaNm2KnZ0dNWrU4Ouvvy5IrUIIIYQog/IdRm7evElwcDCzZ8/O0/wRERE88sgjtG3blgMHDvD222/z6quv8vvvv+e7WCGEEEKUPfm+UF6PHj3o0aNHnuf/+uuvqV69OjNnzgQgKCiIffv28dlnn/HEE0/kd/VCCCFE0VPq1s0A3LrPep7jNHVrmrp7QTkvO/vE/M9TSMvSGxRhMQnU9vfHxuHeV9ctKkV+1d6dO3fStWtXs2ndunVjwYIFZGRkYG1tne09aWlppKWlmZ4nJiYWdZlCCCHySp8BGcmQkQqZqaBPv33LzHqcZpwvp2mZabeeZ5hPM2TeuumN90pv/tzs8R3TlP6ueTLBYDB/bhYe7g4ZhuzTyhFLoAFw8qGZ1O78nCY1FHkYiY2NxcPDw2yah4cHmZmZXLlyBS8vr2zvmTZtGlOmTCnq0oQQomzSZ0DaDUhPgrQkSL8J6TduPU66fZ+ZejtUZKRAZorxPiPl3q8pvdZbWArpcpiUw7Q8z5f/ZSrAoBR6gzJmr7tmuZaiXQgr8jACoLvrw1G3mozunp5lwoQJjBs3zvQ8MTERHx+foitQCCFKAn0mpCVCajykJkDKrfs7b2k3cgkXt6anJRlbG4qLlR1Y2oKlNVjdure0BUsbsLIx3mfdTK/fY5qFJegswcLq1s3y1s3q9k1nYf787nlM77/jXmdhvKEzfmHrLG7fo7v9uk6Xw7x3T7vPvCVI5JWbrD4Wy79HYzkUHW/2Wg13RzrVqUKnIA+a+lYkxFK7E2yLPIx4enoSGxtrNi0uLg4rKysqVaqU43tsbW2xtbUt6tKEEKLwKWUMCMlXIfnardtVSLn1+O5wkRU8UhOM7ytMlrZgWwFsbt1s77q3tjeGCWsHsLYDK3vjtKyblb1xurXDrfnszd9jZVvivnzLO6UUJy/dYPXRWFYfjeVE7A3TazodNPOtSLd6nnQK8sC/sqOGlZor8jASEhLCX3/9ZTZt7dq1NGvWLMf+IkIIUaIY9MYwkRQHSZfg5pVbweLqHYHj1n3WdH36g63TpgLYudy6ud7x2BlsnW4Filv3No63woXTHcHD0fi6pfyNLQ+UUhw6n8C/R2NYczSWyKvJptesLHSE1KxEt3qedK3nQRUnOw0rzV2+w0hSUhJnzpwxPY+IiODgwYO4ublRvXp1JkyYwIULF/jhhx8AGDFiBLNnz2bcuHEMHz6cnTt3smDBAhYvXlx4WyGEEPmhFKRcN4aLpDjj7Wac+fOsaTcvF6xDo5UdOFQCBzewdzM+tq8I9q53BYxbt6zpts5gWSxH0EUpZjAo9kdd5+/DMaw5FktMQqrpNRsrC9rVcqdHfU86BVXB1cFGw0rzJt8/8fv27aNDhw6m51l9O4YMGcKiRYuIiYkhKirK9Lq/vz+rVq1i7NixzJkzB29vb2bNmiWn9QohikZGKtyIgcSLt+8TL8KNW/eJMcbQYcjIx0J14FgZHKvcuq98O2BkBQ6HrMBx697Gocg2UZRPSikOn0/gr0MX+edIjFkAcbSxpEOdKvSo78XDtd1xtC1dgVanVI4nIJcoiYmJuLi4kJCQgLOzs9blCCG0os+ExAsQHwUJ0RAfbR4yblw0HibJK/uKxoBRIevmYbx3vONxhSrgUFlaK4QmlFKExSTy9+EY/jkcQ9S124dgKtha0bWuB4808OKhWpWxs7bUsNKc5fX7W367hBAlhz4DEs7fETai7rhFG4NIXk4rtbIDJy9wrgrOXuDsDU7etx5XvR00rKSjvCiZTl+6wV+HY/j78EXOXr5pmm5vbUnnuh481tCL9oHuJTKAFISEESFE8VHK2HJx7ezt2/XI22HjxsX798+wtAEXH3D1Md6bAkfVWwHE29jiIWd5iFLm/PVk/jh4kb8OXTQ7C8bGyoIOtd3pGexNxzpVcLApe1/dZW+LhBDaUsrYJ+POwGG6RRjH0bgXS1twrX7r5nPr3td47+JjbNWwkAuOi7IhPjmdVUdiWXngAnsir5mmW1vqaFvLnZ7BXnQO8sDJrmyfGSVhRAhRMBkpcPUMXDkFl0/BlZNw5bQxdGQk3/u9ztXAzR/cakBFP6joawwcLj7g6C5hQ5RpqRl6NpyIY+WBC2w8GUeGPmsgUGjp70afRlXpUd8LF4eyHUDuJGFECHFvKdfvCBt3BI/r58jxwl1gHJHSxccYNu6+VfQ1DpwlRDliMCh2RVzljwMXWXU0hhupmabX6ng60adxVXoFe+PtWj5/NySMCCGMMlLh8gm4dAwuHTXe4k4Yx9rIjZ0LVK4N7oHG+8qBUCnAeEjFquSPbSBEUTsek8jKAxf489BFs1NxvVzs6N2oKn0ae1PHU84SlTAiRHmjlPGslKzQEXvU+PjqmdzPVHGuditw3HFzr208pCIdRYUwczE+hT8OXmTlgQucvHS7I6qTnRWPNvCiT+OqtPBzw8JCfneySBgRoiwz6I2HVi4egIsHbweQ1Pic57d3A8/64HHrViXIGDxsKxRn1UKUOsnpmaw6EstvodHsOnu7I6qNpQUd6rjzeOOqPFy7Spk5FbewSRgRoqwwGIytGxcPGG8xByHmUM6dSS2sjIdVPOrdutU3hpAKHtLSIUQeKaXYE3GN30LPs+pIDDfTb7cstvR3o0/jqjxSzjqiFpSEESFKI6WM43NcCL3d6hFzyHgZ+bvZVACvYPBqBF4NjeGjcqAM+CVEAZ2/nszy/Rf4LfS82YiofpUceLJpNR5vUo2q5bQjakFJGBGiNMhIMYaO6D1wfi9E7zZewO1u1g7g2RC8G9++VQqQU2WFeEDJ6ZmsPhrLb6Hn2RF++5IDFWyN/UCebFaNZr4V0UnLYoFIGBGipFHKOCT6+T0QfSt4xB4GQ6b5fJY2xuBRtYkxdHg1MrZ4yDVUhCgUSin2nbvOb/vO88+RGJLSbv8OtgmoxJNNq9GtnmeZHBG1uMknKITWDAbjKbXnthtvUbuNw6LfrYIn+DQHn5ZQrYXx0Iu1XfHXK0QZdzE+heX7z/Nb6Hkir94+DFPdzXgYpm+TqlSrKFdlLkwSRoQobgaD8YyWc9shchuc2wEp18zn0VmCZwNj8PBpYby5+EjnUiGKSIbewPrjcSzZG8XmU5fJup69o40ljzb04smmPjT3k8MwRUXCiBBFTZ9pPMxybjtEboeoHZCaYD6PtYMxcPg+BL4hxsMuNo7a1CtEORJ55SZL9kbzW+h5riSlmaa3quHGU0196NFADsMUB/mEhShsShmvzxK+AcI3QuTW7BeHs6kA1VuBbxvwe8jY30NGLBWiWKRm6FlzLJYle6LZefZ2Z9TKFWx5qlk1+jfzwa+y/DNQnCSMCFEYUq5DxJZbAWQDxEeZv27rYmzx8G0Dfm3AM1g6mgpRzE5dusHiPVGsOHCB+OQMwHjk8+FAd/o3r06noCpYW8qZZ1qQv4ZCFIQ+03iKbVb4uLgflOH26xbWxpaPmh2gRgdjZ1MLGXlRiOKWnJ7J34diWLI3iv1R8abp3i529GvuQ79mPuX24nQliYQRIfIq5TqcWQ+nVsPpddmHVK9cG2p2NAYQ3zYyhLoQGjoek8jPu8+x8sBF0ym5VhY6OgVV4ekW1WlXyx1LuTZMiSFhRIjcKGW8rsup1XBqDUTtMr+QnH3FW+Gjo7H1w6WqdrUKIUjN0LPqSAw/7Tpn1griW8mB/s19eLJpNao4yenwJZGEESHuZDAYD7kc/xOO/2XsiHon9yCo3R0Cu0O15nLoRYgSIOLKTX7ZfY5fQ8+b+oJYWejoVs+TZ1pWJ6RGJblCbgknYUQIfSZE7TSGj+N/mQ84ZmkDfm2N4SOwK1T006xMIcRtGXoD/4Vd4qfd59h+5vYZMVVd7RnQwtgXpIqztIKUFhJGRPmUmQ4RmyHsDzi5CpJv/zHDpgLU6gpBPaFWF7B10q5OIYSZi/EpLN4TxZK90Vy+YRwXJOuMmGdb+fJw7SrSF6QUkjAiyg+D3jji6dHfjCHkzoHH7CtC7UeNAaTGwzLMuhAliFKKraev8MPOSDaciMNwa3TUyhVs6N/ch6ebV8fHTYZnL80kjIiyTSljH5Ajv8HR5ZAUe/u1Ch7G8BHU03j2i6W1dnUKIbJJSstk+f7zfL8jkvDLN03TW9Vw49lWvnSt64mNlYwLUhZIGBFlU8IFOLQYDv4C18JvT7dzgbq9ocFTxgAiHVCFKHEirtzk+x2R/B56nhu3Tst1tLHkyabVGBTiS0AVOXRa1kgYEWVHRiqc/AcO/AxnN94ehMzaAWr3gPpPQkAnsLLVtk4hRDYGg2Lz6ct8vyOSTScvm6bXqOzI4BBfnmhaDSc7ab0sqySMiNJNKYg5aAwgR341H4jMtw00GmhsCZEByIQokW6kZvBb6Hl+2HmOiCu3D8V0qO3OkNZ+tKvlLqfllgMSRkTplJZkDB/7vjNeETeLczVoNAAaPQNuNbSrTwhxT+GXk/hhRyS/hZ7nZrpxMEEnWyueaubD4BBfuVBdOSNhRJQusUeNAeTwMki/YZxmaQtBj0HjZ8G/vfQDEaKEMhgUG0/GsWhHJFtPXzFND6hSgSEhvvRtUg1HW/laKo9kr4uSLyMVwlYaQ0j07tvT3WpCs2HGVhAHN83KE0Lc243UDH7dd57vd0Zy7moyYBwbpFMdD4a29qNNQCV0OjkUU55JGBEl141Y2PN/xhCScs04zcIK6jxmDCH+7Yx/0YQQJVL0tWQW7Yhk2d5o01kxznZWPN2iOoNa+crYIMJEwogoeWIOwc65cPR3MBivM4GLDzQdAo0HgZOntvUJIXKllGJ/1HUWbItg9dFY0wBlNd0dGfaQP483roqDjXz1CHPyEyFKBoPBeHXcXXMhcuvt6T6tIGSksTVE+oIIUWJl6A38ezSWBdsiOBQdb5retlZlhj3kT3s5K0bcg4QRoS19JhxbDltnwOUTxmk6S6jXB1qNgmpNNS1PCHFvCckZLN4bxfc7IolJSAXAxsqCxxtVZdhD/tT2lAHKxP1JGBHayEwzjpC67Qu4HmmcZusMTYdCy5fApZqW1Qkh7iPiyk0Wbo/g133nSckwnppbuYItg1r5MrBVdSpXkMEFRd5JGBHFKz0Z9v8A27+EGxeN0+zdjIdimg8He1dNyxNC5E4pxc6zV/luWwTrT8ShbvUHqePpxPMP+dOrkTe2VnI4VeSfhBFRPDLT4cAPsPnT2xerc/KC1q8YW0NsZIAjIUqqtEw9fx2KYcG2CI7HJJqmd6pThecf8iekppyaKx6MhBFRtAx640ipGz+C+HPGaS7Voe1Y41Dtcp0YIUqs+OR0ft4dxaIdkVy+kQaAvbXxgnXPtfGjhrtcZkEUDgkjomgoBSf+hg0f3O6Y6lgF2r8BTQZLCBGiBIu+lsyCbREs3Rtt6g/i6WzHkNZ+DGjhg6uDjcYVirJGwogofNF7YPUEuLDP+NzOBdqMMXZMlcMxQpRYh6Lj+WbrWf49EmMaH6SulzMvtqvBow29sLa00LZAUWZJGBGFJz4K/ptsHKwMwNoRWv3P2C9EOqYKUSJlXS9m/paz7Im4ZpreLtCdl9rVoLX0BxHFQMKIeHBpScZTdHfOhsxUQAeNB0LHiTJaqhAlVGqGnpUHLvB/W88SfvkmANaWOnoFV+WFtv4EeTlrXKEoTySMiIJTCo7/Cf++dfs0Xd+HoPtH4BWsbW1CiBwlpGTw065zLNweyZUkY6dUJ1srnmlVneda++PpYqdxhaI8kjAiCub6OVj1Gpxea3zu6gtdP4CgnnLxOiFKoLgbqXy3LZKfd50zXbTO28WOYQ/507+5D0521hpXKMozCSMif/QZxsMxm6ZDZgpYWMNDY6DteLC217o6IcRdoq8lM39LOMv2nSc90wBAoEcFRrSvSc9gb+mUKkoECSMi76J2wV9j4PJx43Pfh+CxL8A9UNOyhBDZnYhN5OtN4fx1OAb9rVNjGld3ZeTDAXSqU0UuWidKFAkj4v7SbxrPktnzjfG5vRt0+xCCB8ghGSFKmNBz15i7MZz1J+JM09rWqszIhwNoVcNNzowRJZKEEXFvkdvhj5G3L2bX6Fno+j44uGlalhDiNqUUm09dZu6mcNPpuTodPFLfi/89XJP6VV00rlCIe5MwInKWfhPWT4XdXxufO1eFXrMgoLO2dQkhTPQGxaojMczbFE7YrWvGWFvq6Nu4Gi+1ryHDtYtSQ8KIyO78Plg+HK6dNT5vPMh4WMZO/rsSoiRIy9SzfP8F5m8OJ/JqMgAONpYMaFGdF9r64+UinclF6SJhRNxm0MP2mcaL2hkyja0hPWdBLWkNEaIkSM3Qs3hPFPM3nyU2MRUAVwdrhrb2Y0iIHxUd5ZoxonSSMCKMEi/C8hchcqvxeb3H4bGZMoy7ECVAcnomP++KYv6Ws6aByjyd7XihrT8DWlTH0Vb+lIvSTX6CBYRvgN9fgOSrYO0APT6Bxs/KmTJCaOxGagY/7DzHgm0RXLuZDkBVV3tGdqjJk02rYWtlqXGFQhQOCSPlmcEAW2fAxg8BBZ4N4MmFULmW1pUJUa4lpGSwaHsk322PICElAwDfSg6MejiAx5tUlYHKRJkjYaS8Sr4GK166PZx7kyHGFhFruS6FEFq5fjOdBdsi+H5HpGnI9hrujrzSMYCeDb2xkhAiyigJI+XRxQOwbDDER4GVHTz6ufEqu0IITVxJSuP/tp7lp53nuJmuB6C2hxMvdwzgkQZeWMpoqaKMK1DMnjt3Lv7+/tjZ2dG0aVO2bt16z/l//vlngoODcXBwwMvLi+eee46rV68WqGDxgI78Bgu6GYNIRX94fp0EESE0EpeYyvt/h/HQ9A3M33yWm+l66no58/WzTfh3dFt6BntLEBHlQr5bRpYuXcqYMWOYO3cubdq0Yf78+fTo0YOwsDCqV6+ebf5t27YxePBgvvjiC3r27MmFCxcYMWIEL7zwAitWrCiUjRB5oBRsmgabpxufB3aHx+fL2TJCaOBifApfbw5nyd5o08Xrgqu58GqnWnSsU0WGbBfljk4ppfLzhpYtW9KkSRPmzZtnmhYUFESfPn2YNm1atvk/++wz5s2bR3h4uGnaV199xSeffEJ0dHSe1pmYmIiLiwsJCQk4Ozvnp1wBkJECK/8Hx26Fv9avQufJYCE98YUoTtHXkpm7KZzfQqPJ0Bv/9DbzrcgrnWrRrlZlCSGizMnr93e+WkbS09MJDQ3lrbfeMpvetWtXduzYkeN7WrduzTvvvMOqVavo0aMHcXFx/Pbbbzz66KO5rictLY20tDSzjREFlHwNfukH5/eChZVx7JAmg7SuSohyJfLKTeZsPMPyAxdMV9BtVcONVzvVIqRGJQkhotzLVxi5cuUKer0eDw8Ps+keHh7Exsbm+J7WrVvz888/079/f1JTU8nMzKRXr1589dVXua5n2rRpTJkyJT+liZwkXIAfH4crJ8HOFZ7+Gfwe0roqIcqNyCs3+WrDGVYevB1C2taqzKudatHcTy42KUSWAnVgvTvFK6VyTfZhYWG8+uqrvPfee4SGhrJ69WoiIiIYMWJErsufMGECCQkJplteD+eIO1w5Dd91MwYRJ28YtkaCiBDF5NzVm7z26yE6fb6Z3/efR29QdKjtzoqRrfnx+ZYSRIS4S75aRipXroylpWW2VpC4uLhsrSVZpk2bRps2bXj99dcBaNiwIY6OjrRt25YPPvgALy+vbO+xtbXF1tY2P6WJO108AD89YRxRtVIADFoBrtk7FwshClfU1WS+2nDa7HBMh9rujO4cSCMfV22LE6IEy1cYsbGxoWnTpqxbt47HH3/cNH3dunX07t07x/ckJydjZWW+GktLY8fJfPadFXlx8SD80BtSE8C7MQz8DRwra12VEGVa9LVkZm84w+/7z5N5K4S0D3RnTOdaNK5eUePqhCj58n1q77hx4xg0aBDNmjUjJCSEb775hqioKNNhlwkTJnDhwgV++OEHAHr27Mnw4cOZN28e3bp1IyYmhjFjxtCiRQu8vb0Ld2vKu9gj8GMfYxDxaQnP/g62TlpXJUSZFX0tmTkbz/Bb6O0Q0u5WCGkiIUSIPMt3GOnfvz9Xr15l6tSpxMTEUL9+fVatWoWvry8AMTExREVFmeYfOnQoN27cYPbs2YwfPx5XV1c6duzI9OnTC28rhLGPyA+9IeU6VG1mbBGRICJEkTh/3RhCft13O4S0rVWZMZ0DaeorIUSI/Mr3OCNakHFG7iMxBhZ0hYQo8GoEQ/4EOxetqxKizLkQn3IrhNweJ+ShgMqM6VyLZtIpVYhsimScEVECpSbAz08Zg4hbDeOhGQkiQhSqi7dCyLI7QkibgEqM6RwoZ8YIUQgkjJRmmWmwZCBcOgKOVeDZ5dJZVYhCFJuQypyNZ1i6N5p0vXHY9tY1KzG6Uy1a1qikcXVClB0SRkorgwFWjIDIrWDjBM/+Bm7+WlclRJlwNSmNeZvC+WHXOdO1Y1rVcGNM50BaSQgRotBJGCmttnwCx5aDhTU8/RN4BWtdkRClXkJKBt9uPct32yK4ma4HoLlfRcZ1qU1ITQkhQhQVCSOl0bEVxivwAjz2OdR4WNNyhCjtktMzWbQjkvmbz5KQkgFAg6oujO8aSPtAd7l2jBBFTMJIaXPxIKz4n/Fxq1HQZLCm5QhRmqVl6lm8O4rZG8O5kmS8OGetKhUY3zWQbvU8JYQIUUwkjJQmNy7BkmcgMwUCOkOXqVpXJESplKk3sHz/Bb5cf5oL8SkAVHdzYEznWvRuVBVLCwkhQhQnCSOlhT4Tfh0CiRegUi14YgFYyu4TIj+UUqwNu8Qnq08QfvkmAB7OtrzaqRb9mvlgbVmga4cKIR6QfJuVFhveh6idxjNnBiwBe1etKxKiVNkXeY1p/54g9Nx1ACo6WDOqQwDPtvLFztpS4+qEKN8kjJQGp9bC9pnGx71nQ+UATcsRojQ5E3eD6atPsi7sEgB21ha88FANXmxfA2c7a42rE0KAhJGSL+E8rHjR+LjFi1Cvj6blCFFaxCakMvO/UyzbF41BgaWFjn7NfBjTuRYeznZalyeEuIOEkZJMnwG/Pme8+J1XI+j6gdYVCVHiJaZm8PWmcL7bHkFqhnHAsq51PXijex0CqlTQuDohRE4kjJRkmz6G83vA1gWeWgRWtlpXJESJlZap56ddUczecJrrycaxQpr6VmRCjzpyETshSjgJIyVV9F7Y9rnxca8vZah3IXKhlGLVkVg+Xn2c6GvG03RrujvyZvc6dKnrIWOFCFEKSBgpidJvwoqXQBmgYX+o97jWFQlRIh2Mjuf9v8NMZ8hUcbJlbJdAnmpaDSs5TVeIUkPCSEn032S4Fg5O3tDjE62rEaLEuRCfwierT/DHwYsA2Ftb8lL7GrzYrgYONvJnTYjSRn5rS5qo3bDnG+Pj3rNlPBEh7pCUlsm8TWf4dmsEaZkGdDp4okk1XutaG08XOUNGiNJKwkhJos+Av8caHzd6FgI6aVuPECWE3qBYti+aGWtPciUpHYBWNdx499G61K/qonF1QogHJWGkJNk1F+KOgb2bXHdGiFu2nr7Mh/8c50TsDQD8KzsyoYd0ThWiLJEwUlLERxlP5QXjeCKOlbStRwiNnb50g49WHWfjycsAuNhbM7pTLZ5t5YuNlXROFaIskTBSEigFq16HjGTwbQONntG6IiE0czUpjZn/neaXPVHoDQorCx2DQ/x4tVMArg42WpcnhCgCEkZKghN/w6nVYGENj30B0vQsyqG0TD2Ltkcye8MZbqRlAsaRU9/qUYca7jJyqhBlmYQRrWWkwuq3jY/bjAb32trWI0QxU0qx4UQc7/8dRuTVZADqeTvz7qN1CakphyuFKA8kjGhtzzeQEAVOXtB2vNbVCFGszsQlMfXvMLacMvYLcXey5Y1utXmiSTUsLKSFUIjyQsKIlpKvwZbPjI87TgQbB23rEaKYJKRkMGv9ab7fEUmmQWFtqWPYQ/680rEWFWzlz5IQ5Y381mtpy6eQlgAeDSD4aa2rEaLIZY0X8tmak1y9aRwvpHNQFd55tC7+lR01rk4IoRUJI1q5Gg57/s/4uOv7YGGpbT1CFLE9EdeY8tcxjl1MBIwXs3uvZz3aB7prXJkQQmsSRrSyfgoYMiCgC9TsoHU1QhSZi/EpTPv3BH8dMl5HxsnOijGdAxkc4ou1XMxOCIGEEW1E74GwP0BnISOtijIrNUPP/M1nmbf5DKkZxuvIPN28Oq91DaRSBVutyxNClCASRrSw4X3jfaOB4FFX21qEKGRKKf49GsuH/xznQnwKAM39KjKpZz25jowQIkcSRopb5HaI2GIc4Kz9m1pXI0ShOnXpBu/9cZRdZ68B4O1ix4RHgnisoZdcR0YIkSsJI8Vt0zTjfZNB4OqjbS1CFJKktEy+/O8U322PRG9Q2FpZ8FL7mvyvfU3sbaRzthDi3iSMFKfIbRC51dgqIgOciTJAKcVfh2P48J8wLiWmAcYh3Cc+VhcfNxk3RwiRNxJGilPWVXmbDAaXatrWIsQDOn3pBu/9cYydZ68C4FvJgck969GhThWNKxNClDYSRoqLWavIOK2rEaLAbqZlMmv9aRZsiyDz1iGZUR0CeLFdDeys5ZCMECL/JIwUF2kVEaWcUop/jsTwwd/HiU1MBaBzkAeTesohGSHEg5EwUhzO7TC2iljaSKuIKJXOxCUx6c+jbD9jPCRT3c2BST3r0inIQ+PKhBBlgYSR4rB9lvG+0TPSKiJKleT0TGatP8OCbWfJ0CtsrCwY+XBNRrSvKYdkhBCFRsJIUbt8Ek79C+gg5BWtqxEiz/4Lu8SkP4+ZBi7rVKcKk3rWo3olOSQjhChcEkaK2s7Zxvs6j0LlAG1rESIPYhJSmPznMdYcuwRAVVd7pvSqR+e6ckhGCFE0JIwUpRuX4NAS4+PW0ioiSja9QfH9jkhmrD3JzXQ9VhY6Xmhbg9GdasnAZUKIIiVhpCjt+Qb06VCtBVRvpXU1QuTqyPkEJqw4zNELiQA0qe7KR30bUMfTWePKhBDlgYSRopKZBqELjY9bv6xtLULk4kZqBjPWnuKHnZEYFDjbWfFmjzoMaF4dCwu5lowQonhIGCkqYX9A8lVwrgq1H9W6GiHMKKVYcyyWSX8eMw3j3ruRN+8+Whd3J1uNqxNClDcSRorKvu+M902GgKV8zKLkOH89mUl/HGP9iTjAOIz7+73r0y7QXePKhBDllXxLFoVLxyBqJ+gsjSOuClECZOgNLNwewRfrTpOSocfaUsdL7WrycscAGTNECKEpCSNFIatVpM6j4OylbS1CAAej43nr98OciL0BQAs/Nz58vD61PJw0rkwIISSMFL60JDi01Pi4+fPa1iLKveT0TGasPcXC7REYFLg6WPN2jyCebFpNOqgKIUoMCSOF7dgKSL8BlQLAv73W1YhybOvpy0xYfoTz140jqPZp5M3Ex+pSqYJ0UBVClCwSRgrb4VutIo2eAZ385ymKX3xyOu//fZzf958HwNvFjg/7NqBD7SoaVyaEEDmTMFKY4qOMV+dFBw36aV2NKGeUUvxzJIbJfx7jSlI6Oh0MCfHjtW61qWArv+pCiJJL/kIVpsPLjPd+D4Grj7a1iHIlJiGFiSuP8t9x4+m6AVUqMP2JhjT1rahxZUIIcX8SRgqLUrcP0QQ/rW0totwwGBS/7Ini439PkJSWibWljpEPBzCyQ01sreR0XSFE6SBhpLBc3A9XToGVHQT10roaUQ5EXU3mjd8PsevsNQAa+bgy/YmG1PaU03WFEKWLhJHCknU6b53HwE4uLiaKjsGg+Gn3OT7+9wTJ6XrsrS15vVtthrT2w1JO1xVClEISRgqDwQBhK42PG/bXtBRRtt3dGtLS341PnmyIbyVHjSsTQoiCkzBSGC4egKRLYOMENWRsEVH4cmoNeatHHQa18pXBy4QQpZ6EkcJw8h/jfUAnsJIBpUThktYQIURZZ1GQN82dOxd/f3/s7Oxo2rQpW7duvef8aWlpvPPOO/j6+mJra0vNmjX57rvvClRwiXTyX+N9nUe1rUOUKQaD4oedkXT/cgu7zl7D3tqSKb3qsXh4KwkiQogyJd8tI0uXLmXMmDHMnTuXNm3aMH/+fHr06EFYWBjVq1fP8T39+vXj0qVLLFiwgICAAOLi4sjMzHzg4kuEaxEQF2a8Qm9AZ62rEWWEtIYIIcoTnVJK5ecNLVu2pEmTJsybN880LSgoiD59+jBt2rRs869evZqnn36as2fP4ubmlqd1pKWlkZaWZnqemJiIj48PCQkJODuXsDNVds6FNRPAry0M/VvrakQpJ31DhBBlSWJiIi4uLvf9/s7XYZr09HRCQ0Pp2rWr2fSuXbuyY8eOHN/z559/0qxZMz755BOqVq1KYGAgr732GikpKbmuZ9q0abi4uJhuPj4leDTTk6uM97Uf0bYOUepFX0vmmW938d4fx0hO19PS343VY9oypLWfBBEhRJmWr8M0V65cQa/X4+HhYTbdw8OD2NjYHN9z9uxZtm3bhp2dHStWrODKlSuMHDmSa9eu5dpvZMKECYwbN870PKtlpMRJvgbnboWw2j20rUWUWkopft13nql/h5GUlimtIUKIcqdAZ9Po7roarVIq27QsBoMBnU7Hzz//jIuLCwCff/45Tz75JHPmzMHe3j7be2xtbbG1LQVnpZxeB0oPVeqCm7/W1YhS6PKNNCYsP2y6pkwz34rM6BcsfUOEEOVKvsJI5cqVsbS0zNYKEhcXl621JIuXlxdVq1Y1BREw9jFRSnH+/Hlq1apVgLJLCDlEIx7A6qMxvL3iKNdupmNjacG4roEMb1tDRlEVQpQ7+eozYmNjQ9OmTVm3bp3Z9HXr1tG6desc39OmTRsuXrxIUlKSadqpU6ewsLCgWrVqBSi5hMhMgzPrjY/rSBgReZeQksG4pQcZ8dN+rt1Mp46nE3+83IYR7WtKEBFClEv5Hmdk3LhxfPvtt3z33XccP36csWPHEhUVxYgRIwBjf4/Bgweb5n/mmWeoVKkSzz33HGFhYWzZsoXXX3+dYcOG5XiIptSI3ArpN6CCJ3g11roaUUpsO32F7jO3sPzABSx0MPLhmvzxchuCvErYWWJCCFGM8t1npH///ly9epWpU6cSExND/fr1WbVqFb6+vgDExMQQFRVlmr9ChQqsW7eOV155hWbNmlGpUiX69evHBx98UHhboYVTa4z3tbuDRYHGjhPlSEq6numrT7BoRyQAvpUc+LxfME1983a6uxBClGX5HmdEC3k9T7lYfdUUrp6B/j9D0GNaVyNKsCPnExi99ABnL98E4NlW1ZnQIwhHW7kagxCibMvr97f8NSyI+GhjENFZgn9brasRJZTBoPhm61lmrD1Jhl7h4WzLJ08G0z7QXevShBCiRJEwUhBnNxnvqzYFO5d7zirKp5iEFMYvO8SO8KsAdK/nybS+DajoaKNxZUIIUfJIGCmIsxuN9zU7aFuHKJFWH43hzd+PkJCSgb21JZN71aVfM59cx+IRQojyTsJIfikFkduMj2s8rGkpomS5mZbJ1L/CWLovGoAGVV348ulG1HCvoHFlQghRskkYya/rkZB0CSxtwLuJ1tWIEuLI+QReXXKAiCs30elgRPuajO0ciI2VnGklhBD3I2Ekv6L3GO+9GoG1naalCO0ppfh+RyQfrTpBut6Ap7Mdn/cPpnXNylqXJoQQpYaEkfyK3mW892mhbR1CcwkpGbz522FWHzNeHqFLXQ8+fbIhrg7SSVUIIfJDwkh+Re023ldvpW0dQlMHo+N5+Zf9nL+egrWljrcfCWJoaz/ppCqEEAUgYSQ/UhMgLsz42KeltrUITSilWLAtgumrT5ChV/i42TN7QBOCfVy1Lk0IIUotCSP5cX4voKCiP1SoonU1opjFJ6fz2q+H+e/4JQB61Pfk4yca4mJvrXFlQghRukkYyY+szqvSKlLuhJ67zquLD3AhPgUbSwvefSyIQa185bCMEEIUAgkj+RF1q/NqdQkj5YXBoPi/rWf5dM1JMg0K30oOzHmmCfWrysi7QghRWCSM5JU+Ey6EGh9Ly0i5cO1mOq/9eogNJ+IAeKyhF9P6NsDJTg7LCCFEYZIwkldxxyA9CWydwT1I62pEETsYHc/In0K5mJCKjZUFk3vWY0ALGdJdCCGKgoSRvDq/z3hfrRlYyKiaZZVSip93RzHlr2Nk6BU1Kjsy+5km1PXO/dLXQgghHoyEkby6eMB4791Y2zpEkUlJ1/POyiMs338BMF5p99OnGsphGSGEKGISRvIq5qDxXsJImXTu6k1e+jGUE7E3sNDBm93r8GK7GnJYRgghioGEkbzISIW448bHXo00LUUUvv/CLjF22UFupGZSuYINswY0lmvLCCFEMZIwkhdxx8CQCQ6VwKWa1tWIQqI3KL5Yd4rZG88A0KS6K3MHNsXTRS6AKIQQxUnCSF5k9RfxagTSbF8mXLuZzuglB9h6+goAQ1v78fYjQdhYSedkIYQobhJG8iLmkPHeu5GmZYjCcfRCAi/9GMqF+BTsrS2Z1rcBfRpX1bosIYQotySM5MWlY8Z7zwba1iEe2B8HL/Dm74dJzTDgW8mB+YOaUsdTTtsVQggtSRi5H4P+dudVj/ra1iIKTG9QfLL6BPO3nAWgfaA7s55ujIuDnLYrhBBakzByP9cjISMZrOzBrYbW1YgCiE9O55XFt/uH/O/hmrzWtTaWFtL/RwghSgIJI/cTe8R4XyUILCy1rUXk24nYRF78IZSoa8nYW1vy6VMNeayht9ZlCSGEuIOEkfvJ6i/iUU/bOkS+rToSw2u/HiI5XU+1ivZ8M6iZDOsuhBAlkISR+zGFEekvUlroDYrP151kzsZwANoEVGL2gCZUdLTRuDIhhBA5kTByP5eOGu+lZaRUSEjJYOzSg2w4EQfA8Lb+vNm9DlaWMn6IEEKUVBJG7iU1EeLPGR9LGCnxoq4m89yiPYRfvomtlQXTn2go44cIIUQpIGHkXrJO6XWuCg5u2tYi7in03DWG/xDKtZvpeLnY8X+Dm1G/qovWZQkhhMgDCSP3IodoSoW/Dl1k/K+HSM800KCqCwuGNKOKs1xfRgghSgsJI/ciYaREU0oxd1M4n645CUCXuh58+XQjHGzkx1oIIUoT+at9L3ImTYmVnmng7RVH+C30PAAvPOTPhEeCZCAzIYQohSSM5MZggEthxsfSMlKiJCRnMOKnUHaevYqFDqb0rs+gVr5alyWEEKKAJIzkJvE8pN8ASxuoFKB1NeKW6GvJDF1oPGPG0caS2QOb0KF2Fa3LEkII8QAkjOTm6hnjfUV/sJSLqZUEYRcTGbJwD5dvpOHlYseCIc1lRFUhhCgDJIzk5qpx9E4q1dS2DgHArrNXGf79Pm6kZVLbw4nvh7XA00XOmBFCiLJAwkhussKIXKlXc6uPxvDqkoOkZxpo4efG/w1phou9tFYJIURZIWEkN9eyWkakv4iWftp1jol/HEUp6FrXg1kDGmNnLVdPFkKIskTCSG7kMI2mlFLM/O80X64/DcCAFtX5oE99OXVXCCHKIAkjOdFn3L4mjZuEkeKmNygm/nGUX3ZHAfBqp1qM7VwLnU6CiBBClEUSRnISHwWGTLCyBycvraspV1Iz9IxZcpDVx2LR6WCqjCEihBBlnoSRnNzZedVCLj1fXBJSMhj+wz72RFzDxtKCmU834pEGEgaFEKKskzCSk2vSX6S4xSWmMvi7PZyIvYGTrRXfDG5GSM1KWpclhBCiGEgYyYl0Xi1W0deSGfjtbqKuJePuZMv3z7WQwcyEEKIckTCSk6zRV6XzapE7E3eDZ7/dQ2xiKtXdHPjp+ZZUr+SgdVlCCCGKkYSRnMhhmmJx5HwCg7/bzfXkDGpVqcBPL7TEw1lGVRVCiPJGwsjdMtMgwXhZemkZKTp7Iq7x/KK93EjLpGE1F75/rgUVHW20LksIIYQGJIzc7XokKAPYOEEFuRpsUdh0Mo4RP4WSmmGgpb8b3w5phpOdDO8uhBDllYSRu2X1F6lUA2SQrUL3z+EYxiw9QIZe0bFOFeYObCLDuwshRDknYeRupjFG5BBNYVu2N5q3lh/GoOCxhl583q8RNlYyjosQQpR3EkbuJp1Xi8SPu84xceVRAAa08OGDPg3kOjNCCCEACSPZxUcb711lCPLC8t22CKb+HQbA8w/58+6jQXKdGSGEECYSRu6WeMF471JN2zrKiPmbw5n27wkARrSvyZvda0sQEUIIYUbCyJ2Uun1ar4SRBzZ7w2k+W3sKgFc7BjC2S6AEESGEENlIGLlTagKkJxkfO1fVtpZSTCnFF/+dZtb60wCM6xLIq51qaVyVEEKIkkrCyJ2yDtHYu4GNDEleEEopPl1zkrmbjB2B3+pRhxHtpTOwEEKI3BXovMq5c+fi7++PnZ0dTZs2ZevWrXl63/bt27GysqJRo0YFWW3RMx2ikVaRglBK8dGq46YgMvGxuhJEhBBC3Fe+w8jSpUsZM2YM77zzDgcOHKBt27b06NGDqKioe74vISGBwYMH06lTpwIXW+RMYcRH2zpKIaUUU/4K4/+2RgAwtXc9nn/IX+OqhBBClAb5DiOff/45zz//PC+88AJBQUHMnDkTHx8f5s2bd8/3vfTSSzzzzDOEhITcdx1paWkkJiaa3YpF1mEa6S+SLwaD4p2VR1m0IxKdDqb1bcDgED+tyxJCCFFK5CuMpKenExoaSteuXc2md+3alR07duT6voULFxIeHs6kSZPytJ5p06bh4uJiuvn4FFNLhZxJk28Gg+Kt5Yf5ZXcUOh188kRDBrSornVZQgghSpF8hZErV66g1+vx8PAwm+7h4UFsbGyO7zl9+jRvvfUWP//8M1ZWeesvO2HCBBISEky36Ojo/JRZcAkyxkh+GFtEjrBs33ksdPBFv0Y81UwOcQkhhMifAp1Nc/dYEUqpHMeP0Ov1PPPMM0yZMoXAwMA8L9/W1hZbW9uClPZgEm6FHjlMc19KKSb9eYzFe6KNQaR/I3o3ks9NCCFE/uUrjFSuXBlLS8tsrSBxcXHZWksAbty4wb59+zhw4AAvv/wyAAaDAaUUVlZWrF27lo4dOz5A+YVIKbhxa7ucvbWtpYRTSjH17zB+3HUOnQ4+fTJYgogQQogCy9dhGhsbG5o2bcq6devMpq9bt47WrVtnm9/Z2ZkjR45w8OBB023EiBHUrl2bgwcP0rJlywervjClxoM+zfi4QvZgJYyUUkz79wQLt0cC8HHfBjzRVA5rCSGEKLh8H6YZN24cgwYNolmzZoSEhPDNN98QFRXFiBEjAGN/jwsXLvDDDz9gYWFB/fr1zd5fpUoV7Ozssk3XXFariH1FsLbTtpYSSinFZ2tP8s2WswB80Kc+/ZtLZ1UhhBAPJt9hpH///ly9epWpU6cSExND/fr1WbVqFb6+xqvcxsTE3HfMkRLpRozxvoKntnWUYF+uP82cjcYBzSb3rMuzreTKxkIIIR6cTimltC7ifhITE3FxcSEhIQFnZ+eiWcnBxbByBNToAINXFs06SrE5G8/w6ZqTALz7aBAvtK2hcUVCCCFKurx+fxdoOPgyKatlxMlL2zpKoG+2hJuCyJvd60gQEUIIUagkjGTJ6jPiJJ1X77RoewQfrToBwPgugfzvYbnWjBBCiMIlYSRLUlYYkZaRLCsPXGDyX2EAvNoxgFc61dK4IiGEEGWRhJEsppYR6cAKsPFkHK/9egiA59r4MbZL3getE0IIIfJDwkiWrDAiZ9OwP+o6I3/aT6ZB0aeRNxMfrZvjCLtCCCFEYZAwAuajr5bzlpHTl24wbNFeUjL0tA9055Mng7GwkCAihBCi6EgYAUi5fnv01XIcRi7EpzD4uz3EJ2fQyMeVec82wcZKfkSEEEIULfmmAUi+Zry3dQYrDS7QVwJcu5nO4AW7iUlIJaBKBRYObY6DTYGuoyiEEELki4QRgPQk471NBW3r0EhKup5hi/YSfvkmXi52/DCsBRUdbbQuSwghRDkhYQQg/abx3sZB2zo0oDcoRi85wMHoeFwdrPnx+RZ4u9prXZYQQohyRMIIQEay8d7GUds6NPDRquOsDbuEjaUF/ze4GQFVnLQuSQghRDkjYQRuH6axLl9h5IedkSzYFgHAZ/2Cae7npnFFQgghyiMJIwDp5a9lZP3xS0z+8xgAr3erTa9gb40rEkIIUV5JGIFy12fk6IUEXll8AIOC/s18GCnXmxFCCKEhCSMAGVlhpOyfTXMxPoVhi/aSnK7noYDKfPB4fRldVQghhKYkjMDtlhHrst0yciM1g2GL9hJ3I41AjwrMfbYJ1pbyIyCEEEJb8k0E5aLPSIbewKhfDnAi9gaVK9jy3dDmONtZa12WEEIIIWEEuGPQs7IZRpRSvPfHMbacuoydtQULhjSjWsWy3QokhBCi9JAwAmV+nJFvtpxl8Z4odDqY9XRjgn1ctS5JCCGEMJEwAmW6z8iaY7F8vPoEAO8+Wpeu9crvhQCFEEKUTBJG4I5Te8vW2TTHLiYwdulBlIJnW1VnWBs/rUsSQgghspEwAmVynJG4G6kM/36f6RTeST3rySm8QgghSiQJI1Dm+oykZuh58YdQLiakUqOyI3OekVN4hRBClFzyDQVl6to0Sine+O0wB6PjcbG3ZsHQ5rg4yCm8QgghSi4JI1Cmxhn5asMZ/jx0ESsLHfOebYJ/5dK/TUIIIco2CSNQZvqM/HM4hs/XnQLg/T71aV2zssYVCSGEEPcnYcSgh8wU4+NSfDbN4fPxjP/1IADD2vgzoEV1bQsSQggh8kjCSFbnVSi144xcSkxl+A/7SM0w0KG2O+88GqR1SUIIIUSeSRjJ6i+CDqztNS2lINIy9Yz4KZRLicaL380a0BhLCzmFVwghROkhYeTO69KUwnE43v87jANR8TjbWfF/g5vhJBe/E0IIUcpIGCnFY4ws2xfNT7uM15z5ckBjfCuVvm0QQgghJIyU0uvSHD4fz7srjwIwtnMgHWpX0bgiIYQQomAkjJTC69JcTUpjxI+hpGca6BxUhZc7BGhdkhBCCFFgEkZK2RgjmXoDry45wMWEVPwrO/J5/0ZYSIdVIYQQpZiEkVLWZ+TL9afZfuYqDjaWfP1sU5ylw6oQQohSTsJIVhgpBX1GdoZfZfbGMwBM69uA2p5OGlckhBBCPDgJI5lpxnsrW23ruI9rN9MZs/QASkG/ZtXo3aiq1iUJIYQQhULCSGaq8d7KTts67iHrSryXEtOo4e7I5F71tC5JCCGEKDQSRkpBy8iPu87x3/FL2Fha8NWAxjjYWGldkhBCCFFoJIyU8JaRk7E3+OCf4wBMeKQO9bxdNK5ICCGEKFwSRkpwy0happ4xSw+Snmm8AN7Q1n5alySEEEIUOgkjpjBS8lpGZv53muMxiVR0sGb6kw3RlcJr5wghhBD3I2GkhLaM7I28xvzN4YDxNN4qTiUvLAkhhBCFQcJIVp8Ry5ITRpLSMhm37CAGBU80qUb3+l5alySEEEIUGQkjpg6sJSeMfPB3GNHXUqjqas+kXnW1LkcIIYQoUhJGSlifkXVhl1iyNxqdDmb0C5bh3oUQQpR5EkZKUMvI1aQ0Jiw/DMALD/nTqkYljSsSQgghip6EkRLSMqKUYsLyI1xJSqe2hxPju9bWtB4hhBCiuEgY0ZeMMLJ8/wXWhl3C2lLHF/0bYWdtqWk9QgghRHGRMFICTu29kpTG+/+EATCmcyB1vZ01q0UIIYQobhJGSkCfkal/hRGfnEFdL2debFdDszqEEEIILUgY0bhlZOOJOP48dBELHUx/oiHWlrJLhBBClC/yzafhhfJS0vW8u/IoAM8/5E+DanIRPCGEEOWPhBENW0bmbQ7nQrxxcLOxXQKLff1CCCFESSBhRKOWkehryaZrz7zzaBAONlbFun4hhBCipCjfYUSfCcpgfFzMLSMfrTpOWqaBVjXc6FHfs1jXLYQQQpQk5TuMZLWKQLG2jOw4c4V/j8ZioYPJveqh0+mKbd1CCCFESVOgMDJ37lz8/f2xs7OjadOmbN26Ndd5ly9fTpcuXXB3d8fZ2ZmQkBDWrFlT4IILVVZ/ESi2q/Zm6g1M/usYAM+28qWOp4wpIoQQonzLdxhZunQpY8aM4Z133uHAgQO0bduWHj16EBUVleP8W7ZsoUuXLqxatYrQ0FA6dOhAz549OXDgwAMX/8CyWkYsrMGieBqJftp1jlOXknB1sGacdFoVQggh0CmlVH7e0LJlS5o0acK8efNM04KCgujTpw/Tpk3L0zLq1atH//79ee+99/I0f2JiIi4uLiQkJODsXIgtCVfD4asmYOMEb58vvOXmIjE1g/afbOR6cgbv96nPoFa+Rb5OIYQQQit5/f7OV3NAeno6oaGhdO3a1Wx6165d2bFjR56WYTAYuHHjBm5ubrnOk5aWRmJiotmtSBTzab3fbD7L9eQMaro7MqC5T7GsUwghhCjp8hVGrly5gl6vx8PDw2y6h4cHsbGxeVrGjBkzuHnzJv369ct1nmnTpuHi4mK6+fgU0Rd3MZ7WG5eYyoJtEQC83q0OVjLSqhBCCAEUsAPr3Wd/KKXydEbI4sWLmTx5MkuXLqVKlSq5zjdhwgQSEhJMt+jo6IKUeX/6dON9MbSMzNpwmpQMPY2ru9Ktnsf93yCEEEKUE/kaaaty5cpYWlpmawWJi4vL1lpyt6VLl/L888/z66+/0rlz53vOa2tri61tMRw6KaaWkYgrN1m8xxio3uxeR07lFUIIIe6Qr5YRGxsbmjZtyrp168ymr1u3jtatW+f6vsWLFzN06FB++eUXHn300YJVWhRMfUZsinQ1n609id6g6FDbnVY1KhXpuoQQQojSJt9jkI8bN45BgwbRrFkzQkJC+Oabb4iKimLEiBGA8RDLhQsX+OGHHwBjEBk8eDBffvklrVq1MrWq2Nvb4+Ki8YXhiqFl5PD5eP45HINOB290r1Nk6xFCCCFKq3yHkf79+3P16lWmTp1KTEwM9evXZ9WqVfj6Gk9TjYmJMRtzZP78+WRmZjJq1ChGjRplmj5kyBAWLVr04FvwIIrhbJrP1p4CoE+jqgR5yQBnQgghxN0KdHW2kSNHMnLkyBxfuztgbNq0qSCrKB5F3DKyJ+IaW05dxspCx9jOMsCZEEIIkZPyfX5pEbaMKKX4bO1JAPo196F6JYdCX4cQQghRFkgYgSJpGdl25gp7Iq5hY2XBKx0DCn35QgghRFlRzsNI1mGawm0ZMbaKGPuKDGxZHS8X+0JdvhBCCFGWlPMwcqtlpJCv2Lv+eByHouOxt7bkfw/XLNRlCyGEEGVNOQ8jhd8yYjAoZqwztooMae1HFaeiH2peCCGEKM3KeRgp/D4ja8NiOR6TSAVbK15qV6PQliuEEEKUVeU8jBTuqb1KKeZsDAdgaGs/KjoW7ciuQgghRFlQzsNI4Z7au/X0FY5cSMDe2pJhD/kXyjKFEEKIsq58hxF94R6mmbPxDAADWlTHTVpFhBBCiDwp0AisZUaXqdBmNDhXfeBF7Yu8xu6Ia1hb6hjeTlpFhBBCiLwq32HEtbrxVgjmbjL2FXmiSTUZV0QIIYTIh/J9mKaQHLuYwIYTcVjo4KX2Mq6IEEIIkR8SRgrBvFutIo829Ma/sqPG1QghhBCli4SRBxR1NZlVR2IAGCmjrQohhBD5JmHkAS3cEYFBQbtAd4K8nLUuRwghhCh1JIw8gMTUDJbtjQbgBRlXRAghhCgQCSMPYNneaG6m6wn0qEDbWpW1LkcIIYQolSSMFFCm3sDC7ZEADGvjj06n07YgIYQQopSSMFJAa45d4kJ8Cm6ONvRp/OCDpgkhhBDlVfke9OwBLNh2FoBnW1bHztpS42pEWaPX68nIyNC6DCGEuCdra2ssLR/8O1DCSAEciLrO/qh4bCwteDbEV+tyRBmilCI2Npb4+HitSxFCiDxxdXXF09PzgborSBgpgAXbIgDo1cibKk6Fc5E9IQBTEKlSpQoODg7SF0kIUWIppUhOTiYuLg4ALy+vAi9Lwkg+XYhP4d+jsYCx46oQhUWv15uCSKVKlbQuRwgh7sve3ngttri4OKpUqVLgQzbSgTWfvt8Rid6gaF2zEnW9ZZAzUXiy+og4ODhoXIkQQuRd1t+sB+nnJmEkH26mZbJ4TxQAz8sgZ6KIyKEZIURpUhh/sySM5MOv+6K5kZpJjcqOdKhdRetyhBBCiDJBwkge6Q2KhTsiAXjuIX8sLOS/VyFKE51Ox8qVK0vMcu4lNjaWLl264OjoiKura5Guqzht2rQJnU4nZ4uJbCSM5NH645c4dzUZF3trnmgig5wJcbfY2FheeeUVatSoga2tLT4+PvTs2ZP169drXVqBTJ48mUaNGmWbHhMTQ48ePYp03V988QUxMTEcPHiQU6dOFdpy/fz8mDlzZqEtL79at25NTEwMLi4umtVwPw8//DBjxozRuoxyR86myaOs03mfaVkdBxv52IS4U2RkJG3atMHV1ZVPPvmEhg0bkpGRwZo1axg1ahQnTpzQusRC4+npWeTrCA8Pp2nTptSqVavI11UQ6enp2NjY5Pt9NjY2xfL5FURGRgbW1tZal1F+qVIgISFBASohIUGT9R85H6983/xb1Zzwj4qJT9GkBlH2paSkqLCwMJWScvtnzGAwqJtpGZrcDAZDnmvv0aOHqlq1qkpKSsr22vXr15VSSkVERChAHThwwOw1QG3cuFEppdTGjRsVoFavXq0aNWqk7OzsVIcOHdSlS5fUqlWrVJ06dZSTk5N6+umn1c2bN03L8fX1VV988YXZeoODg9WkSZNMzwG1YsUK0/M33nhD1apVS9nb2yt/f3/17rvvqvT0dKWUUgsXLlSA2W3hwoXZltOqVSv15ptvmq03Li5OWVlZqQ0bNiillEpLS1Ovv/668vb2Vg4ODqpFixam7c2Jr6+v2XqHDBmilFIqPj5eDR8+XLm7uysnJyfVoUMHdfDgQdP7zpw5o3r16qWqVKmiHB0dVbNmzdS6detMr7dv3z7bNiml1KRJk1RwcLBZDV988YXy9fU1PR8yZIjq3bu3+uijj5SXl5fptfPnz6t+/fopV1dX5ebmpnr16qUiIiJy3bas/Zv1M7Fw4ULl4uKi/vrrLxUYGKjs7e3VE088oZKSktSiRYuUr6+vcnV1VS+//LLKzMw0+4ymTp2qBgwYoBwdHZWXl5eaNWuW2brOnTunevXqpRwdHZWTk5N66qmnVGxsrOn1rO1esGCB8vf3VzqdTg0ePDjbZxQREaEyMzPVsGHDlJ+fn7Kzs1OBgYFq5syZZuvL+ow+/fRT5enpqdzc3NTIkSNNP1NKKZWamqpef/11Va1aNWVjY6MCAgLUt99+a3r92LFjqkePHsrR0VFVqVJFPfvss+ry5cu5fp4lRU5/u7Lk9ftb/sXPg6xWkUcbeuHpIoOcieKTkqGn7ntrNFl32NRueWoFvHbtGqtXr+bDDz/E0dEx2+sF6fMwefJkZs+ejYODA/369aNfv37Y2tryyy+/kJSUxOOPP85XX33Fm2++me9lZ3FycmLRokV4e3tz5MgRhg8fjpOTE2+88Qb9+/fn6NGjrF69mv/++w8gx0MLAwcO5NNPP2XatGmmMwqWLl2Kh4cH7du3B+C5554jMjKSJUuW4O3tzYoVK+jevTtHjhzJseVj7969DB48GGdnZ7788kvs7e1RSvHoo4/i5ubGqlWrcHFxYf78+XTq1IlTp07h5uZGUlISjzzyCB988AF2dnZ8//339OzZk5MnT1K9enWWL19OcHAwL774IsOHD8/357V+/XqcnZ1Zt26dabCrDh060LZtW7Zs2YKVlRUffPAB3bt35/Dhw3luOUlOTmbWrFksWbKEGzdu0LdvX/r27YurqyurVq3i7NmzPPHEEzz00EP079/f9L5PP/2Ut99+m8mTJ7NmzRrGjh1LnTp16NKlC0op+vTpg6OjI5s3byYzM5ORI0fSv39/Nm3aZFrGmTNnWLZsGb///juWlpb4+vpy+vRp6tevz9SpUwFwd3fHYDBQrVo1li1bRuXKldmxYwcvvvgiXl5e9OvXz7S8jRs34uXlxcaNGzlz5gz9+/enUaNGps978ODB7Ny5k1mzZhEcHExERARXrlwBjIf/2rdvz/Dhw/n8889JSUnhzTffpF+/fmzYsCHf+6u0kTByH5cSU/nr0EVATucVIidnzpxBKUWdOnUKbZkffPABbdq0AeD5559nwoQJhIeHU6NGDQCefPJJNm7c+EBh5N133zU99vPzY/z48SxdupQ33ngDe3t7KlSogJWV1T0PK/Tv35+xY8eybds22rZtC8Avv/zCM888g4WFBeHh4SxevJjz58/j7e0NwGuvvcbq1atZuHAhH330UbZluru7Y2tri729vWndGzZs4MiRI8TFxWFrawvAZ599xsqVK/ntt9948cUXCQ4OJjg42OwzXLFiBX/++Scvv/wybm5uWFpa4uTkVKBDJY6Ojnz77bemkPHdd99hYWHBt99+awpiCxcuxNXVlU2bNtG1a9c8LTcjI4N58+ZRs2ZNwLhvf/zxRy5dukSFChWoW7cuHTp0YOPGjWZhpE2bNrz11lsABAYGsn37dr744gu6dOnCf//9x+HDh4mIiMDHxweAH3/8kXr16rF3716aN28OGA83/fjjj7i7u5uWa2Njg4ODg9lnZGlpyZQpU0zP/f392bFjB8uWLTMLIxUrVmT27NlYWlpSp04dHn30UdavX8/w4cM5deoUy5YtY926dXTu3BnA9PMMMG/ePJo0aWL2M/Hdd9/h4+PDqVOnCAwMzNPnWVpJGLmPH3ZGkmlQNPerSMNqrlqXI8oZe2tLwqZ202zdeaGUAgp3fJSGDRuaHnt4eODg4GD2h9vDw4M9e/Y80Dp+++03Zs6cyZkzZ0hKSiIzMxNn5/wNZOju7k6XLl34+eefadu2LREREezcuZN58+YBsH//fpRS2b5I0tLS8jXKbmhoKElJSdnek5KSQnh4OAA3b95kypQp/P3331y8eJHMzExSUlKIiorK1zblpkGDBmatHaGhoZw5cwYnJyez+VJTU0015YWDg4MpiIBx3/r5+VGhQgWzaVlDjmcJCQnJ9jyrc+7x48fx8fExBRGAunXr4urqyvHjx01hxNfX1yyI3MvXX3/Nt99+y7lz50hJSSE9PT1bB+d69eqZjUDq5eXFkSNHADh48CCWlpamFrO7hYaGsnHjRrPtzhIeHi5hpDxLSdfz8+6sQc5q3GduIQqfTqcr8R2ma9WqhU6n4/jx4/Tp0yfX+SwsjCfvZYUXyH3Exjs7Eup0umwdC3U6HQaDwWzZdy73XssG2LVrF08//TRTpkyhW7duuLi4sGTJEmbMmJHre3IzcOBARo8ezVdffcUvv/xCvXr1TC0UBoMBS0tLQkNDsw2TndOXTm4MBgNeXl5mhxiyZB0Ge/3111mzZg2fffYZAQEB2Nvb8+STT5Kenn7PZef1s7v7EJzBYKBp06b8/PPP2ebN6xc8kOO+vd/+zk1WIFZK5RiO756e02HFnCxbtoyxY8cyY8YMQkJCcHJy4tNPP2X37t333ZasurOGTc+NwWCgZ8+eTJ8+PdtrD3LNl9KiZP+V09jyA+eJT86gupsDXep6aF2OECWSm5sb3bp1Y86cObz66qvZ/sDHx8fj6upq+oKKiYmhcePGgPG/xcLg7u5OTEyM6XliYiIRERG5zr99+3Z8fX155513TNPOnTtnNo+NjQ16vf6+6+7Tpw8vvfQSq1ev5pdffmHQoEGm1xo3boxerycuLs50GKcgmjRpQmxsLFZWVvj5+eU4z9atWxk6dCiPP/44AElJSURGRt53m9zd3YmNjTX7os7LfmnSpAlLly6lSpUq+W5RKgy7du3K9jzrUGHdunWJiooiOjra1DoSFhZGQkICQUFB91xuTp/R1q1bad26NSNHjjRNy0/rDxhblgwGA5s3bzYdprlTkyZN+P333/Hz88PKqvx9Ncs4I7kwGBTf3eq4OrS1H5YyyJkQuZo7dy56vZ4WLVrw+++/c/r0aY4fP86sWbNMzen29va0atWKjz/+mLCwMLZs2WLWb+NBdOzYkR9//JGtW7dy9OhRhgwZcs8LdgUEBBAVFcWSJUsIDw9n1qxZrFixwmwePz8/IiIiOHjwIFeuXCEtLS3HZTk6OtK7d28mTpzI8ePHeeaZZ0yvBQYGMnDgQAYPHszy5cuJiIhg7969TJ8+nVWrVuV5+zp37kxISAh9+vRhzZo1REZGsmPHDt5991327dtn2qbly5dz8OBBDh06xDPPPJOtNcHPz48tW7Zw4cIFU8fJhx9+mMuXL/PJJ58QHh7OnDlz+Pfff+9b08CBA6lcuTK9e/dm69atREREsHnzZkaPHs358+fzvG0FtX37dj755BNOnTrFnDlz+PXXXxk9ejRg/LwaNmzIwIED2b9/P3v27GHw4MG0b9+eZs2a3XO5fn5+7N69m8jISK5cuYLBYCAgIIB9+/axZs0aTp06xcSJE9m7d2++6vXz82PIkCEMGzaMlStXEhERwaZNm1i2bBkAo0aN4tq1awwYMIA9e/Zw9uxZ1q5dy7Bhw/IUiks7CSO52B5+hfDLN3GytaJfc5/7v0GIcszf35/9+/fToUMHxo8fT/369enSpQvr16839Z8AY4e8jIwMmjVrxujRo/nggw8KZf0TJkygXbt2PPbYYzzyyCP06dPHrB/C3Xr37s3YsWN5+eWXadSoETt27GDixIlm8zzxxBN0796dDh064O7uzuLFi3Nd3sCBAzl06BBt27alevXqZq8tXLiQwYMHM378eGrXrk2vXr3YvXu3WX+G+9HpdKxatYp27doxbNgwAgMDefrpp4mMjMTDw9hq+8UXX1CxYkVat25Nz5496datG02aNDFbztSpU4mMjKRmzZqmlqqgoCDmzp3LnDlzCA4OZs+ePbz22mv3rcnBwYEtW7ZQvXp1+vbtS1BQEMOGDSMlJaVYWkrGjx9PaGgojRs35v3332fGjBl062bsX5U1Sm7FihVp164dnTt3pkaNGixduvS+y33ttdewtLSkbt26uLu7ExUVxYgRI+jbty/9+/enZcuWXL161ayVJK/mzZvHk08+yciRI6lTpw7Dhw/n5s2bAHh7e7N9+3b0ej3dunWjfv36jB49GhcXF9MhzrJMp+4+WFgCJSYm4uLiQkJCQrE1B774wz7Whl1iaGs/JveqVyzrFOVbamoqERER+Pv7Y2cnp5ALkRs/Pz/GjBkjI6WWEPf625XX7++yH7cK4GJ8Cv8dvwTAs62q32duIYQQQjwICSM5WLwnCoOCkBqVCKjidP83CCGEEKLAyl+X3ftIzzSweE80AINCfDWuRgghxN3uPktIlH7SMnKXNcdiuZKURhUnWzmdVwghhCgGEkbu8uMu41gDA1pUx9pSPh4hhBCiqMm37R1Oxt5gT8Q1LC10DGghHVeFEEKI4iBh5A4/3WoV6VrXQ67OK4QQQhQTCSO3JKVlsny/cdTAQa2k46oQQghRXCSM3LLiwAVupuup4e5ISM28X01TCCGEEA9GwgjGKzn+tNN4iGZQK99CvRS6EKJkyBoivKQsR2tDhw6951WW7xYZGYlOpyu0ixvei1afcXFuY0kyefJkGjVqpGkNEkaAw+cTOHnpBnbWFvRtUk3rcoQolWJjY3nllVeoUaMGtra2+Pj40LNnT9avX691aQWS2x/omJgYevToUfwFaczHx4eYmBjq16+vdSlFpjxsY05B77XXXtP891QGPQP+OWK89HiXup642FtrXI0QpU9kZCRt2rTB1dWVTz75hIYNG5KRkcGaNWsYNWoUJ06c0LrEQuPp6al1CZqwtLQs09uenp6OjY1NqdxGvV6PTqcr8AX1KlSoQIUKFQq5qvwp9y0jSin+OWwMI4828NK4GiHuohSk39Tmlo9raI4cORKdTseePXt48sknCQwMpF69eowbN45du3YBOTeBx8fHo9Pp2LRpEwCbNm1Cp9OxZs0aGjdujL29PR07diQuLo5///2XoKAgnJ2dGTBgAMnJyabl+Pn5MXPmTLOaGjVqxOTJk3Ot+c033yQwMBAHBwdq1KjBxIkTycjIAGDRokVMmTKFQ4cOodPp0Ol0LFq0CDD/zzIkJIS33nrLbLmXL1/G2tqajRs3AsYvuTfeeIOqVavi6OhIy5YtTdubm4SEBF588UWqVKmCs7MzHTt25NChQ6ble3p68tFHH5nm3717NzY2Nqxduxa43aozf/58fHx8cHBw4KmnniI+Pj7Xda5evZqHHnoIV1dXKlWqxGOPPUZ4eLjp9bv3X9a+Wr9+Pc2aNcPBwYHWrVtz8uRJs+X+9ddfNG3aFDs7O2rUqMGUKVPIzMw0vX769GnatWuHnZ0ddevWZd26dff8bObPn0/VqlUxGAxm03v16sWQIUMACA8Pp3fv3nh4eFChQgWaN2/Of//9Zza/n58fH3zwAUOHDsXFxYXhw4dn20a9Xs/zzz+Pv78/9vb21K5dmy+//NJsOVmHuz777DO8vLyoVKkSo0aNMv0sAaSlpfHGG2/g4+ODra0ttWrVYsGCBabXw8LCeOSRR6hQoQIeHh4MGjSIK1eu5PoZLFq0CFdXV/7++2/q1q2Lra0t586dY+/evXTp0oXKlSvj4uJC+/bt2b9/v9k2Azz++OPodDrT87tbAQ0GA1OnTqVatWrY2trSqFEjVq9efc/98qDKfcvIgeh4LsSn4GhjycO13bUuRwhzGcnwkbc26377Itg43ne2a9eusXr1aj788EMcHbPP7+rqmu9VT548mdmzZ+Pg4EC/fv3o168ftra2/PLLLyQlJfH444/z1Vdf8eabb+Z72VmcnJxYtGgR3t7eHDlyhOHDh+Pk5MQbb7xB//79OXr0KKtXrzZ9ibm4uGRbxsCBA/n000+ZNm2aqa/Z0qVL8fDwoH379gA899xzREZGsmTJEry9vVmxYgXdu3fnyJEj1KpVK9sylVI8+uijuLm5sWrVKlxcXJg/fz6dOnXi1KlTuLu7891339GnTx+6du1KnTp1ePbZZxk5ciRdu3Y1LefMmTMsW7aMv/76i8TERJ5//nlGjRrFzz//nOPncfPmTcaNG0eDBg24efMm7733Ho8//jgHDx6853/c77zzDjNmzMDd3Z0RI0YwbNgwtm/fDsCaNWt49tlnmTVrFm3btiU8PJwXX3wRgEmTJmEwGOjbty+VK1dm165dJCYm3vdKvE899RSvvvoqGzdupFOnTgBcv36dNWvW8NdffwGQlJTEI488wgcffICdnR3ff/89PXv25OTJk1SvfnsMqU8//ZSJEyfy7rvv5rgug8FAtWrVWLZsGZUrV2bHjh28+OKLeHl50a9fP9N8GzduxMvLi40bN3LmzBn69+9Po0aNGD58OACDBw9m586dzJo1i+DgYCIiIkxhIyYmhvbt2zN8+HA+//xzUlJSePPNN+nXrx8bNmzI9XNITk5m2rRpfPvtt1SqVIkqVaoQERHBkCFDmDVrFgAzZszgkUce4fTp0zg5ObF3716qVKnCwoUL6d69O5aWljku+8svv2TGjBnMnz+fxo0b891339GrVy+OHTuW489soVClQEJCggJUQkJCoS976l/HlO+bf6tXF+8v9GULkR8pKSkqLCxMpaSk3J6YlqTUJGdtbmlJeap79+7dClDLly+/53wREREKUAcOHDBNu379ugLUxo0blVJKbdy4UQHqv//+M80zbdo0Bajw8HDTtJdeekl169bN9NzX11d98cUXZusLDg5WkyZNMj0H1IoVK3Kt75NPPlFNmzY1PZ80aZIKDg7ONt+dy4mLi1NWVlZqy5YtptdDQkLU66+/rpRS6syZM0qn06kLFy6YLaNTp05qwoQJOdaxfv165ezsrFJTU82m16xZU82fP9/0fOTIkSowMFANHDhQ1a9f3+znZtKkScrS0lJFR0ebpv3777/KwsJCxcTEKKWUGjJkiOrdu3eun0dcXJwC1JEjR5RS2fdfTvvqn3/+UYCplrZt26qPPvrIbLk//vij8vLyUkoptWbNmhzrvN++6tWrlxo2bJjp+fz585Wnp6fKzMzM9T1169ZVX331lem5r6+v6tOnj9k8Of2M3m3kyJHqiSeeMD0fMmSI8vX1NVv3U089pfr376+UUurkyZMKUOvWrctxeRMnTlRdu3Y1mxYdHa0AdfLkyRzfs3DhQgWogwcP5lqnUkplZmYqJycn9ddff5mm5fTZ3v2z7u3trT788EOzeZo3b65GjhyZ43py/Nt1S16/v8t1y4jBoFh1q7/IYw01+u9TiHuxdjC2UGi17jxQtw7nFOZZaA0bNjQ99vDwMB1KuXPanj17Hmgdv/32GzNnzuTMmTMkJSWRmZmJs7Nzvpbh7u5Oly5d+Pnnn2nbti0RERHs3LmTefPmAbB//36UUgQGBpq9Ly0tjUqVch5CIDQ0lKSkpGyvp6SkmB02+eyzz6hfvz7Lli1j37592NmZD9RYvXp1qlW73SE/JCQEg8HAyZMnc+wXER4ezsSJE9m1axdXrlwxHQaJioq6Z4fOO/eVl5fxUHdcXBzVq1cnNDSUvXv38uGHH5rm0ev1pKamkpyczPHjx3Os834GDhzIiy++yNy5c7G1teXnn3/m6aefNv2nf/PmTaZMmcLff//NxYsXyczMJCUlhaioKLPlNGvW7L7r+vrrr/n22285d+4cKSkppKenZ+vYXK9ePbNWBi8vL44cOQLAwYMHsbS0NLWU3S00NJSNGzfm2GcjPDw8289OFhsbG7PPHoyf+3vvvceGDRu4dOkSer2e5OTkbNt9L4mJiVy8eJE2bdqYTW/Tpo3pUGFRKNdh5ED0dWISUnGytaJtrcpalyNEdjpdng6VaKlWrVrodDqOHz9+z1NFs5r61R19Ue48rn4na+vbHcl1Op3Z86xpd/YZsLCwMFvuvZYNsGvXLp5++mmmTJlCt27dcHFxYcmSJcyYMSPX9+Rm4MCBjB49mq+++opffvmFevXqERwcDBib+S0tLQkNDc3WJJ5bh0GDwYCXl1eO/UruPOR19uxZLl68iMFg4Ny5c9m+mO6WFRZzC409e/bEx8eH//u//8Pb2xuDwUD9+vVJT0+/53Lv3ldZ25B1P2XKFPr27ZvtfXZ2dtn22b3qu7tWg8HAP//8Q/Pmzdm6dSuff/656fXXX3+dNWvW8NlnnxEQEIC9vT1PPvlktm3J6bDinZYtW8bYsWOZMWMGISEhODk58emnn7J7926z+e7182lvb3/PdRgMBnr27Mn06dOzvZYV7nJib2+f7bMaOnQoly9fZubMmfj6+mJra0tISMh992FO7l62UqpIh70o12Hkr0NZZ9F4YGed87EzIcS9ubm50a1bN+bMmcOrr76a7Q98fHw8rq6uuLsb+2TFxMTQuHFjgEIbz8Hd3Z2YmBjT88TERCIiInKdf/v27fj6+vLOO++Ypp07d85sHhsbG/R6/X3X3adPH1566SVWr17NL7/8wqBBg0yvNW7cGL1eT1xcHG3bts3TtjRp0oTY2FisrKxMHQzvlp6ezsCBA+nfvz916tTh+eef58iRI3h43L7SeFRUFBcvXsTb29jqu3PnTiwsLHL8T/vq1ascP36c+fPnm+rctm1bnuq937acPHmSgICAHF+vW7dujnXej729PX379uXnn3/mzJkzBAYG0rRpU9PrW7duZejQoTz++OOAsQ9JZGRkvuvfunUrrVu3ZuTIkaZpd7ZO5UWDBg0wGAxs3ryZzp07Z3u9SZMm/P777/j5+WFl9WBfyVu3bmXu3Lk88sgjAERHR2frCGttbX3Pn2tnZ2e8vb3Ztm0b7dq1M03fsWMHLVq0eKD67qVcn01zIzUTSwsdjzaUs2iEeBBz585Fr9fTokULfv/9d06fPs3x48eZNWuWqdnd3t6eVq1a8fHHHxMWFsaWLVty7TiYXx07duTHH39k69atHD16lCFDhuTaOQ8gICCAqKgolixZQnh4OLNmzWLFihVm8/j5+REREcHBgwe5cuUKaWlpOS7L0dGR3r17M3HiRI4fP84zzzxjei0wMJCBAwcyePBgli9fTkREBHv37mX69OmsWrUqx+V17tyZkJAQ+vTpw5o1a4iMjGTHjh28++677Nu3DzB2Gk1ISGDWrFm88cYbBAUF8fzzz5stx87OjiFDhnDo0CG2bt3Kq6++Sr9+/XI8RFOxYkUqVarEN998w5kzZ9iwYQPjxo3L9fPLq/fee48ffviByZMnc+zYMY4fP87SpUtN+71z587Url2bwYMHm+q8MyDey8CBA/nnn3/47rvvePbZZ81eCwgIYPny5Rw8eJBDhw7xzDPPZDv7Ji8CAgLYt28fa9as4dSpU0ycOJG9e/fmaxl+fn4MGTKEYcOGsXLlSiIiIti0aRPLli0DYNSoUVy7do0BAwawZ88ezp49y9q1axk2bFiewvDd9f74448cP36c3bt3M3DgwGwtM35+fqxfv57Y2FiuX7+e43Jef/11pk+fztKlSzl58iRvvfUWBw8eZPTo0fmqJ1/u2aMkF3PmzFF+fn7K1tZWNWnSxKzzVk42bdqkmjRpomxtbZW/v7+aN29evtZXlB1YryalqbQMfaEvV4j8ulcnsNLg4sWLatSoUcrX11fZ2NioqlWrql69epk6pyqlVFhYmGrVqpWyt7dXjRo1UmvXrs2xA+v169dN71m4cKFycXExW9fdHe4SEhJUv379lLOzs/Lx8VGLFi26bwfW119/XVWqVElVqFBB9e/fX33xxRdm60lNTVVPPPGEcnV1VYBauHBhjstR6nbHzXbt2mX7XNLT09V7772n/Pz8lLW1tfL09FSPP/64Onz4cK6fZWJionrllVeUt7e3sra2Vj4+PmrgwIEqKipKbdy4UVlZWamtW7ea5j937pxycXFRc+fONft85s6dq7y9vZWdnZ3q27evunbtmuk9d3dgXbdunQoKClK2traqYcOGatOmTWbbmlsH1jv31YEDBxSgIiIiTNNWr16tWrdurezt7ZWzs7Nq0aKF+uabb0yvnzx5Uj300EPKxsZGBQYGqtWrV9+3A6tSxs6ZXl5e2To3Z9XaoUMHZW9vr3x8fNTs2bNV+/bt1ejRo03z5NTp+e5tTE1NVUOHDlUuLi7K1dVV/e9//1NvvfWW2c9eTh2BR48erdq3b296npKSosaOHau8vLyUjY2NCggIUN99953p9VOnTqnHH39cubq6Knt7e1WnTh01ZswYZTAYctz2nH4nlFJq//79qlmzZsrW1lbVqlVL/frrr9m2888//1QBAQHKyspK+fr6KqWy/z7p9Xo1ZcoUVbVqVWVtba2Cg4PVv//+m2MtWdv3oB1YdUrlYzABjKetDRo0iLlz59KmTRvmz5/Pt99+S1hYmNkpU1kiIiKoX78+w4cP56WXXmL79u2MHDmSxYsX88QTT+RpnYmJibi4uJCQkJDvDmZClBapqalERETg7++frTOiEPkxefJkVq5cWe6GNRfauNffrrx+f+f7MM3nn3/O888/zwsvvEBQUBAzZ87Ex8fH1Hv8bl9//TXVq1dn5syZBAUF8cILLzBs2DA+++yz/K5aCCGEEGVQvsJIeno6oaGhZgPrAHTt2pUdO3bk+J6dO3dmm79bt27s27cv197uaWlpJCYmmt2EEEIIUTblK4xcuXIFvV5v1mMbjOf8x8bG5vie2NjYHOfPzMzMdbjbadOm4eLiYrr5+Pjkp0whhCjXJk+eLIdoRKlSoLNp8nv+cU7z5zQ9y4QJE0hISDDdoqOjC1KmEEIIIUqBfJ3UXLlyZSwtLbO1gsTFxWVr/cji6emZ4/xWVla5jkBoa2uLra1tfkoToszIZ59yIYTQVGH8zcpXy4iNjQ1NmzbNdlXFdevW0bp16xzfExISkm3+tWvX0qxZs2yj1glRnmX9Ptx5NVohhCjpsv5mPch3er6Hexs3bhyDBg2iWbNmhISE8M033xAVFcWIESMA4yGWCxcu8MMPPwAwYsQIZs+ezbhx4xg+fDg7d+5kwYIFLF68uMBFC1EWWVpa4urqSlxcHAAODg5FOvyyEEI8CKUUycnJxMXF4erqes+BBu8n32Gkf//+XL16lalTpxITE0P9+vVZtWoVvr6+gHGo5zsvyuPv78+qVasYO3Ysc+bMwdvbm1mzZuV5jBEhypOs0TGzAokQQpR0rq6uOY7smx/5HvRMCzLomShv9Hr9PS/0JoQQJYG1tfU9W0Ty+v1dri+UJ0RJZWlp+UBNnkIIUZqU6wvlCSGEEEJ7EkaEEEIIoSkJI0IIIYTQVKnoM5LVx1auUSOEEEKUHlnf2/c7V6ZUhJEbN24AyDVqhBBCiFLoxo0buLi45Pp6qTi112AwcPHiRZycnAp1EKjExER8fHyIjo4ud6cMy7bLtsu2lw/ldbtBtr0kbLtSihs3buDt7Y2FRe49Q0pFy4iFhQXVqlUrsuU7OzuXux/ULLLtsu3lTXnd9vK63SDbrvW236tFJIt0YBVCCCGEpiSMCCGEEEJT5TqM2NraMmnSJGxtbbUupdjJtsu2lzflddvL63aDbHtp2vZS0YFVCCGEEGVXuW4ZEUIIIYT2JIwIIYQQQlMSRoQQQgihKQkjQgghhNBUuQ4jc+fOxd/fHzs7O5o2bcrWrVu1LqlQTZ48GZ1OZ3bz9PQ0va6UYvLkyXh7e2Nvb8/DDz/MsWPHNKy44LZs2ULPnj3x9vZGp9OxcuVKs9fzsq1paWm88sorVK5cGUdHR3r16sX58+eLcSsK5n7bPnTo0Gw/B61atTKbpzRu+7Rp02jevDlOTk5UqVKFPn36cPLkSbN5yup+z8u2l9X9Pm/ePBo2bGgazCskJIR///3X9HpZ3edw/20vzfu83IaRpUuXMmbMGN555x0OHDhA27Zt6dGjB1FRUVqXVqjq1atHTEyM6XbkyBHTa5988gmff/45s2fPZu/evXh6etKlSxfTtYBKk5s3bxIcHMzs2bNzfD0v2zpmzBhWrFjBkiVL2LZtG0lJSTz22GPo9fri2owCud+2A3Tv3t3s52DVqlVmr5fGbd+8eTOjRo1i165drFu3jszMTLp27crNmzdN85TV/Z6XbYeyud+rVavGxx9/zL59+9i3bx8dO3akd+/epsBRVvc53H/boRTvc1VOtWjRQo0YMcJsWp06ddRbb72lUUWFb9KkSSo4ODjH1wwGg/L09FQff/yxaVpqaqpycXFRX3/9dTFVWDQAtWLFCtPzvGxrfHy8sra2VkuWLDHNc+HCBWVhYaFWr15dbLU/qLu3XSmlhgwZonr37p3re8rKtsfFxSlAbd68WSlVvvb73duuVPnZ70opVbFiRfXtt9+Wq32eJWvblSrd+7xctoykp6cTGhpK165dzaZ37dqVHTt2aFRV0Th9+jTe3t74+/vz9NNPc/bsWQAiIiKIjY01+wxsbW1p3759mfsM8rKtoaGhZGRkmM3j7e1N/fr1y8TnsWnTJqpUqUJgYCDDhw8nLi7O9FpZ2faEhAQA3NzcgPK13+/e9ixlfb/r9XqWLFnCzZs3CQkJKVf7/O5tz1Ja93mpuFBeYbty5Qp6vR4PDw+z6R4eHsTGxmpUVeFr2bIlP/zwA4GBgVy6dIkPPviA1q1bc+zYMdN25vQZnDt3Totyi0xetjU2NhYbGxsqVqyYbZ7S/jPRo0cPnnrqKXx9fYmIiGDixIl07NiR0NBQbG1ty8S2K6UYN24cDz30EPXr1wfKz37PaduhbO/3I0eOEBISQmpqKhUqVGDFihXUrVvX9IValvd5btsOpXufl8swkkWn05k9V0plm1aa9ejRw/S4QYMGhISEULNmTb7//ntTp6ay/hncqSDbWhY+j/79+5se169fn2bNmuHr68s///xD3759c31fadr2l19+mcOHD7Nt27Zsr5X1/Z7btpfl/V67dm0OHjxIfHw8v//+O0OGDGHz5s2m18vyPs9t2+vWrVuq93m5PExTuXJlLC0tsyXBuLi4bIm6LHF0dKRBgwacPn3adFZNefgM8rKtnp6epKenc/369VznKSu8vLzw9fXl9OnTQOnf9ldeeYU///yTjRs3Uq1aNdP08rDfc9v2nJSl/W5jY0NAQADNmjVj2rRpBAcH8+WXX5aLfZ7btuekNO3zchlGbGxsaNq0KevWrTObvm7dOlq3bq1RVUUvLS2N48eP4+Xlhb+/P56enmafQXp6Ops3by5zn0FetrVp06ZYW1ubzRMTE8PRo0fL3Odx9epVoqOj8fLyAkrvtiulePnll1m+fDkbNmzA39/f7PWyvN/vt+05KSv7PSdKKdLS0sr0Ps9N1rbnpFTt82LvMltCLFmyRFlbW6sFCxaosLAwNWbMGOXo6KgiIyO1Lq3QjB8/Xm3atEmdPXtW7dq1Sz322GPKycnJtI0ff/yxcnFxUcuXL1dHjhxRAwYMUF5eXioxMVHjyvPvxo0b6sCBA+rAgQMKUJ9//rk6cOCAOnfunFIqb9s6YsQIVa1aNfXff/+p/fv3q44dO6rg4GCVmZmp1Wblyb22/caNG2r8+PFqx44dKiIiQm3cuFGFhISoqlWrlvpt/9///qdcXFzUpk2bVExMjOmWnJxsmqes7vf7bXtZ3u8TJkxQW7ZsUREREerw4cPq7bffVhYWFmrt2rVKqbK7z5W697aX9n1ebsOIUkrNmTNH+fr6KhsbG9WkSROz0+LKgv79+ysvLy9lbW2tvL29Vd++fdWxY8dMrxsMBjVp0iTl6empbG1tVbt27dSRI0c0rLjgNm7cqIBstyFDhiil8ratKSkp6uWXX1Zubm7K3t5ePfbYYyoqKkqDrcmfe217cnKy6tq1q3J3d1fW1taqevXqasiQIdm2qzRue07bDKiFCxea5imr+/1+216W9/uwYcNMf7fd3d1Vp06dTEFEqbK7z5W697aX9n2uU0qp4muHEUIIIYQwVy77jAghhBCi5JAwIoQQQghNSRgRQgghhKYkjAghhBBCUxJGhBBCCKEpCSNCCCGE0JSEESGEEEJoSsKIEEIIITQlYUSIcioyMhKdTsfBgwe1LsXkxIkTtGrVCjs7Oxo1apTjPEopXnzxRdzc3Epc/UKIgpEwIoRGhg4dik6n4+OPPzabvnLlSs0v562VSZMm4ejoyMmTJ1m/fn2O86xevZpFixbx999/ExMTQ/369Qtl3UOHDqVPnz6FsiwhRP5IGBFCQ3Z2dkyfPj3bJb1Ls/T09AK/Nzw8nIceeghfX18qVaqU6zxeXl60bt0aT09PrKysCry+oqDX6zEYDFqXIUSpImFECA117twZT09Ppk2blus8kydPznbIYubMmfj5+ZmeZ/1X/9FHH+Hh4YGrqytTpkwhMzOT119/HTc3N6pVq8Z3332XbfknTpygdevW2NnZUa9ePTZt2mT2elhYGI888ggVKlTAw8ODQYMGceXKFdPrDz/8MC+//DLjxo2jcuXKdOnSJcftMBgMTJ06lWrVqmFra0ujRo1YvXq16XWdTkdoaChTp05Fp9MxefLkbMsYOnQor7zyClFRUeh0OtNnoJTik08+oUaNGtjb2xMcHMxvv/1mep9er+f555/H398fe3t7ateuzZdffmn2GX///ff88ccf6HQ6dDodmzZtYtOmTeh0OuLj403zHjx4EJ1OR2RkJACLFi3C1dWVv//+m7p162Jra8u5c+dIT0/njTfeoGrVqjg6OtKyZUuzz/bcuXP07NmTihUr4ujoSL169Vi1alWOn50QZZ2EESE0ZGlpyUcffcRXX33F+fPnH2hZGzZs4OLFi2zZsoXPP/+cyZMn89hjj1GxYkV2797NiBEjGDFiBNHR0Wbve/311xk/fjwHDhygdevW9OrVi6tXrwIQExND+/btadSoEfv27WP16tVcunSJfv36mS3j+++/x8rKiu3btzN//vwc6/vyyy+ZMWMGn332GYcPH6Zbt2706tWL06dPm9ZVr149xo8fT0xMDK+99lqOy8gKNDExMezduxeAd999l4ULFzJv3jyOHTvG2LFjefbZZ9m8eTNgDELVqlVj2bJlhIWF8d577/H222+zbNkyAF577TX69etH9+7diYmJISYmhtatW+f5s09OTmbatGl8++23HDt2jCpVqvDcc8+xfft2lixZwuHDh3nqqafo3r27aXtHjRpFWloaW7Zs4ciRI0yfPp0KFSrkeZ1ClCnaXjRYiPJryJAhqnfv3koppVq1aqWGDRumlFJqxYoV6s5fzUmTJqng4GCz937xxRfK19fXbFm+vr5Kr9ebptWuXVu1bdvW9DwzM1M5OjqqxYsXK6WUioiIUID6+OOPTfNkZGSoatWqqenTpyullJo4caLq2rWr2bqjo6MVoE6ePKmUUqp9+/aqUaNG991eb29v9eGHH5pNa968uRo5cqTpeXBwsJo0adI9l3P3ticlJSk7Ozu1Y8cOs/mef/55NWDAgFyXM3LkSPXEE0+Ynt+5P7Js3LhRAer69eumaQcOHFCAioiIUEoptXDhQgWogwcPmuY5c+aM0ul06sKFC2bL69Spk5owYYJSSqkGDRqoyZMn33NbhSgvStbBViHKqenTp9OxY0fGjx9f4GXUq1cPC4vbjZ0eHh5mnTstLS2pVKkScXFxZu8LCQkxPbaysqJZs2YcP34cgNDQUDZu3Jjjf+zh4eEEBgYC0KxZs3vWlpiYyMWLF2nTpo3Z9DZt2nDo0KE8bmHOwsLCSE1NzXZ4KD09ncaNG5uef/3113z77becO3eOlJQU0tPTcz1jJ79sbGxo2LCh6fn+/ftRSpk+nyxpaWmmvjCvvvoq//vf/1i7di2dO3fmiSeeMFuGEOWJhBEhSoB27drRrVs33n77bYYOHWr2moWFBUops2kZGRnZlmFtbW32XKfT5TgtL50rs87mMRgM9OzZk+nTp2ebx8vLy/TY0dHxvsu8c7lZlFIPfOZQ1vb8888/VK1a1ew1W1tbAJYtW8bYsWOZMWMGISEhODk58emnn7J79+57Ljsr3N35+ef02dvb25tth8FgwNLSktDQUCwtLc3mzQp2L7zwAt26deOff/5h7dq1TJs2jRkzZvDKK6/kddOFKDMkjAhRQnz88cc0atQo23/T7u7uxMbGmn1xF+bYGrt27aJdu3YAZGZmEhoayssvvwxAkyZN+P333/Hz83ugs1acnZ3x9vZm27ZtpnUB7NixgxYtWjxQ/VmdRqOiomjfvn2O82zdupXWrVszcuRI07Tw8HCzeWxsbNDr9WbT3N3dAWN/looVKwJ5++wbN26MXq8nLi6Otm3b5jqfj4+PqS/PhAkT+L//+z8JI6Jckg6sQpQQDRo0YODAgXz11Vdm0x9++GEuX77MJ598Qnh4OHPmzOHff/8ttPXOmTOHFStWcOLECUaNGsX169cZNmwYYOxkee3aNQYMGMCePXs4e/Ysa9euZdiwYdm+uO/n9ddfZ/r06SxdupSTJ0/y1ltvcfDgQUaPHv1A9Ts5OfHaa68xduxYvv/+e8LDwzlw4ABz5szh+++/ByAgIIB9+/axZs0aTp06xcSJE02dX7P4+flx+PBhTp48yZUrV8jIyCAgIAAfHx8mT57MqVOn+Oeff5gxY8Z9awoMDGTgwIEMHjyY5cuXExERwd69e5k+fbrpjJkxY8awZs0aIiIi2L9/Pxs2bCAoKOiBPgshSisJI0KUIO+//362QzJBQUHMnTuXOXPmEBwczJ49e3I806SgPv74Y6ZPn05wcDBbt27ljz/+oHLlygB4e3uzfft29Ho93bp1o379+owePRoXFxez/il58eqrrzJ+/HjGjx9PgwYNWL16NX/++Se1atV64G14//33ee+995g2bRpBQUF069aNv/76C39/fwBGjBhB37596d+/Py1btuTq1atmrSQAw4cPp3bt2jRr1gx3d3e2b9+OtbU1ixcv5sSJEwQHBzN9+nQ++OCDPNW0cOFCBg8ezPjx46lduza9evVi9+7d+Pj4AMbTjUeNGkVQUBDdu3endu3azJ0794E/CyFKI526+y+fEEIIIUQxkpYRIYQQQmhKwogQQgghNCVhRAghhBCakjAihBBCCE1JGBFCCCGEpiSMCCGEEEJTEkaEEEIIoSkJI0IIIYTQlIQRIYQQQmhKwogQQgghNCVhRAghhBCa+n8MlajXMpY7kQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(feature_importances)[::-1].cumsum(), label='Cumulative feature importance')\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum(), label='Cumulative explained variance ratio')\n",
    "plt.xlabel('Number of features')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sample number of 0 is 1469\n",
      "The original sample number of 1 is 18177\n",
      "The original sample number of -1 is 21946\n",
      "\n",
      "The current sample number of 0 is 21946\n",
      "The current sample number of 1 is 21946\n",
      "The current sample number of -1 is 21946\n"
     ]
    }
   ],
   "source": [
    "num_z = np.sum(y_train==0)\n",
    "num_p = np.sum(y_train==1)\n",
    "num_n = np.sum(y_train==-1)\n",
    "print('The original sample number of 0 is', num_z)\n",
    "print('The original sample number of 1 is', num_p)\n",
    "print('The original sample number of -1 is', num_n)\n",
    "\n",
    "# oversampling\n",
    "sm = SMOTE(random_state=random_state)\n",
    "X_train_os, y_train_os = sm.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "num_z = np.sum(y_train_os==0)\n",
    "num_p = np.sum(y_train_os==1)\n",
    "num_n = np.sum(y_train_os==-1)\n",
    "\n",
    "print('')\n",
    "print('The current sample number of 0 is', num_z)\n",
    "print('The current sample number of 1 is', num_p)\n",
    "print('The current sample number of -1 is', num_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip OS\n",
    "X_train_os, y_train_os = X_train_selected, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_tensor(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        X = np.float32(X)\n",
    "        X = torch.from_numpy(X)\n",
    "        # In pytorch, labels start from 0\n",
    "        # shift required\n",
    "        y = np.longlong(y) - y.min()\n",
    "        y = torch.from_numpy(y)\n",
    "        \n",
    "        self.X = X.to(device)\n",
    "        self.y = y.to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN class for random search\n",
    "class DNN_rs(nn.Module):\n",
    "    # Available activation functions: ReLU, Sigmoid, Tanh, LeakyReLU, ELU, SELU, Softplus, Softsign, LogSigmoid, PReLU, Softmin, Softmax, if the input is not in the list, ReLU will be used\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activition_layer=nn.ReLU()):\n",
    "        super(DNN_rs, self).__init__()\n",
    "        depth=len(hidden_sizes)\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i])) \n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            if activition_layer==\"ReLU\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activition_layer==\"Sigmoid\":\n",
    "                layers.append(nn.Sigmoid())\n",
    "            elif activition_layer==\"Tanh\":\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activition_layer==\"LeakyReLU\":\n",
    "                layers.append(nn.LeakyReLU())\n",
    "            elif activition_layer==\"ELU\":\n",
    "                layers.append(nn.ELU())\n",
    "            elif activition_layer==\"SELU\":\n",
    "                layers.append(nn.SELU())\n",
    "            elif activition_layer==\"Softplus\":\n",
    "                layers.append(nn.Softplus())\n",
    "            elif activition_layer==\"Softsign\":\n",
    "                layers.append(nn.Softsign())\n",
    "            elif activition_layer==\"LogSigmoid\":\n",
    "                layers.append(nn.LogSigmoid())\n",
    "            elif activition_layer==\"PReLU\":\n",
    "                layers.append(nn.PReLU())\n",
    "            elif activition_layer==\"Softmin\":\n",
    "                layers.append(nn.Softmin())\n",
    "            elif activition_layer==\"Softmax\":\n",
    "                layers.append(nn.Softmax())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear( hidden_sizes[-1], output_size))\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN_rs(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=360, out_features=100, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test output for DNN_rs\n",
    "input_size = X_train_scaled.shape[1]\n",
    "output_size = 3\n",
    "model_rs = DNN_rs(input_size, hidden_sizes=[100], output_size=output_size, activition_layer=\"PReLU\")\n",
    "print(model_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test functions\n",
    "def train(dataloader, model, loss_fn, optimizer):  \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>5f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # CrossEntropyLoss for multi-classification\n",
    "optimizer_rs = torch.optim.Adam(model_rs.parameters(),weight_decay=0.005)   # Adam optimizer for random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k fold cross validation function\n",
    "def Kfold_split(X_train, y_train, Shuffle_state, k=5):   # Split the data into training and validation sets\n",
    "    #example : X_k_train, y_k_train, X_k_val, y_k_val = Kfold_split(X_train, y_train, Shuffle_state)\n",
    "    kf = KFold(n_splits=k, random_state=Shuffle_state, shuffle=True)    # 5-fold cross validation\n",
    "    kf.get_n_splits(X_train)    \n",
    "    X_k_train = []\n",
    "    y_k_train = []\n",
    "    X_k_val = []\n",
    "    y_k_val = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):  # Split the data into training and validation sets\n",
    "        X_k_train.append(X_train[train_index])\n",
    "        y_k_train.append(y_train[train_index])\n",
    "        X_k_val.append(X_train[val_index])\n",
    "        y_k_val.append(y_train[val_index])\n",
    "    \n",
    "    return X_k_train, y_k_train, X_k_val, y_k_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_eval(true, pred, score_display=True, matrix_display=False, result_return=False):\n",
    "    Accuracy = accuracy_score(true, pred)\n",
    "    F1 = accuracy_score(true, pred)\n",
    "    Precision = accuracy_score(true, pred)\n",
    "    Recall = accuracy_score(true, pred)\n",
    "    \n",
    "    if score_display==True:\n",
    "        print(\"Accuracy: \" + str(Accuracy))\n",
    "        print(\"F1 score: \" + str(F1))\n",
    "        print(\"Recall score: \" + str(Recall))\n",
    "        print(\"Precision score: \" + str(Precision))\n",
    "        \n",
    "    if matrix_display==True:\n",
    "        label = ['Non-seizure', 'Transition','Seizure']\n",
    "        cm = confusion_matrix(true, pred)\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=label).plot()\n",
    "        plt.show(cm_display)\n",
    "    \n",
    "    if result_return:\n",
    "        return Accuracy, F1, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_dataloader_list = []\n",
    "val_dataloader_list = []\n",
    "\n",
    "X_k_train_list, y_k_train_list, X_k_val_list, y_k_val_list = Kfold_split(X_train_os, y_train_os, random_state)   # K-fold cross validation for DNN\n",
    "for i in range(5):\n",
    "    trainset_gpu = Data_tensor(X_k_train_list[i], y_k_train_list[i])\n",
    "    valset_gpu = Data_tensor(X_k_val_list[i], y_k_val_list[i])\n",
    "    train_dataloader_list.append(DataLoader(trainset_gpu, batch_size=batch_size, shuffle=True))\n",
    "    val_dataloader_list.append(DataLoader(valset_gpu, batch_size=batch_size, shuffle=True))\n",
    "    \n",
    "valset_gpu_k = Data_tensor(X_train_os, y_train_os)\n",
    "val_dataloader_k = DataLoader(valset_gpu_k, batch_size=batch_size, shuffle=True)\n",
    "testset_gpu = Data_tensor(X_test_selected, y_test)\n",
    "test_dataloader = DataLoader(testset_gpu, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_width=8\n",
    "max_width=512\n",
    "min_hl=1\n",
    "max_hl=3\n",
    "activition_list = [\"ReLU\", \"Tanh\", \"LeakyReLU\",\"Sigmoid\"]\n",
    "\n",
    "optimizer_list = [\"SGD\", \"Adam\"]\n",
    "min_learning_rate=0.0001\n",
    "max_learning_rate=0.5\n",
    "\n",
    "def get_hps():\n",
    "    num_hl = random.randint(min_hl, max_hl)\n",
    "    hl = []\n",
    "    for i in range(num_hl):\n",
    "        hl.append(random.randint(min_width, max_width))\n",
    "    alpha = np.power(10, random.uniform(-4, 1))\n",
    "    activition = random.choice(activition_list)\n",
    "    optimizer = random.choice(optimizer_list)\n",
    "    lr = random.uniform(min_learning_rate, max_learning_rate)\n",
    "    \n",
    "    return {'hl': hl, 'alpha': alpha, 'activition': activition, 'optimizer': optimizer, 'lr': lr}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "{'hl': [65, 20, 387], 'alpha': 0.002372174274717322, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.3682619599605898}\n",
      "loss: 1.084967  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.819485 \n",
      "\n",
      "loss: 0.813361  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 0.930218 \n",
      "\n",
      "loss: 0.933107  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.808825 \n",
      "\n",
      "loss: 0.810124  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.808517 \n",
      "\n",
      "loss: 0.809554  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.735090 \n",
      "\n",
      "loss: 0.695214  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.851597 \n",
      "\n",
      "loss: 0.861432  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.739834 \n",
      "\n",
      "loss: 0.722039  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.694718 \n",
      "\n",
      "loss: 0.700813  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.872041 \n",
      "\n",
      "loss: 0.891815  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.642200 \n",
      "\n",
      "loss: 0.644619  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.973160 \n",
      "\n",
      "loss: 0.972894  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.601396 \n",
      "\n",
      "loss: 0.564302  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.641366 \n",
      "\n",
      "loss: 0.659122  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.805793 \n",
      "\n",
      "loss: 0.787892  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.655087 \n",
      "\n",
      "loss: 0.684093  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.611082 \n",
      "\n",
      "loss: 0.596038  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.613044 \n",
      "\n",
      "loss: 0.623679  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.566927 \n",
      "\n",
      "loss: 0.565722  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.567380 \n",
      "\n",
      "loss: 0.581558  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.575596 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [387, 464, 287], 'alpha': 0.00027207846149075325, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.014995629997091364}\n",
      "loss: 1.193285  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.818066 \n",
      "\n",
      "loss: 0.821132  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.819188 \n",
      "\n",
      "loss: 0.820224  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.821424 \n",
      "\n",
      "loss: 0.812473  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.811700 \n",
      "\n",
      "loss: 0.818225  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.809663 \n",
      "\n",
      "loss: 0.796675  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.814194 \n",
      "\n",
      "loss: 0.795600  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.823293 \n",
      "\n",
      "loss: 0.826140  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.804274 \n",
      "\n",
      "loss: 0.837205  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.808736 \n",
      "\n",
      "loss: 0.799731  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.802931 \n",
      "\n",
      "loss: 0.808968  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.809179 \n",
      "\n",
      "loss: 0.813816  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.822034 \n",
      "\n",
      "loss: 0.790072  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.798675 \n",
      "\n",
      "loss: 0.801019  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.805285 \n",
      "\n",
      "loss: 0.794278  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.798004 \n",
      "\n",
      "loss: 0.800307  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.792976 \n",
      "\n",
      "loss: 0.784440  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.813730 \n",
      "\n",
      "loss: 0.789632  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.791014 \n",
      "\n",
      "loss: 0.804393  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.789268 \n",
      "\n",
      "loss: 0.767494  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.784016 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [127], 'alpha': 0.03363383896644277, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.3580382044999095}\n",
      "loss: 117.901085  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.840473 \n",
      "\n",
      "loss: 0.840633  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.835791 \n",
      "\n",
      "loss: 0.825403  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.828536 \n",
      "\n",
      "loss: 0.824992  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.826116 \n",
      "\n",
      "loss: 0.834593  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.824997 \n",
      "\n",
      "loss: 0.830744  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.832242 \n",
      "\n",
      "loss: 0.811279  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.837599 \n",
      "\n",
      "loss: 0.835528  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.834139 \n",
      "\n",
      "loss: 0.838982  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.829653 \n",
      "\n",
      "loss: 0.832346  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.825401 \n",
      "\n",
      "loss: 0.829517  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.835594 \n",
      "\n",
      "loss: 0.841643  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.837986 \n",
      "\n",
      "loss: 0.820327  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.824401 \n",
      "\n",
      "loss: 0.801102  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.825423 \n",
      "\n",
      "loss: 0.827486  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.831033 \n",
      "\n",
      "loss: 0.812876  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.830418 \n",
      "\n",
      "loss: 0.805217  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.837639 \n",
      "\n",
      "loss: 0.829320  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.829277 \n",
      "\n",
      "loss: 0.849824  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.831920 \n",
      "\n",
      "loss: 0.843816  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.827363 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [287, 222, 120], 'alpha': 0.017621595643491885, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.3794278028281707}\n",
      "loss: 3.479918  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [365], 'alpha': 0.01297394315557648, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.10773534967758364}\n",
      "loss: 39.543423  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [60, 55], 'alpha': 0.007936636741682485, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.30190264308030884}\n",
      "loss: 49.877220  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 5032.476156 \n",
      "\n",
      "loss: 1838.372559  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 510.646610 \n",
      "\n",
      "loss: 600.383301  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 232.742864 \n",
      "\n",
      "loss: 274.723846  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 182.738672 \n",
      "\n",
      "loss: 173.334152  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 367.746087 \n",
      "\n",
      "loss: 310.298859  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 230.843622 \n",
      "\n",
      "loss: 54.007965  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 99.332414 \n",
      "\n",
      "loss: 101.469337  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 86.488074 \n",
      "\n",
      "loss: 73.505112  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 109.833690 \n",
      "\n",
      "loss: 84.564972  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 79.657135 \n",
      "\n",
      "loss: 59.682190  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 49.666638 \n",
      "\n",
      "loss: 53.858917  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 54.007084 \n",
      "\n",
      "loss: 53.670540  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 37.270208 \n",
      "\n",
      "loss: 26.881670  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 34.159074 \n",
      "\n",
      "loss: 73.811707  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 43.190282 \n",
      "\n",
      "loss: 87.282204  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 151.947578 \n",
      "\n",
      "loss: 40.788090  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 676.738759 \n",
      "\n",
      "loss: 37.061382  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 130.581643 \n",
      "\n",
      "loss: 134.210861  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 115.756859 \n",
      "\n",
      "loss: 144.526962  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 60.190885 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [381], 'alpha': 0.019820946913584596, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.039492219019421244}\n",
      "loss: 29.543341  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 31.079622 \n",
      "\n",
      "loss: 23.213165  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 18.601168 \n",
      "\n",
      "loss: 1.990211  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 2.944234 \n",
      "\n",
      "loss: 10.389318  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 2.109195 \n",
      "\n",
      "loss: 2.524652  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 2.393126 \n",
      "\n",
      "loss: 0.949885  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 4.624870 \n",
      "\n",
      "loss: 2.047092  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 5.320458 \n",
      "\n",
      "loss: 0.764813  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.745672 \n",
      "\n",
      "loss: 0.785609  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.730869 \n",
      "\n",
      "loss: 0.744525  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.715402 \n",
      "\n",
      "loss: 0.728585  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.729257 \n",
      "\n",
      "loss: 0.704326  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1.230078 \n",
      "\n",
      "loss: 1.478693  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 3.024698 \n",
      "\n",
      "loss: 1.028053  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 1.928579 \n",
      "\n",
      "loss: 1.591435  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 2.224561 \n",
      "\n",
      "loss: 16.826790  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.689689 \n",
      "\n",
      "loss: 0.684344  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.600281 \n",
      "\n",
      "loss: 0.577672  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.539431 \n",
      "\n",
      "loss: 0.597636  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 1.469248 \n",
      "\n",
      "loss: 2.056264  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.611305 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [432, 329], 'alpha': 0.12376526094181511, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.35231546092384025}\n",
      "loss: 9.230151  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [346], 'alpha': 0.0013787685809762544, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.4276733287354719}\n",
      "loss: 28.806095  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [202], 'alpha': 0.0024539630296714055, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.18515346546172962}\n",
      "loss: 43.941406  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [351], 'alpha': 0.0021621663806048173, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.267116524527151}\n",
      "loss: 27.646336  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.862882 \n",
      "\n",
      "loss: 0.851828  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.826991 \n",
      "\n",
      "loss: 0.800536  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.814937 \n",
      "\n",
      "loss: 0.834708  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.818518 \n",
      "\n",
      "loss: 0.796060  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.815213 \n",
      "\n",
      "loss: 0.798919  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.811835 \n",
      "\n",
      "loss: 0.809064  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.835393 \n",
      "\n",
      "loss: 0.821648  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.813664 \n",
      "\n",
      "loss: 0.840125  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.812562 \n",
      "\n",
      "loss: 0.797769  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.810026 \n",
      "\n",
      "loss: 0.795595  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.817479 \n",
      "\n",
      "loss: 0.800687  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.826497 \n",
      "\n",
      "loss: 0.820501  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.816196 \n",
      "\n",
      "loss: 0.816414  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816875 \n",
      "\n",
      "loss: 0.801532  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.822076 \n",
      "\n",
      "loss: 0.787439  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.815219 \n",
      "\n",
      "loss: 0.828380  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.827821 \n",
      "\n",
      "loss: 0.827083  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.812220 \n",
      "\n",
      "loss: 0.801213  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.818878 \n",
      "\n",
      "loss: 0.818917  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.822695 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [91], 'alpha': 0.020478637537502738, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.3423386640698383}\n",
      "loss: 55.327866  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [125], 'alpha': 1.2853250450978235, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.13394366390025378}\n",
      "loss: 31.371216  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 16.243473 \n",
      "\n",
      "loss: 10.176921  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 39.5%, Avg loss: 49.543388 \n",
      "\n",
      "loss: 59.122505  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 55.121145 \n",
      "\n",
      "loss: 21.473301  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 35.835535 \n",
      "\n",
      "loss: 45.448391  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 38.991703 \n",
      "\n",
      "loss: 18.212460  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 85.905776 \n",
      "\n",
      "loss: 87.249718  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 117.868936 \n",
      "\n",
      "loss: 49.459797  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 63.857209 \n",
      "\n",
      "loss: 62.722252  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 14.8%, Avg loss: 502.733551 \n",
      "\n",
      "loss: 499.037720  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 113.498728 \n",
      "\n",
      "loss: 123.373550  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 87.832300 \n",
      "\n",
      "loss: 91.750519  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 64.007169 \n",
      "\n",
      "loss: 50.615448  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 87.800655 \n",
      "\n",
      "loss: 176.331131  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 106.793633 \n",
      "\n",
      "loss: 130.486420  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 35.768436 \n",
      "\n",
      "loss: 36.795208  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 44.893108 \n",
      "\n",
      "loss: 44.208206  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 17.233653 \n",
      "\n",
      "loss: 17.305195  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 24.007127 \n",
      "\n",
      "loss: 27.220261  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 74.821053 \n",
      "\n",
      "loss: 72.889420  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 46.447887 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [475], 'alpha': 5.182610512973461, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.32775378878091055}\n",
      "loss: 66.970970  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [460, 476], 'alpha': 0.16377942837573226, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.06990119673080061}\n",
      "loss: 1.117878  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 3.016449 \n",
      "\n",
      "loss: 3.030010  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 18.7%, Avg loss: 2.241906 \n",
      "\n",
      "loss: 2.188980  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 49.4%, Avg loss: 1.378623 \n",
      "\n",
      "loss: 1.566637  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 5.571088 \n",
      "\n",
      "loss: 5.611450  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 5.477416 \n",
      "\n",
      "loss: 5.655365  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.0%, Avg loss: 1.825534 \n",
      "\n",
      "loss: 1.987152  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.528557 \n",
      "\n",
      "loss: 2.512883  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 2.158375 \n",
      "\n",
      "loss: 2.161652  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.435345 \n",
      "\n",
      "loss: 1.625006  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 5.6%, Avg loss: 2.396343 \n",
      "\n",
      "loss: 2.420046  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.354737 \n",
      "\n",
      "loss: 1.276435  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 7.612082 \n",
      "\n",
      "loss: 7.627303  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 36.8%, Avg loss: 2.930387 \n",
      "\n",
      "loss: 2.909767  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 1.265349 \n",
      "\n",
      "loss: 1.243347  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 3.036662 \n",
      "\n",
      "loss: 3.225364  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 2.668854 \n",
      "\n",
      "loss: 2.646626  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.025015 \n",
      "\n",
      "loss: 0.999813  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 3.485653 \n",
      "\n",
      "loss: 3.327059  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 1.123517 \n",
      "\n",
      "loss: 1.143070  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 3.592187 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [295, 283, 142], 'alpha': 0.5433367219666622, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.18106202221243411}\n",
      "loss: 1.073473  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.008486 \n",
      "\n",
      "loss: 1.057162  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 0.855985 \n",
      "\n",
      "loss: 0.831426  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.996639 \n",
      "\n",
      "loss: 0.968902  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.879606 \n",
      "\n",
      "loss: 0.878140  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.828792 \n",
      "\n",
      "loss: 0.858199  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.034406 \n",
      "\n",
      "loss: 1.034413  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 0.871035 \n",
      "\n",
      "loss: 0.873245  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.898798 \n",
      "\n",
      "loss: 0.908704  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.931125 \n",
      "\n",
      "loss: 0.931222  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.864623 \n",
      "\n",
      "loss: 0.867974  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 0.898373 \n",
      "\n",
      "loss: 0.914660  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.975322 \n",
      "\n",
      "loss: 0.937764  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.918492 \n",
      "\n",
      "loss: 0.918110  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.994832 \n",
      "\n",
      "loss: 0.997859  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.124941 \n",
      "\n",
      "loss: 1.076962  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 3.215507 \n",
      "\n",
      "loss: 3.294898  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.889561 \n",
      "\n",
      "loss: 1.828222  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.857644 \n",
      "\n",
      "loss: 0.869386  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.284702 \n",
      "\n",
      "loss: 1.251012  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 0.982294 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [268], 'alpha': 0.029348044873745035, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.0765053755421957}\n",
      "loss: 29.475412  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.924785 \n",
      "\n",
      "loss: 0.913263  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.881928 \n",
      "\n",
      "loss: 0.871684  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.856720 \n",
      "\n",
      "loss: 0.857371  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.826135 \n",
      "\n",
      "loss: 0.828487  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.836548 \n",
      "\n",
      "loss: 0.837190  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.832068 \n",
      "\n",
      "loss: 0.810924  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.845367 \n",
      "\n",
      "loss: 0.828588  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.837143 \n",
      "\n",
      "loss: 0.847605  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.812934 \n",
      "\n",
      "loss: 0.826840  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.811769 \n",
      "\n",
      "loss: 0.823236  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.824502 \n",
      "\n",
      "loss: 0.825557  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.822961 \n",
      "\n",
      "loss: 0.806452  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.805524 \n",
      "\n",
      "loss: 0.794901  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.825938 \n",
      "\n",
      "loss: 0.825144  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.817615 \n",
      "\n",
      "loss: 0.820361  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.795139 \n",
      "\n",
      "loss: 0.795114  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.806284 \n",
      "\n",
      "loss: 0.778881  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.789206 \n",
      "\n",
      "loss: 0.776347  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.790878 \n",
      "\n",
      "loss: 0.773640  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.784810 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [413], 'alpha': 0.25258413428440013, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.19087148132461773}\n",
      "loss: 20.374077  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 418.472021 \n",
      "\n",
      "loss: 149.786713  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 123.961527 \n",
      "\n",
      "loss: 22.112099  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 26.994262 \n",
      "\n",
      "loss: 46.706757  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 4.383957 \n",
      "\n",
      "loss: 2.176000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 2.807416 \n",
      "\n",
      "loss: 4.147396  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 1.606139 \n",
      "\n",
      "loss: 12.102661  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 5.456131 \n",
      "\n",
      "loss: 1.770240  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 7.189760 \n",
      "\n",
      "loss: 25.640877  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 45.123510 \n",
      "\n",
      "loss: 21.094538  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 50.668312 \n",
      "\n",
      "loss: 111.775909  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 46.2%, Avg loss: 85.889336 \n",
      "\n",
      "loss: 51.342785  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 73.979397 \n",
      "\n",
      "loss: 51.379120  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 3.206712 \n",
      "\n",
      "loss: 4.755648  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 51.633468 \n",
      "\n",
      "loss: 170.202957  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 32.560148 \n",
      "\n",
      "loss: 282.989624  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 7.337306 \n",
      "\n",
      "loss: 0.901188  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 8.545345 \n",
      "\n",
      "loss: 0.900545  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 8.636438 \n",
      "\n",
      "loss: 0.957292  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 4.323571 \n",
      "\n",
      "loss: 2.270199  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 0.981029 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [278, 136], 'alpha': 7.167899178925419, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.3408870134763847}\n",
      "loss: 18.606503  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [392, 144, 401], 'alpha': 0.16025404222065995, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.2174391488094856}\n",
      "loss: 5.655415  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.9%, Avg loss: 1355.845757 \n",
      "\n",
      "loss: 1356.815186  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.8%, Avg loss: 37618.974274 \n",
      "\n",
      "loss: 12408.972656  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1314.662348 \n",
      "\n",
      "loss: 10851.622070  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.068720 \n",
      "\n",
      "loss: 1.084946  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 2.912616 \n",
      "\n",
      "loss: 2.933094  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 17.644311 \n",
      "\n",
      "loss: 1.008176  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 5.362480 \n",
      "\n",
      "loss: 1.204015  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.324007 \n",
      "\n",
      "loss: 0.971660  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.228915 \n",
      "\n",
      "loss: 1.301810  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.826111 \n",
      "\n",
      "loss: 0.849835  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 6.445784 \n",
      "\n",
      "loss: 0.823279  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.262638 \n",
      "\n",
      "loss: 1.393893  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 7.684716 \n",
      "\n",
      "loss: 7.513193  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.9%, Avg loss: 1.928854 \n",
      "\n",
      "loss: 23.013309  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.973373 \n",
      "\n",
      "loss: 0.971906  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 5.248825 \n",
      "\n",
      "loss: 39.502342  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.811955 \n",
      "\n",
      "loss: 0.831557  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 3.473285 \n",
      "\n",
      "loss: 19.338326  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816825 \n",
      "\n",
      "loss: 0.827755  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 0.817073 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [9, 496], 'alpha': 0.4078122870250983, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.2538900691347022}\n",
      "loss: 9.917511  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [453], 'alpha': 0.13381381972127276, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.18701953491031487}\n",
      "loss: 1.319112  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 24.347058 \n",
      "\n",
      "loss: 25.457537  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 15.648411 \n",
      "\n",
      "loss: 15.508863  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 10.733877 \n",
      "\n",
      "loss: 10.701026  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 16.501770 \n",
      "\n",
      "loss: 16.544136  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 11.735548 \n",
      "\n",
      "loss: 12.275949  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 12.058543 \n",
      "\n",
      "loss: 11.530308  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 7.804436 \n",
      "\n",
      "loss: 8.433035  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 18.807826 \n",
      "\n",
      "loss: 18.430748  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 6.591984 \n",
      "\n",
      "loss: 6.477719  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 18.647523 \n",
      "\n",
      "loss: 18.989376  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 7.330109 \n",
      "\n",
      "loss: 7.442426  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 14.503817 \n",
      "\n",
      "loss: 14.251526  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 46.3%, Avg loss: 9.524347 \n",
      "\n",
      "loss: 9.960943  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 17.554721 \n",
      "\n",
      "loss: 16.970015  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 6.698755 \n",
      "\n",
      "loss: 6.596843  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 14.984488 \n",
      "\n",
      "loss: 14.959307  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 46.1%, Avg loss: 12.843543 \n",
      "\n",
      "loss: 13.148132  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 15.254134 \n",
      "\n",
      "loss: 15.846380  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 8.018247 \n",
      "\n",
      "loss: 7.801578  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 22.673276 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [284], 'alpha': 5.854620354066272, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.24435399344367104}\n",
      "loss: 45.443623  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 306.138538 \n",
      "\n",
      "loss: 403.391907  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 1362.878432 \n",
      "\n",
      "loss: 1294.172729  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 458.576191 \n",
      "\n",
      "loss: 365.002655  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 227.087535 \n",
      "\n",
      "loss: 213.394318  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 133.847551 \n",
      "\n",
      "loss: 61.535156  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 189.327182 \n",
      "\n",
      "loss: 175.767303  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 133.837202 \n",
      "\n",
      "loss: 241.470810  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 46.249660 \n",
      "\n",
      "loss: 61.658394  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 46.365015 \n",
      "\n",
      "loss: 38.362385  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 44.507650 \n",
      "\n",
      "loss: 38.206764  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 167.020741 \n",
      "\n",
      "loss: 81.800400  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 438.944651 \n",
      "\n",
      "loss: 464.155609  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 335.384257 \n",
      "\n",
      "loss: 302.071991  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 144.892479 \n",
      "\n",
      "loss: 79.890892  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 299.892147 \n",
      "\n",
      "loss: 156.767929  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 460.438677 \n",
      "\n",
      "loss: 566.182861  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 1710.379937 \n",
      "\n",
      "loss: 1816.437256  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 352.355794 \n",
      "\n",
      "loss: 1581.917114  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1498.531203 \n",
      "\n",
      "loss: 830.336792  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 2999.459690 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [483], 'alpha': 0.006529859485637705, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.029056790730444357}\n",
      "loss: 47.771614  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 160398554369652111372713984.000000 \n",
      "\n",
      "loss: 151692049853551517504634880.000000  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [492, 48, 51], 'alpha': 0.45660217323948477, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.06428289310343142}\n",
      "loss: 8.800291  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [492, 289], 'alpha': 0.0006693215295907433, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.46446385515178823}\n",
      "loss: 0.966641  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.700709 \n",
      "\n",
      "loss: 0.668185  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.573459 \n",
      "\n",
      "loss: 0.568592  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.515984 \n",
      "\n",
      "loss: 0.498947  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.508018 \n",
      "\n",
      "loss: 0.506151  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.518545 \n",
      "\n",
      "loss: 0.502393  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.484099 \n",
      "\n",
      "loss: 0.475887  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.511647 \n",
      "\n",
      "loss: 0.487678  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.457534 \n",
      "\n",
      "loss: 0.446639  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.452996 \n",
      "\n",
      "loss: 0.421998  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.500030 \n",
      "\n",
      "loss: 0.515343  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.569698 \n",
      "\n",
      "loss: 0.618997  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.487218 \n",
      "\n",
      "loss: 0.460491  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.445185 \n",
      "\n",
      "loss: 0.438663  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.446317 \n",
      "\n",
      "loss: 0.423912  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.456797 \n",
      "\n",
      "loss: 0.452751  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.477143 \n",
      "\n",
      "loss: 0.442617  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.476872 \n",
      "\n",
      "loss: 0.485651  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.445962 \n",
      "\n",
      "loss: 0.438125  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.419901 \n",
      "\n",
      "loss: 0.397377  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.421409 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [361, 110, 373], 'alpha': 0.003618940614103234, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.4498170809130084}\n",
      "loss: 4.480542  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1788866.756944 \n",
      "\n",
      "loss: 1491953.125000  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 6354201.388889 \n",
      "\n",
      "loss: 4271661.500000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 1546332.305556 \n",
      "\n",
      "loss: 1777510.375000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 792892.800347 \n",
      "\n",
      "loss: 812075.812500  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 670805.222222 \n",
      "\n",
      "loss: 714107.250000  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 787049.663194 \n",
      "\n",
      "loss: 385755.000000  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 402600.550347 \n",
      "\n",
      "loss: 402483.406250  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 507972.656250 \n",
      "\n",
      "loss: 630191.812500  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 384973.784722 \n",
      "\n",
      "loss: 321312.812500  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 118584.089410 \n",
      "\n",
      "loss: 91427.125000  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 100701.104167 \n",
      "\n",
      "loss: 77724.109375  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 142836.532118 \n",
      "\n",
      "loss: 91631.687500  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 68284.957465 \n",
      "\n",
      "loss: 53466.894531  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 147548.996528 \n",
      "\n",
      "loss: 164019.750000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 256915.731771 \n",
      "\n",
      "loss: 90117.984375  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 61466.934462 \n",
      "\n",
      "loss: 40554.113281  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 90713.578993 \n",
      "\n",
      "loss: 61512.914062  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 62407.641276 \n",
      "\n",
      "loss: 105757.265625  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 345945.976128 \n",
      "\n",
      "loss: 681739.625000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 1960696.041667 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [69, 134], 'alpha': 0.0013288828148723235, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.29419552835677093}\n",
      "loss: 26.087244  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [309], 'alpha': 0.0012620801094670535, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.11454799772719608}\n",
      "loss: 100.418861  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.890171 \n",
      "\n",
      "loss: 0.894390  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.854282 \n",
      "\n",
      "loss: 0.843173  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.829826 \n",
      "\n",
      "loss: 0.841704  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.818533 \n",
      "\n",
      "loss: 0.833306  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.812814 \n",
      "\n",
      "loss: 0.815641  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.824380 \n",
      "\n",
      "loss: 0.822135  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.823830 \n",
      "\n",
      "loss: 0.844543  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.817620 \n",
      "\n",
      "loss: 0.823740  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816448 \n",
      "\n",
      "loss: 0.814273  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.824199 \n",
      "\n",
      "loss: 0.830590  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.814977 \n",
      "\n",
      "loss: 0.788935  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.825342 \n",
      "\n",
      "loss: 0.814524  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.810950 \n",
      "\n",
      "loss: 0.822233  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.815861 \n",
      "\n",
      "loss: 0.825134  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.804371 \n",
      "\n",
      "loss: 0.816963  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.819220 \n",
      "\n",
      "loss: 0.795288  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.828965 \n",
      "\n",
      "loss: 0.827424  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.813258 \n",
      "\n",
      "loss: 0.795213  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816917 \n",
      "\n",
      "loss: 0.806886  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.812975 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [448], 'alpha': 0.004488925992678658, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.33452199137031063}\n",
      "loss: 1.201518  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 8.993115 \n",
      "\n",
      "loss: 8.448677  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 26.3%, Avg loss: 8.947349 \n",
      "\n",
      "loss: 9.304650  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 11.184490 \n",
      "\n",
      "loss: 9.055231  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 13.901644 \n",
      "\n",
      "loss: 14.580455  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 47.1%, Avg loss: 17.181202 \n",
      "\n",
      "loss: 20.327591  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 5.378843 \n",
      "\n",
      "loss: 5.236335  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 12.847150 \n",
      "\n",
      "loss: 10.718234  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 6.931781 \n",
      "\n",
      "loss: 6.641467  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 4.7%, Avg loss: 31.938176 \n",
      "\n",
      "loss: 31.642225  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 19.401384 \n",
      "\n",
      "loss: 19.609005  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 22.922818 \n",
      "\n",
      "loss: 24.552341  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 23.070908 \n",
      "\n",
      "loss: 24.145535  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 12.149662 \n",
      "\n",
      "loss: 10.634052  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 14.975268 \n",
      "\n",
      "loss: 17.772999  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 4.510883 \n",
      "\n",
      "loss: 5.415688  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 34.370883 \n",
      "\n",
      "loss: 33.490948  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 34.551540 \n",
      "\n",
      "loss: 33.758038  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 7.307398 \n",
      "\n",
      "loss: 7.241174  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 5.002725 \n",
      "\n",
      "loss: 5.099209  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 16.036804 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [284], 'alpha': 0.00045873221879225867, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.3923312502029476}\n",
      "loss: 1.162722  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.738525 \n",
      "\n",
      "loss: 0.757625  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.732370 \n",
      "\n",
      "loss: 0.710696  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.754992 \n",
      "\n",
      "loss: 0.773918  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.514275 \n",
      "\n",
      "loss: 0.487793  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.728838 \n",
      "\n",
      "loss: 0.733406  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.511471 \n",
      "\n",
      "loss: 0.527216  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.698189 \n",
      "\n",
      "loss: 0.727796  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.565234 \n",
      "\n",
      "loss: 0.550539  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.629616 \n",
      "\n",
      "loss: 0.669622  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.574413 \n",
      "\n",
      "loss: 0.571273  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.492453 \n",
      "\n",
      "loss: 0.479743  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.540176 \n",
      "\n",
      "loss: 0.586828  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.528013 \n",
      "\n",
      "loss: 0.528988  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.506769 \n",
      "\n",
      "loss: 0.475212  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.506365 \n",
      "\n",
      "loss: 0.504327  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.618024 \n",
      "\n",
      "loss: 0.599678  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.515776 \n",
      "\n",
      "loss: 0.511526  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.561345 \n",
      "\n",
      "loss: 0.618321  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.484381 \n",
      "\n",
      "loss: 0.485250  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.469606 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [105, 56], 'alpha': 0.0003052488743843354, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.21184695364765838}\n",
      "loss: 0.934601  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 0.838518 \n",
      "\n",
      "loss: 0.823166  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.054582 \n",
      "\n",
      "loss: 1.044827  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.731880 \n",
      "\n",
      "loss: 0.752260  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.833549 \n",
      "\n",
      "loss: 0.813123  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.642897 \n",
      "\n",
      "loss: 1.629262  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.905664 \n",
      "\n",
      "loss: 0.907771  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.862784 \n",
      "\n",
      "loss: 0.871981  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.766465 \n",
      "\n",
      "loss: 0.759685  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.760578 \n",
      "\n",
      "loss: 0.770521  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.756240 \n",
      "\n",
      "loss: 0.754642  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.861133 \n",
      "\n",
      "loss: 0.863731  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.892102 \n",
      "\n",
      "loss: 0.886698  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.775232 \n",
      "\n",
      "loss: 0.797410  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.892820 \n",
      "\n",
      "loss: 0.897802  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.809709 \n",
      "\n",
      "loss: 0.790367  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 0.964876 \n",
      "\n",
      "loss: 0.976890  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.873938 \n",
      "\n",
      "loss: 0.874558  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.807638 \n",
      "\n",
      "loss: 0.796784  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 0.819315 \n",
      "\n",
      "loss: 0.820307  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 0.815269 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [450, 381], 'alpha': 0.00018657606794409573, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.20137037892292414}\n",
      "loss: 30.769863  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [417, 449], 'alpha': 0.00035183995885073016, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.26818924837978114}\n",
      "loss: 1.093248  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 56.138122 \n",
      "\n",
      "loss: 56.980743  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 17.624536 \n",
      "\n",
      "loss: 17.285095  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 29.204336 \n",
      "\n",
      "loss: 30.625076  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 10.767391 \n",
      "\n",
      "loss: 11.847076  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 18.267699 \n",
      "\n",
      "loss: 18.286430  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 9.289996 \n",
      "\n",
      "loss: 8.926990  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 10.325998 \n",
      "\n",
      "loss: 11.013218  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 22.821574 \n",
      "\n",
      "loss: 22.707048  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 2.320411 \n",
      "\n",
      "loss: 2.349070  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 9.591270 \n",
      "\n",
      "loss: 9.193237  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 9.591646 \n",
      "\n",
      "loss: 9.491426  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 14.337340 \n",
      "\n",
      "loss: 15.266678  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 13.925032 \n",
      "\n",
      "loss: 13.697679  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 20.273193 \n",
      "\n",
      "loss: 19.804493  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 5.840145 \n",
      "\n",
      "loss: 5.401006  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.926527 \n",
      "\n",
      "loss: 2.828370  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 4.046238 \n",
      "\n",
      "loss: 3.911763  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 3.104823 \n",
      "\n",
      "loss: 2.933313  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 3.621345 \n",
      "\n",
      "loss: 3.709036  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 6.761925 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [224], 'alpha': 0.0008268756043227057, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.4372555284398829}\n",
      "loss: 1.107637  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 1.108221 \n",
      "\n",
      "loss: 1.129744  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.649112 \n",
      "\n",
      "loss: 0.698036  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.752552 \n",
      "\n",
      "loss: 0.775493  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.636401 \n",
      "\n",
      "loss: 0.581788  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.597877 \n",
      "\n",
      "loss: 0.566593  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.532859 \n",
      "\n",
      "loss: 0.550251  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.555049 \n",
      "\n",
      "loss: 0.557108  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.640672 \n",
      "\n",
      "loss: 0.681732  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.531223 \n",
      "\n",
      "loss: 0.544690  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.593085 \n",
      "\n",
      "loss: 0.533209  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.777763 \n",
      "\n",
      "loss: 0.749567  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.590695 \n",
      "\n",
      "loss: 0.584674  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.717707 \n",
      "\n",
      "loss: 0.780302  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.586273 \n",
      "\n",
      "loss: 0.634620  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.680288 \n",
      "\n",
      "loss: 0.668489  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.653680 \n",
      "\n",
      "loss: 0.699598  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.606905 \n",
      "\n",
      "loss: 0.597427  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.583500 \n",
      "\n",
      "loss: 0.541600  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.843573 \n",
      "\n",
      "loss: 0.824997  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.806875 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [234], 'alpha': 1.096759042295366, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.3261072325098612}\n",
      "loss: 30.030247  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.012579 \n",
      "\n",
      "loss: 1.010208  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.016898 \n",
      "\n",
      "loss: 1.012833  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.011699 \n",
      "\n",
      "loss: 1.013288  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.014223 \n",
      "\n",
      "loss: 1.014363  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.010918 \n",
      "\n",
      "loss: 1.011451  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.013394 \n",
      "\n",
      "loss: 1.015687  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.014076 \n",
      "\n",
      "loss: 1.013626  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.013924 \n",
      "\n",
      "loss: 1.012151  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.013061 \n",
      "\n",
      "loss: 1.013083  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.011230 \n",
      "\n",
      "loss: 1.015998  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.013501 \n",
      "\n",
      "loss: 1.012987  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.013084 \n",
      "\n",
      "loss: 1.009862  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.014110 \n",
      "\n",
      "loss: 1.014075  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.010252 \n",
      "\n",
      "loss: 1.010066  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.012852 \n",
      "\n",
      "loss: 1.012699  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.012644 \n",
      "\n",
      "loss: 1.015164  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.012432 \n",
      "\n",
      "loss: 1.009037  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.012866 \n",
      "\n",
      "loss: 1.012027  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.011400 \n",
      "\n",
      "loss: 1.006900  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.012831 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [436, 15, 504], 'alpha': 0.00029262665363958537, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.20326638757044074}\n",
      "loss: 1.079155  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.826305 \n",
      "\n",
      "loss: 0.822417  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.780195 \n",
      "\n",
      "loss: 0.772800  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.696138 \n",
      "\n",
      "loss: 0.689373  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.694613 \n",
      "\n",
      "loss: 0.699770  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.859190 \n",
      "\n",
      "loss: 0.899565  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.668820 \n",
      "\n",
      "loss: 0.648432  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.533505 \n",
      "\n",
      "loss: 0.535942  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.536494 \n",
      "\n",
      "loss: 0.511390  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.585733 \n",
      "\n",
      "loss: 0.627824  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.892555 \n",
      "\n",
      "loss: 0.855207  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.489520 \n",
      "\n",
      "loss: 0.522754  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.698612 \n",
      "\n",
      "loss: 0.669216  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.466068 \n",
      "\n",
      "loss: 0.454468  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.528771 \n",
      "\n",
      "loss: 0.501145  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.505611 \n",
      "\n",
      "loss: 0.498929  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.519100 \n",
      "\n",
      "loss: 0.494691  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.546299 \n",
      "\n",
      "loss: 0.501181  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.657363 \n",
      "\n",
      "loss: 0.612768  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.501779 \n",
      "\n",
      "loss: 0.530527  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.527411 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [117, 450], 'alpha': 0.010120488197724748, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.18954866217656036}\n",
      "loss: 29.005358  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 35029.868668 \n",
      "\n",
      "loss: 315262.156250  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.825793 \n",
      "\n",
      "loss: 0.829240  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.811965 \n",
      "\n",
      "loss: 0.822229  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.818348 \n",
      "\n",
      "loss: 0.809887  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.814437 \n",
      "\n",
      "loss: 0.807408  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.816507 \n",
      "\n",
      "loss: 0.814056  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.828826 \n",
      "\n",
      "loss: 0.789800  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.824220 \n",
      "\n",
      "loss: 0.822333  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.810905 \n",
      "\n",
      "loss: 0.840875  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.807660 \n",
      "\n",
      "loss: 0.815846  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.811998 \n",
      "\n",
      "loss: 0.816463  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.825990 \n",
      "\n",
      "loss: 0.818193  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.814428 \n",
      "\n",
      "loss: 0.821705  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.813872 \n",
      "\n",
      "loss: 0.824351  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.812459 \n",
      "\n",
      "loss: 0.836808  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.815489 \n",
      "\n",
      "loss: 0.806528  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.827875 \n",
      "\n",
      "loss: 0.840609  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.812995 \n",
      "\n",
      "loss: 0.847559  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.823231 \n",
      "\n",
      "loss: 0.810877  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.809440 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [143, 482], 'alpha': 0.8324401326318135, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.21156144224648243}\n",
      "loss: 1.055390  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.362956 \n",
      "\n",
      "loss: 2.374539  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.813255 \n",
      "\n",
      "loss: 2.746028  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 4.980840 \n",
      "\n",
      "loss: 4.934740  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.493906 \n",
      "\n",
      "loss: 1.428637  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.635073 \n",
      "\n",
      "loss: 1.832989  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.500841 \n",
      "\n",
      "loss: 1.498071  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.866776 \n",
      "\n",
      "loss: 2.776195  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 7.420920 \n",
      "\n",
      "loss: 7.382228  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 5.222266 \n",
      "\n",
      "loss: 5.101013  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 4.703703 \n",
      "\n",
      "loss: 4.422848  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.922343 \n",
      "\n",
      "loss: 1.921930  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.016698 \n",
      "\n",
      "loss: 1.847142  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 5.794567 \n",
      "\n",
      "loss: 5.594676  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 3.098716 \n",
      "\n",
      "loss: 2.907263  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 11.266841 \n",
      "\n",
      "loss: 11.852977  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 3.967180 \n",
      "\n",
      "loss: 3.888399  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 6.613985 \n",
      "\n",
      "loss: 6.305877  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 2.013078 \n",
      "\n",
      "loss: 2.023880  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 4.970427 \n",
      "\n",
      "loss: 5.016299  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 3.005306 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [409, 292, 346], 'alpha': 0.39087813410005207, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.14842424196473264}\n",
      "loss: 1.112748  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.806105 \n",
      "\n",
      "loss: 0.812909  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.723947 \n",
      "\n",
      "loss: 0.730757  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.738620 \n",
      "\n",
      "loss: 0.756523  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.735281 \n",
      "\n",
      "loss: 0.744339  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.759056 \n",
      "\n",
      "loss: 0.771752  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.778486 \n",
      "\n",
      "loss: 0.781302  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.772801 \n",
      "\n",
      "loss: 0.751072  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.836221 \n",
      "\n",
      "loss: 0.822414  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.773603 \n",
      "\n",
      "loss: 0.772386  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.771091 \n",
      "\n",
      "loss: 0.775623  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.737220 \n",
      "\n",
      "loss: 0.737884  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.818310 \n",
      "\n",
      "loss: 0.817336  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.795767 \n",
      "\n",
      "loss: 0.789771  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.786133 \n",
      "\n",
      "loss: 0.803875  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.748021 \n",
      "\n",
      "loss: 0.734840  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.783925 \n",
      "\n",
      "loss: 0.810592  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.777336 \n",
      "\n",
      "loss: 0.775292  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.804466 \n",
      "\n",
      "loss: 0.798921  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.787848 \n",
      "\n",
      "loss: 0.798413  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.767933 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [304], 'alpha': 0.4771840226513163, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.02867691992645079}\n",
      "loss: 77.004700  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 8.811942 \n",
      "\n",
      "loss: 4.351908  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 4.588705 \n",
      "\n",
      "loss: 3.283542  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.782754 \n",
      "\n",
      "loss: 1.364538  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 1.696103 \n",
      "\n",
      "loss: 1.235085  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 3.410665 \n",
      "\n",
      "loss: 1.743379  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.951619 \n",
      "\n",
      "loss: 0.886734  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 1.406165 \n",
      "\n",
      "loss: 1.357138  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.981529 \n",
      "\n",
      "loss: 0.833394  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.913498 \n",
      "\n",
      "loss: 0.830478  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 5.155266 \n",
      "\n",
      "loss: 4.023590  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 1.105952 \n",
      "\n",
      "loss: 0.866257  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.466629 \n",
      "\n",
      "loss: 1.336514  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 1.111881 \n",
      "\n",
      "loss: 0.846703  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.931385 \n",
      "\n",
      "loss: 1.159405  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 0.984770 \n",
      "\n",
      "loss: 0.856695  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 1.705095 \n",
      "\n",
      "loss: 1.042961  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 3.072965 \n",
      "\n",
      "loss: 0.849636  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.945757 \n",
      "\n",
      "loss: 0.958730  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.570113 \n",
      "\n",
      "loss: 5.192640  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.226097 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [252, 265, 478], 'alpha': 1.834845327334873, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.4803933737468977}\n",
      "loss: 1.085054  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.040035 \n",
      "\n",
      "loss: 1.041735  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.039823 \n",
      "\n",
      "loss: 1.038548  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.041606 \n",
      "\n",
      "loss: 1.043884  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.037273 \n",
      "\n",
      "loss: 1.037839  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.038865 \n",
      "\n",
      "loss: 1.040087  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.038736 \n",
      "\n",
      "loss: 1.039235  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.040459 \n",
      "\n",
      "loss: 1.038256  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.040066 \n",
      "\n",
      "loss: 1.039913  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.039206 \n",
      "\n",
      "loss: 1.037813  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.037405 \n",
      "\n",
      "loss: 1.035552  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.038798 \n",
      "\n",
      "loss: 1.039087  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.038711 \n",
      "\n",
      "loss: 1.037063  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.038661 \n",
      "\n",
      "loss: 1.039314  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.039566 \n",
      "\n",
      "loss: 1.039187  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.040682 \n",
      "\n",
      "loss: 1.040905  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.040611 \n",
      "\n",
      "loss: 1.037663  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.040760 \n",
      "\n",
      "loss: 1.040192  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.042570 \n",
      "\n",
      "loss: 1.044542  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.040959 \n",
      "\n",
      "loss: 1.042636  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.040830 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [443], 'alpha': 0.0008494245424178485, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.20194739810400691}\n",
      "loss: 71.063286  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.865003 \n",
      "\n",
      "loss: 0.864666  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.833740 \n",
      "\n",
      "loss: 0.821624  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.815583 \n",
      "\n",
      "loss: 0.808291  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.817621 \n",
      "\n",
      "loss: 0.845065  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.811535 \n",
      "\n",
      "loss: 0.835665  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.815501 \n",
      "\n",
      "loss: 0.799171  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.827128 \n",
      "\n",
      "loss: 0.830177  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.811996 \n",
      "\n",
      "loss: 0.824542  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.813138 \n",
      "\n",
      "loss: 0.819543  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.809348 \n",
      "\n",
      "loss: 0.824736  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.821189 \n",
      "\n",
      "loss: 0.807136  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.833321 \n",
      "\n",
      "loss: 0.796868  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.814152 \n",
      "\n",
      "loss: 0.827240  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816415 \n",
      "\n",
      "loss: 0.827237  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.813613 \n",
      "\n",
      "loss: 0.806719  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.813773 \n",
      "\n",
      "loss: 0.823295  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.821193 \n",
      "\n",
      "loss: 0.817312  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.816471 \n",
      "\n",
      "loss: 0.799383  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.820900 \n",
      "\n",
      "loss: 0.824107  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.811237 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [134, 304, 312], 'alpha': 0.00015801863700144815, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.32872084491981396}\n",
      "loss: 11.003640  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 6590.319173 \n",
      "\n",
      "loss: 14146.727539  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 25664.804589 \n",
      "\n",
      "loss: 2507.782959  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.811240 \n",
      "\n",
      "loss: 0.825931  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 0.819936 \n",
      "\n",
      "loss: 0.827519  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.812905 \n",
      "\n",
      "loss: 0.811674  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.812337 \n",
      "\n",
      "loss: 0.825948  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.822617 \n",
      "\n",
      "loss: 0.800030  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.830044 \n",
      "\n",
      "loss: 0.832447  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.817536 \n",
      "\n",
      "loss: 0.840513  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.811393 \n",
      "\n",
      "loss: 0.799093  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.825930 \n",
      "\n",
      "loss: 0.822111  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.827287 \n",
      "\n",
      "loss: 0.831159  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.815403 \n",
      "\n",
      "loss: 0.795144  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816212 \n",
      "\n",
      "loss: 0.835894  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.814175 \n",
      "\n",
      "loss: 0.832135  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.818436 \n",
      "\n",
      "loss: 0.810801  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.827315 \n",
      "\n",
      "loss: 0.808147  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.811683 \n",
      "\n",
      "loss: 0.824809  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.814532 \n",
      "\n",
      "loss: 0.797788  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.806698 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [275, 169, 486], 'alpha': 0.0020129290986662433, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.13288069481442377}\n",
      "loss: 6.685584  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [351], 'alpha': 0.16885111941871558, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.4645162640195865}\n",
      "loss: 1.130694  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 8.205261 \n",
      "\n",
      "loss: 7.020965  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.937199 \n",
      "\n",
      "loss: 1.810642  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 8.626911 \n",
      "\n",
      "loss: 8.964078  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 6.240400 \n",
      "\n",
      "loss: 6.163183  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 3.393047 \n",
      "\n",
      "loss: 3.259749  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 10.832223 \n",
      "\n",
      "loss: 11.387599  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 1.493453 \n",
      "\n",
      "loss: 1.546462  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 3.198270 \n",
      "\n",
      "loss: 3.465101  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 7.824467 \n",
      "\n",
      "loss: 7.797178  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 6.324629 \n",
      "\n",
      "loss: 6.266560  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.950801 \n",
      "\n",
      "loss: 2.874290  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 6.713680 \n",
      "\n",
      "loss: 6.673304  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 4.373654 \n",
      "\n",
      "loss: 4.307644  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 3.500619 \n",
      "\n",
      "loss: 3.469501  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 1.144365 \n",
      "\n",
      "loss: 1.228322  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 3.420116 \n",
      "\n",
      "loss: 3.528922  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.429492 \n",
      "\n",
      "loss: 2.270476  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 9.227440 \n",
      "\n",
      "loss: 8.994263  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 14.577504 \n",
      "\n",
      "loss: 14.482201  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.195068 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [12], 'alpha': 0.01956264676406525, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.2688634386104035}\n",
      "loss: 31.506157  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.849031 \n",
      "\n",
      "loss: 0.831923  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.839376 \n",
      "\n",
      "loss: 0.820391  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.822278 \n",
      "\n",
      "loss: 0.804623  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.819650 \n",
      "\n",
      "loss: 0.833741  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.816004 \n",
      "\n",
      "loss: 0.828599  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.831031 \n",
      "\n",
      "loss: 0.802782  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.834562 \n",
      "\n",
      "loss: 0.811931  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.821683 \n",
      "\n",
      "loss: 0.815850  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.822799 \n",
      "\n",
      "loss: 0.814502  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.822533 \n",
      "\n",
      "loss: 0.823452  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.823439 \n",
      "\n",
      "loss: 0.810305  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.832745 \n",
      "\n",
      "loss: 0.831522  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.831310 \n",
      "\n",
      "loss: 0.826487  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.821309 \n",
      "\n",
      "loss: 0.816382  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.817541 \n",
      "\n",
      "loss: 0.811675  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.830223 \n",
      "\n",
      "loss: 0.818429  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.832360 \n",
      "\n",
      "loss: 0.820732  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.820005 \n",
      "\n",
      "loss: 0.829678  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.829404 \n",
      "\n",
      "loss: 0.812835  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.822667 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [143, 75, 485], 'alpha': 0.005559608046637538, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.18482659166056808}\n",
      "loss: 6.021066  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [232], 'alpha': 1.4747716254292624, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.3339648155359205}\n",
      "loss: 23.709305  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [161, 485, 347], 'alpha': 0.00032955849401379, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.0578027923425092}\n",
      "loss: 1.082864  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 1.618582 \n",
      "\n",
      "loss: 1.618534  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.540973 \n",
      "\n",
      "loss: 1.402687  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.302670 \n",
      "\n",
      "loss: 1.142295  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.979383 \n",
      "\n",
      "loss: 0.980400  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.861901 \n",
      "\n",
      "loss: 1.825856  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.436171 \n",
      "\n",
      "loss: 1.434949  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.085822 \n",
      "\n",
      "loss: 1.022695  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.815654 \n",
      "\n",
      "loss: 0.786316  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.161893 \n",
      "\n",
      "loss: 1.127079  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.948905 \n",
      "\n",
      "loss: 0.948347  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.009666 \n",
      "\n",
      "loss: 1.880274  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.912011 \n",
      "\n",
      "loss: 2.743258  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 2.869646 \n",
      "\n",
      "loss: 2.888803  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.794338 \n",
      "\n",
      "loss: 2.085945  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.002031 \n",
      "\n",
      "loss: 0.985354  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.829678 \n",
      "\n",
      "loss: 0.808857  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.766169 \n",
      "\n",
      "loss: 1.789973  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.951484 \n",
      "\n",
      "loss: 0.961041  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.127173 \n",
      "\n",
      "loss: 1.109308  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.921469 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [388], 'alpha': 0.05836039254922183, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.30245443053240895}\n",
      "loss: 36.897560  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 4559.246724 \n",
      "\n",
      "loss: 5341.990723  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 2119.032633 \n",
      "\n",
      "loss: 1182.660645  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 386.459493 \n",
      "\n",
      "loss: 337.456085  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 543.578608 \n",
      "\n",
      "loss: 824.470703  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 374.518144 \n",
      "\n",
      "loss: 679.800110  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 388.297991 \n",
      "\n",
      "loss: 1094.738159  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 343.947403 \n",
      "\n",
      "loss: 407.971069  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1047.886637 \n",
      "\n",
      "loss: 1167.947876  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 1275.302049 \n",
      "\n",
      "loss: 1408.353760  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1036.534058 \n",
      "\n",
      "loss: 867.082581  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 483.638075 \n",
      "\n",
      "loss: 380.015839  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 795.141317 \n",
      "\n",
      "loss: 833.972717  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1317.320211 \n",
      "\n",
      "loss: 1028.150879  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 1036.470011 \n",
      "\n",
      "loss: 884.443054  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 905.617581 \n",
      "\n",
      "loss: 756.789490  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 1207.572001 \n",
      "\n",
      "loss: 875.756592  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 1444.524889 \n",
      "\n",
      "loss: 2517.925781  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 959.067729 \n",
      "\n",
      "loss: 752.593567  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 545.184411 \n",
      "\n",
      "loss: 441.562225  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 701.536930 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [183, 112, 359], 'alpha': 0.14831661135493762, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.12563551390974007}\n",
      "loss: 6.758339  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 42909.442491 \n",
      "\n",
      "loss: 35826.925781  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 3752.683929 \n",
      "\n",
      "loss: 21819.787109  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 5897.004964 \n",
      "\n",
      "loss: 4513.404297  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 2931.533895 \n",
      "\n",
      "loss: 2114.626465  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 167.698287 \n",
      "\n",
      "loss: 167.439682  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 301.529438 \n",
      "\n",
      "loss: 364.519226  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 577.780056 \n",
      "\n",
      "loss: 442.926788  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 110.138554 \n",
      "\n",
      "loss: 123.069962  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 88.586087 \n",
      "\n",
      "loss: 73.185608  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 158.198689 \n",
      "\n",
      "loss: 108.261475  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 97.794620 \n",
      "\n",
      "loss: 100.417542  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 377.299025 \n",
      "\n",
      "loss: 328.844574  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 231.261612 \n",
      "\n",
      "loss: 245.927887  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 101.613583 \n",
      "\n",
      "loss: 109.161018  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 110.448362 \n",
      "\n",
      "loss: 99.018700  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 187.176936 \n",
      "\n",
      "loss: 62.999622  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 57.303777 \n",
      "\n",
      "loss: 35.522316  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 30.120458 \n",
      "\n",
      "loss: 26.092936  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 87.391530 \n",
      "\n",
      "loss: 224.940765  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 44.9%, Avg loss: 73.508503 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [55], 'alpha': 0.14839601428674118, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.001872489974982372}\n",
      "loss: 19.772114  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 1.062041 \n",
      "\n",
      "loss: 0.807254  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.860989 \n",
      "\n",
      "loss: 0.672512  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.685477 \n",
      "\n",
      "loss: 0.626753  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.594255 \n",
      "\n",
      "loss: 0.543145  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.619544 \n",
      "\n",
      "loss: 0.593645  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.543451 \n",
      "\n",
      "loss: 0.577360  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.592305 \n",
      "\n",
      "loss: 0.511388  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.507976 \n",
      "\n",
      "loss: 0.476548  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.504301 \n",
      "\n",
      "loss: 0.499550  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.558298 \n",
      "\n",
      "loss: 0.497397  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.510854 \n",
      "\n",
      "loss: 0.491277  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.522760 \n",
      "\n",
      "loss: 0.494040  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.479599 \n",
      "\n",
      "loss: 0.504822  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.462365 \n",
      "\n",
      "loss: 0.449957  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.480968 \n",
      "\n",
      "loss: 0.436265  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.488352 \n",
      "\n",
      "loss: 0.505524  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.500338 \n",
      "\n",
      "loss: 0.475943  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.459450 \n",
      "\n",
      "loss: 0.466078  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.470458 \n",
      "\n",
      "loss: 0.465882  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.456708 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [334], 'alpha': 7.924826614167679, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.275885042521083}\n",
      "loss: 1.068390  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.379031 \n",
      "\n",
      "loss: 1.408986  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.5%, Avg loss: 2.542101 \n",
      "\n",
      "loss: 2.592638  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 6.0%, Avg loss: 3.914414 \n",
      "\n",
      "loss: 3.953325  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 5.2%, Avg loss: 8.212362 \n",
      "\n",
      "loss: 8.099122  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 8.284625 \n",
      "\n",
      "loss: 8.369637  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 11.681507 \n",
      "\n",
      "loss: 11.797012  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 13.623414 \n",
      "\n",
      "loss: 13.669520  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 11.501990 \n",
      "\n",
      "loss: 11.518860  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 11.476227 \n",
      "\n",
      "loss: 11.656366  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 9.752194 \n",
      "\n",
      "loss: 9.629364  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 9.175475 \n",
      "\n",
      "loss: 9.279737  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 9.045755 \n",
      "\n",
      "loss: 9.134758  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 9.075966 \n",
      "\n",
      "loss: 9.126663  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 10.217063 \n",
      "\n",
      "loss: 9.979109  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 11.291096 \n",
      "\n",
      "loss: 11.162020  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 9.427574 \n",
      "\n",
      "loss: 9.534010  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 11.123374 \n",
      "\n",
      "loss: 11.049175  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 8.459418 \n",
      "\n",
      "loss: 8.376699  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 4.1%, Avg loss: 8.596462 \n",
      "\n",
      "loss: 8.818808  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 8.243067 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [295, 12], 'alpha': 0.00036255124676536245, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.41731405044101977}\n",
      "loss: 1.140154  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.734794 \n",
      "\n",
      "loss: 0.774761  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.571727 \n",
      "\n",
      "loss: 0.568739  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.534520 \n",
      "\n",
      "loss: 0.557418  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.730223 \n",
      "\n",
      "loss: 0.717326  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.524285 \n",
      "\n",
      "loss: 0.544298  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.594341 \n",
      "\n",
      "loss: 0.588675  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.785452 \n",
      "\n",
      "loss: 0.683054  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.541025 \n",
      "\n",
      "loss: 0.550727  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.507808 \n",
      "\n",
      "loss: 0.522974  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.495772 \n",
      "\n",
      "loss: 0.516576  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.539883 \n",
      "\n",
      "loss: 0.520249  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.575989 \n",
      "\n",
      "loss: 0.583140  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.630196 \n",
      "\n",
      "loss: 0.606412  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.503919 \n",
      "\n",
      "loss: 0.461416  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.496061 \n",
      "\n",
      "loss: 0.500051  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.521375 \n",
      "\n",
      "loss: 0.528589  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.511398 \n",
      "\n",
      "loss: 0.490586  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.500135 \n",
      "\n",
      "loss: 0.504413  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.509928 \n",
      "\n",
      "loss: 0.487077  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.494268 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [290, 83, 228], 'alpha': 0.00043373812368277954, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.44950084622242076}\n",
      "loss: 8.548731  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 1906483.041667 \n",
      "\n",
      "loss: 1842313.125000  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 711376.748264 \n",
      "\n",
      "loss: 3574467.250000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 512634.725694 \n",
      "\n",
      "loss: 810574.625000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 829609.923611 \n",
      "\n",
      "loss: 813655.437500  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 200940.064236 \n",
      "\n",
      "loss: 97789.726562  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 60615.651476 \n",
      "\n",
      "loss: 45158.453125  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 55812.453559 \n",
      "\n",
      "loss: 39749.062500  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 207214.276042 \n",
      "\n",
      "loss: 209622.171875  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 49995.904297 \n",
      "\n",
      "loss: 43593.597656  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 132396.530816 \n",
      "\n",
      "loss: 73884.617188  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 65785.273655 \n",
      "\n",
      "loss: 43057.699219  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 325930.430556 \n",
      "\n",
      "loss: 245972.734375  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 202487.147569 \n",
      "\n",
      "loss: 212678.031250  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 123177.972222 \n",
      "\n",
      "loss: 441183.781250  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 35295.406467 \n",
      "\n",
      "loss: 24072.644531  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 62017.515191 \n",
      "\n",
      "loss: 102954.898438  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 458230.447917 \n",
      "\n",
      "loss: 398648.812500  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 18.3%, Avg loss: 1044806.152778 \n",
      "\n",
      "loss: 1281787.125000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 9439674.111111 \n",
      "\n",
      "loss: 33987224.000000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 74127236.000000 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [468], 'alpha': 0.006152422634646263, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.1769039494438477}\n",
      "loss: 0.895825  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 7.059616 \n",
      "\n",
      "loss: 7.524448  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 8.725898 \n",
      "\n",
      "loss: 8.689622  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 5.202813 \n",
      "\n",
      "loss: 5.378280  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 7.659757 \n",
      "\n",
      "loss: 7.566605  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 2.569870 \n",
      "\n",
      "loss: 2.664039  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 6.086239 \n",
      "\n",
      "loss: 6.384691  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 2.897757 \n",
      "\n",
      "loss: 3.142694  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 6.104849 \n",
      "\n",
      "loss: 5.647642  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 3.483677 \n",
      "\n",
      "loss: 3.809377  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 5.678132 \n",
      "\n",
      "loss: 5.597854  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 2.076438 \n",
      "\n",
      "loss: 2.086475  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 5.635930 \n",
      "\n",
      "loss: 5.120415  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 5.898066 \n",
      "\n",
      "loss: 5.581991  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 8.036874 \n",
      "\n",
      "loss: 7.836247  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 4.989393 \n",
      "\n",
      "loss: 4.937011  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 5.520166 \n",
      "\n",
      "loss: 5.185233  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 3.863987 \n",
      "\n",
      "loss: 3.945867  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 2.841947 \n",
      "\n",
      "loss: 3.224877  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 2.864927 \n",
      "\n",
      "loss: 3.179464  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 7.491197 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [460, 455, 216], 'alpha': 7.399205907653401, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.43231638754036}\n",
      "loss: 1.125888  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 7911767131932.444336 \n",
      "\n",
      "loss: 7939296854016.000000  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 1548163877075546324074496.000000 \n",
      "\n",
      "loss: 1542882503790929228333056.000000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 304663227866372778297526840104845312.000000 \n",
      "\n",
      "loss: 299166116058040367169669509662375936.000000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [459], 'alpha': 0.011520519192355904, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.39121500942420107}\n",
      "loss: 1.220181  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 7.329976 \n",
      "\n",
      "loss: 7.372884  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 4.279438 \n",
      "\n",
      "loss: 4.548950  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 20.973782 \n",
      "\n",
      "loss: 21.531559  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 11.434274 \n",
      "\n",
      "loss: 12.546836  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 13.481640 \n",
      "\n",
      "loss: 12.155071  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 7.847369 \n",
      "\n",
      "loss: 7.826789  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 10.5%, Avg loss: 8.776837 \n",
      "\n",
      "loss: 8.857475  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 19.699029 \n",
      "\n",
      "loss: 20.342312  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 5.296319 \n",
      "\n",
      "loss: 4.971046  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 11.750587 \n",
      "\n",
      "loss: 13.319801  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 49.886531 \n",
      "\n",
      "loss: 48.376949  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 11.913724 \n",
      "\n",
      "loss: 11.897232  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 21.792726 \n",
      "\n",
      "loss: 21.514185  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 19.509618 \n",
      "\n",
      "loss: 17.340607  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 7.803533 \n",
      "\n",
      "loss: 7.533744  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.269815 \n",
      "\n",
      "loss: 2.144264  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 19.774875 \n",
      "\n",
      "loss: 18.802219  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 19.066062 \n",
      "\n",
      "loss: 18.226843  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 9.196145 \n",
      "\n",
      "loss: 9.085464  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 18.454116 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [418, 350], 'alpha': 2.090836831467693, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.07968300170606755}\n",
      "loss: 1.165826  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 2.011164 \n",
      "\n",
      "loss: 2.016781  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.014582 \n",
      "\n",
      "loss: 2.108433  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.858034 \n",
      "\n",
      "loss: 0.855557  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.895913 \n",
      "\n",
      "loss: 0.886911  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.905376 \n",
      "\n",
      "loss: 0.902196  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.896920 \n",
      "\n",
      "loss: 0.905513  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.882082 \n",
      "\n",
      "loss: 0.880402  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 7.9%, Avg loss: 1.524836 \n",
      "\n",
      "loss: 1.543970  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 3.441805 \n",
      "\n",
      "loss: 3.366916  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 6.195793 \n",
      "\n",
      "loss: 6.246123  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 7.172968 \n",
      "\n",
      "loss: 6.912707  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.3%, Avg loss: 1.967128 \n",
      "\n",
      "loss: 2.058660  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 2.667048 \n",
      "\n",
      "loss: 2.883422  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 2.092061 \n",
      "\n",
      "loss: 2.068645  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 3.665088 \n",
      "\n",
      "loss: 3.668127  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 2.430525 \n",
      "\n",
      "loss: 2.424714  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.142599 \n",
      "\n",
      "loss: 2.172244  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.628332 \n",
      "\n",
      "loss: 1.664902  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.591490 \n",
      "\n",
      "loss: 1.637700  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 3.778777 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [63, 203, 454], 'alpha': 0.00015617914405316642, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.09986645772987338}\n",
      "loss: 1.212359  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.384850 \n",
      "\n",
      "loss: 1.419250  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.283108 \n",
      "\n",
      "loss: 1.258992  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.905868 \n",
      "\n",
      "loss: 0.901112  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.809789 \n",
      "\n",
      "loss: 0.831866  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.828980 \n",
      "\n",
      "loss: 0.845998  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 0.840379 \n",
      "\n",
      "loss: 0.834059  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.772087 \n",
      "\n",
      "loss: 0.756318  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.866864 \n",
      "\n",
      "loss: 0.885653  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.727811 \n",
      "\n",
      "loss: 0.736224  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.641926 \n",
      "\n",
      "loss: 0.642144  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.616659 \n",
      "\n",
      "loss: 0.577290  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.570773 \n",
      "\n",
      "loss: 0.585119  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.730924 \n",
      "\n",
      "loss: 0.719344  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.505121 \n",
      "\n",
      "loss: 0.568996  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.490759 \n",
      "\n",
      "loss: 0.514542  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.515197 \n",
      "\n",
      "loss: 0.549224  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.491410 \n",
      "\n",
      "loss: 0.484789  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.512067 \n",
      "\n",
      "loss: 0.498631  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.497840 \n",
      "\n",
      "loss: 0.512835  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.475025 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [187, 164], 'alpha': 1.26672372543513, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.011929850907255615}\n",
      "loss: 1.154878  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.692485 \n",
      "\n",
      "loss: 0.688428  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.681659 \n",
      "\n",
      "loss: 0.695341  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.666498 \n",
      "\n",
      "loss: 0.677647  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.651120 \n",
      "\n",
      "loss: 0.650770  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.658000 \n",
      "\n",
      "loss: 0.662249  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.656245 \n",
      "\n",
      "loss: 0.650634  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.693424 \n",
      "\n",
      "loss: 0.690666  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.691730 \n",
      "\n",
      "loss: 0.676967  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.718068 \n",
      "\n",
      "loss: 0.708654  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.648090 \n",
      "\n",
      "loss: 0.663998  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.722435 \n",
      "\n",
      "loss: 0.715574  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.681223 \n",
      "\n",
      "loss: 0.687842  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.658643 \n",
      "\n",
      "loss: 0.655681  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.653403 \n",
      "\n",
      "loss: 0.671015  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.642589 \n",
      "\n",
      "loss: 0.654928  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.643184 \n",
      "\n",
      "loss: 0.661196  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.647921 \n",
      "\n",
      "loss: 0.641496  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.698567 \n",
      "\n",
      "loss: 0.713284  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.676923 \n",
      "\n",
      "loss: 0.662951  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.738594 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [212], 'alpha': 0.004378342745277836, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.17563794992846782}\n",
      "loss: 29.639082  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 50.9%, Avg loss: 70.738609 \n",
      "\n",
      "loss: 34.325069  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 156.663481 \n",
      "\n",
      "loss: 6.241557  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 200.686317 \n",
      "\n",
      "loss: 12.598064  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 10.096725 \n",
      "\n",
      "loss: 42.428429  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.9%, Avg loss: 5.264348 \n",
      "\n",
      "loss: 20.859837  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 2.493979 \n",
      "\n",
      "loss: 0.806220  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 1.917329 \n",
      "\n",
      "loss: 1.573280  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.813398 \n",
      "\n",
      "loss: 0.922368  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 0.797045 \n",
      "\n",
      "loss: 0.788042  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.835710 \n",
      "\n",
      "loss: 0.802510  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 0.846557 \n",
      "\n",
      "loss: 1.025866  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 18.082492 \n",
      "\n",
      "loss: 0.821216  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 13.339838 \n",
      "\n",
      "loss: 0.857710  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 188.141360 \n",
      "\n",
      "loss: 20.971115  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 34.045348 \n",
      "\n",
      "loss: 115.250114  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 72.260208 \n",
      "\n",
      "loss: 12.420792  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.493258 \n",
      "\n",
      "loss: 0.821494  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 23.845159 \n",
      "\n",
      "loss: 8.382778  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 4.516236 \n",
      "\n",
      "loss: 1.003976  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 0.856020 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [212, 355, 510], 'alpha': 1.6531254668439463, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.05775934175052865}\n",
      "loss: 8.319023  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [99, 305], 'alpha': 6.4963823658346636, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.05429043335263408}\n",
      "loss: 14.104613  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [184, 381], 'alpha': 0.855344153120213, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.1926591971290291}\n",
      "loss: 0.957844  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.185776 \n",
      "\n",
      "loss: 2.246089  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.541757 \n",
      "\n",
      "loss: 1.571162  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 2.590482 \n",
      "\n",
      "loss: 2.549556  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.836498 \n",
      "\n",
      "loss: 1.903062  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.303912 \n",
      "\n",
      "loss: 2.259290  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.666154 \n",
      "\n",
      "loss: 1.704036  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.454609 \n",
      "\n",
      "loss: 2.464990  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.717172 \n",
      "\n",
      "loss: 1.637906  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.270206 \n",
      "\n",
      "loss: 2.242805  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.721837 \n",
      "\n",
      "loss: 1.705613  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.327958 \n",
      "\n",
      "loss: 2.286849  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.405139 \n",
      "\n",
      "loss: 1.408418  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 2.488591 \n",
      "\n",
      "loss: 2.497816  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.922618 \n",
      "\n",
      "loss: 1.883896  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.417216 \n",
      "\n",
      "loss: 2.428668  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.814470 \n",
      "\n",
      "loss: 1.762168  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.434442 \n",
      "\n",
      "loss: 2.348835  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 2.086673 \n",
      "\n",
      "loss: 2.045432  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.438052 \n",
      "\n",
      "loss: 2.415284  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 2.005785 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [105, 138, 30], 'alpha': 0.3498858821987227, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.18217431374345397}\n",
      "loss: 5.626927  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [493], 'alpha': 0.20950212120905315, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.33172739938514556}\n",
      "loss: 89.862221  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 4650.234999 \n",
      "\n",
      "loss: 1930.961548  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 4030.788059 \n",
      "\n",
      "loss: 2833.686035  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 3711.951389 \n",
      "\n",
      "loss: 2385.799805  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 1116.039693 \n",
      "\n",
      "loss: 1286.229248  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 429.110084 \n",
      "\n",
      "loss: 287.504089  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 622.921460 \n",
      "\n",
      "loss: 491.260376  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1498.682068 \n",
      "\n",
      "loss: 893.586365  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 2716.457167 \n",
      "\n",
      "loss: 4595.869629  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 2510.932102 \n",
      "\n",
      "loss: 1385.360229  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 1267.394253 \n",
      "\n",
      "loss: 456.132507  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1168.281677 \n",
      "\n",
      "loss: 1626.696899  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 1401.098233 \n",
      "\n",
      "loss: 691.321289  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 1621.701681 \n",
      "\n",
      "loss: 2703.959473  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 1732.241591 \n",
      "\n",
      "loss: 2187.510742  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 3796.415961 \n",
      "\n",
      "loss: 1815.949463  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 2071.887112 \n",
      "\n",
      "loss: 1907.209595  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 2910.251790 \n",
      "\n",
      "loss: 3722.043701  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 1326.730387 \n",
      "\n",
      "loss: 3190.971680  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 1500.008870 \n",
      "\n",
      "loss: 1057.612427  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1730.807834 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [376], 'alpha': 3.163983248479132, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.1631588341382142}\n",
      "loss: 28.423138  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 640.781779 \n",
      "\n",
      "loss: 341.482147  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 610.937585 \n",
      "\n",
      "loss: 517.769104  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 188.007813 \n",
      "\n",
      "loss: 217.197586  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 88.578493 \n",
      "\n",
      "loss: 184.687439  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 191.183690 \n",
      "\n",
      "loss: 174.759811  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 288.314129 \n",
      "\n",
      "loss: 136.667053  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 318.239070 \n",
      "\n",
      "loss: 256.085968  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 395.691501 \n",
      "\n",
      "loss: 1021.552795  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 146.375593 \n",
      "\n",
      "loss: 162.017014  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 690.117072 \n",
      "\n",
      "loss: 912.358521  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 748.490353 \n",
      "\n",
      "loss: 255.165741  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 78.369050 \n",
      "\n",
      "loss: 39.046452  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 474.777952 \n",
      "\n",
      "loss: 544.981873  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 274.140142 \n",
      "\n",
      "loss: 276.951019  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 695.044676 \n",
      "\n",
      "loss: 609.550049  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 311.097780 \n",
      "\n",
      "loss: 290.003906  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 234.175922 \n",
      "\n",
      "loss: 142.111526  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 534.289508 \n",
      "\n",
      "loss: 453.768127  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 220.715279 \n",
      "\n",
      "loss: 147.057556  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 109.346076 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [159, 291, 73], 'alpha': 0.0009103333742719545, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.3077959064840951}\n",
      "loss: 1.246661  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.806807 \n",
      "\n",
      "loss: 0.802020  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.805212 \n",
      "\n",
      "loss: 0.785003  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.721229 \n",
      "\n",
      "loss: 0.729787  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.623450 \n",
      "\n",
      "loss: 0.659388  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.537311 \n",
      "\n",
      "loss: 0.548115  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.587143 \n",
      "\n",
      "loss: 0.588390  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.520715 \n",
      "\n",
      "loss: 0.529553  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.596364 \n",
      "\n",
      "loss: 0.613087  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.564675 \n",
      "\n",
      "loss: 0.564263  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.549079 \n",
      "\n",
      "loss: 0.579193  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.541974 \n",
      "\n",
      "loss: 0.535111  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.593459 \n",
      "\n",
      "loss: 0.586310  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.499090 \n",
      "\n",
      "loss: 0.520546  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.461949 \n",
      "\n",
      "loss: 0.462122  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.444262 \n",
      "\n",
      "loss: 0.474723  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.487152 \n",
      "\n",
      "loss: 0.472523  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.531347 \n",
      "\n",
      "loss: 0.510498  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.473263 \n",
      "\n",
      "loss: 0.475919  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.435790 \n",
      "\n",
      "loss: 0.425471  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.440699 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [215, 288], 'alpha': 1.4744357297415664, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.1051703241728327}\n",
      "loss: 14.309910  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 78.576440 \n",
      "\n",
      "loss: 78.234604  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 33.290290 \n",
      "\n",
      "loss: 12.260179  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 23.863542 \n",
      "\n",
      "loss: 19.863882  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 7.066021 \n",
      "\n",
      "loss: 7.404891  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 4.977106 \n",
      "\n",
      "loss: 4.189649  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 2.639706 \n",
      "\n",
      "loss: 1.395956  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 2.350150 \n",
      "\n",
      "loss: 1.791970  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.966374 \n",
      "\n",
      "loss: 0.866861  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.701507 \n",
      "\n",
      "loss: 0.818660  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 2.049767 \n",
      "\n",
      "loss: 1.922240  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.821215 \n",
      "\n",
      "loss: 0.820848  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.708290 \n",
      "\n",
      "loss: 0.826100  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.590778 \n",
      "\n",
      "loss: 0.699703  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 6.480098 \n",
      "\n",
      "loss: 8.352080  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 34.7%, Avg loss: 8.862225 \n",
      "\n",
      "loss: 7.940651  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 35.626810 \n",
      "\n",
      "loss: 22.271847  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 7.933683 \n",
      "\n",
      "loss: 11.465190  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 62.825844 \n",
      "\n",
      "loss: 63.335934  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 37.134498 \n",
      "\n",
      "loss: 53.256020  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 26.815902 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [318, 343, 172], 'alpha': 0.02113201699244151, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.2556358810376061}\n",
      "loss: 1.107689  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.844459 \n",
      "\n",
      "loss: 0.847158  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 0.836153 \n",
      "\n",
      "loss: 0.819347  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.821998 \n",
      "\n",
      "loss: 0.809044  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.818393 \n",
      "\n",
      "loss: 0.798142  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.882209 \n",
      "\n",
      "loss: 0.877007  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.834503 \n",
      "\n",
      "loss: 0.832357  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 0.865734 \n",
      "\n",
      "loss: 0.865757  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.838602 \n",
      "\n",
      "loss: 0.837941  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 0.765972 \n",
      "\n",
      "loss: 0.772189  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.145583 \n",
      "\n",
      "loss: 1.085320  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.698011 \n",
      "\n",
      "loss: 0.717435  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.588832 \n",
      "\n",
      "loss: 0.534866  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.533572 \n",
      "\n",
      "loss: 0.523712  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.578972 \n",
      "\n",
      "loss: 0.602655  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.568327 \n",
      "\n",
      "loss: 0.583699  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.532837 \n",
      "\n",
      "loss: 0.523767  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.521415 \n",
      "\n",
      "loss: 0.541448  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.635866 \n",
      "\n",
      "loss: 0.613937  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.570446 \n",
      "\n",
      "loss: 0.610230  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.705936 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [94, 345, 51], 'alpha': 0.00262380783729638, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.4092298899218785}\n",
      "loss: 9.356671  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [352], 'alpha': 0.003565310198594198, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.012310251324640488}\n",
      "loss: 1.520140  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.514813 \n",
      "\n",
      "loss: 0.525026  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.486086 \n",
      "\n",
      "loss: 0.463907  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.463204 \n",
      "\n",
      "loss: 0.477884  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.450468 \n",
      "\n",
      "loss: 0.439851  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.430812 \n",
      "\n",
      "loss: 0.418359  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.426207 \n",
      "\n",
      "loss: 0.392850  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.422387 \n",
      "\n",
      "loss: 0.420166  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.415625 \n",
      "\n",
      "loss: 0.403967  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.404808 \n",
      "\n",
      "loss: 0.422945  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.388229 \n",
      "\n",
      "loss: 0.394601  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.410215 \n",
      "\n",
      "loss: 0.388444  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.416956 \n",
      "\n",
      "loss: 0.414500  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.400502 \n",
      "\n",
      "loss: 0.425442  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.394140 \n",
      "\n",
      "loss: 0.370738  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.381142 \n",
      "\n",
      "loss: 0.391721  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.377920 \n",
      "\n",
      "loss: 0.390941  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.389667 \n",
      "\n",
      "loss: 0.373960  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.386961 \n",
      "\n",
      "loss: 0.416131  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.389968 \n",
      "\n",
      "loss: 0.420871  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.371014 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [511], 'alpha': 0.023750990234495464, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.2072790613977599}\n",
      "loss: 43.599129  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 986.092968 \n",
      "\n",
      "loss: 1006.062683  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 372.013997 \n",
      "\n",
      "loss: 15.598267  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 96.079767 \n",
      "\n",
      "loss: 188.197418  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 338.317411 \n",
      "\n",
      "loss: 887.345825  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 47.496723 \n",
      "\n",
      "loss: 67.070641  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 95.727712 \n",
      "\n",
      "loss: 87.889481  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 83.070304 \n",
      "\n",
      "loss: 0.931308  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 36.121804 \n",
      "\n",
      "loss: 8.542653  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 27.903998 \n",
      "\n",
      "loss: 33.827885  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 7.971678 \n",
      "\n",
      "loss: 65.422478  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 3.433271 \n",
      "\n",
      "loss: 0.806840  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 2.353262 \n",
      "\n",
      "loss: 0.815224  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 1.266724 \n",
      "\n",
      "loss: 0.804575  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 2.088087 \n",
      "\n",
      "loss: 0.805990  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 12.772999 \n",
      "\n",
      "loss: 0.799491  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 27.567087 \n",
      "\n",
      "loss: 6.614581  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 17.362900 \n",
      "\n",
      "loss: 10.185134  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 113.528435 \n",
      "\n",
      "loss: 18.484900  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.9%, Avg loss: 8.146528 \n",
      "\n",
      "loss: 4.710243  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.843123 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [302, 107, 375], 'alpha': 0.3032293622030006, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.12206782134526159}\n",
      "loss: 1.255162  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.094080 \n",
      "\n",
      "loss: 1.098632  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 0.939636 \n",
      "\n",
      "loss: 0.937096  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.878376 \n",
      "\n",
      "loss: 0.882907  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.422192 \n",
      "\n",
      "loss: 1.396111  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 1.134658 \n",
      "\n",
      "loss: 1.124388  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.887966 \n",
      "\n",
      "loss: 0.880429  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.895377 \n",
      "\n",
      "loss: 0.896852  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.905659 \n",
      "\n",
      "loss: 0.905808  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.989856 \n",
      "\n",
      "loss: 0.967982  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 2.049951 \n",
      "\n",
      "loss: 2.087600  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.359732 \n",
      "\n",
      "loss: 1.303097  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.340673 \n",
      "\n",
      "loss: 1.319388  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.997814 \n",
      "\n",
      "loss: 0.940750  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.762827 \n",
      "\n",
      "loss: 1.864399  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.392070 \n",
      "\n",
      "loss: 1.524880  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 6.488879 \n",
      "\n",
      "loss: 6.352112  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.964356 \n",
      "\n",
      "loss: 1.846439  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 2.797453 \n",
      "\n",
      "loss: 2.903065  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.959140 \n",
      "\n",
      "loss: 1.077468  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 0.987209 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [360, 10, 465], 'alpha': 0.5686203300525338, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.10949868429247848}\n",
      "loss: 10.111843  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 0.890952 \n",
      "\n",
      "loss: 0.897581  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.970341 \n",
      "\n",
      "loss: 0.969871  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.993748 \n",
      "\n",
      "loss: 0.992772  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.961307 \n",
      "\n",
      "loss: 0.952399  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.951918 \n",
      "\n",
      "loss: 0.957821  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.961777 \n",
      "\n",
      "loss: 0.952183  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.992504 \n",
      "\n",
      "loss: 0.997200  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.001924 \n",
      "\n",
      "loss: 1.036705  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.952586 \n",
      "\n",
      "loss: 0.951087  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.937995 \n",
      "\n",
      "loss: 0.944992  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.940862 \n",
      "\n",
      "loss: 0.937221  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 0.958563 \n",
      "\n",
      "loss: 0.960663  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.945242 \n",
      "\n",
      "loss: 0.950118  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.959427 \n",
      "\n",
      "loss: 0.952285  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.947592 \n",
      "\n",
      "loss: 0.969617  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.948900 \n",
      "\n",
      "loss: 0.935975  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.960478 \n",
      "\n",
      "loss: 0.945006  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.947530 \n",
      "\n",
      "loss: 0.937474  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.950102 \n",
      "\n",
      "loss: 0.960845  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.968481 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [273, 245, 33], 'alpha': 0.06123912407005947, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.06675621886795874}\n",
      "loss: 25.108927  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.349574 \n",
      "\n",
      "loss: 0.864179  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.844961 \n",
      "\n",
      "loss: 0.854967  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.846556 \n",
      "\n",
      "loss: 0.841104  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.847214 \n",
      "\n",
      "loss: 0.837722  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.845151 \n",
      "\n",
      "loss: 0.839567  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.858333 \n",
      "\n",
      "loss: 0.861905  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.853525 \n",
      "\n",
      "loss: 0.842187  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.847834 \n",
      "\n",
      "loss: 0.832407  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.848348 \n",
      "\n",
      "loss: 0.842650  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.842638 \n",
      "\n",
      "loss: 0.853106  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.844368 \n",
      "\n",
      "loss: 0.838415  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.848131 \n",
      "\n",
      "loss: 0.853821  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.847376 \n",
      "\n",
      "loss: 0.851158  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.846443 \n",
      "\n",
      "loss: 0.843644  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.841343 \n",
      "\n",
      "loss: 0.829959  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.851147 \n",
      "\n",
      "loss: 0.854748  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.851332 \n",
      "\n",
      "loss: 0.860366  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.842183 \n",
      "\n",
      "loss: 0.850398  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.840845 \n",
      "\n",
      "loss: 0.849031  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.839065 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [349, 279], 'alpha': 8.698858477633713, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.30636489614860524}\n",
      "loss: 20.903273  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 6205.320584 \n",
      "\n",
      "loss: 5072.384766  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 1590.003079 \n",
      "\n",
      "loss: 2481.024414  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 928.027384 \n",
      "\n",
      "loss: 1446.503174  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 302.479919 \n",
      "\n",
      "loss: 288.385742  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 40.191118 \n",
      "\n",
      "loss: 35.683708  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 185.400970 \n",
      "\n",
      "loss: 177.379929  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 38.801769 \n",
      "\n",
      "loss: 37.969620  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 17.503799 \n",
      "\n",
      "loss: 16.990501  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 13.283685 \n",
      "\n",
      "loss: 13.848276  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 234.257136 \n",
      "\n",
      "loss: 202.601929  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.1%, Avg loss: 18752.042318 \n",
      "\n",
      "loss: 15300.644531  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 3823.007582 \n",
      "\n",
      "loss: 3637.434082  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 964.153727 \n",
      "\n",
      "loss: 2388.142578  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 564.118544 \n",
      "\n",
      "loss: 711.564941  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 76.965418 \n",
      "\n",
      "loss: 62.293148  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 26.597998 \n",
      "\n",
      "loss: 31.275089  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 151.960585 \n",
      "\n",
      "loss: 190.876648  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 55862.674913 \n",
      "\n",
      "loss: 50837.238281  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 9696.134332 \n",
      "\n",
      "loss: 9459.763672  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 12550.147895 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [464, 266, 226], 'alpha': 1.4218240258013333, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.37185335553557647}\n",
      "loss: 1.153050  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 3.614469 \n",
      "\n",
      "loss: 3.534755  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.961184 \n",
      "\n",
      "loss: 2.844464  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 3.046231 \n",
      "\n",
      "loss: 3.189984  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 3.013040 \n",
      "\n",
      "loss: 2.985600  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 3.680990 \n",
      "\n",
      "loss: 3.781201  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 3.259863 \n",
      "\n",
      "loss: 3.084627  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 3.611761 \n",
      "\n",
      "loss: 3.551462  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 3.399581 \n",
      "\n",
      "loss: 3.338050  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 3.027108 \n",
      "\n",
      "loss: 3.028975  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 2.994681 \n",
      "\n",
      "loss: 3.035456  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 3.724488 \n",
      "\n",
      "loss: 4.018091  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.804696 \n",
      "\n",
      "loss: 2.828908  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 3.441175 \n",
      "\n",
      "loss: 3.498160  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 2.677684 \n",
      "\n",
      "loss: 2.782846  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 4.052510 \n",
      "\n",
      "loss: 4.231092  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 2.893574 \n",
      "\n",
      "loss: 2.755074  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 3.402244 \n",
      "\n",
      "loss: 3.573431  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 3.046884 \n",
      "\n",
      "loss: 3.051672  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 3.203786 \n",
      "\n",
      "loss: 3.165468  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 3.188641 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [238, 140], 'alpha': 0.5739630470474949, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.3134115436471925}\n",
      "loss: 10.710609  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 27457.422852 \n",
      "\n",
      "loss: 8472.506836  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 6915.489258 \n",
      "\n",
      "loss: 4458.653320  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 3362.852973 \n",
      "\n",
      "loss: 4120.557617  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 1161.262275 \n",
      "\n",
      "loss: 1386.005371  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 913.725423 \n",
      "\n",
      "loss: 1176.836914  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1432.932366 \n",
      "\n",
      "loss: 942.923218  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 1081.024550 \n",
      "\n",
      "loss: 671.259277  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 481.855014 \n",
      "\n",
      "loss: 369.429047  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 880.238281 \n",
      "\n",
      "loss: 948.683350  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 3448.300035 \n",
      "\n",
      "loss: 2715.231445  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 8813.238010 \n",
      "\n",
      "loss: 7374.595703  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: 3403.345405 \n",
      "\n",
      "loss: 2616.202637  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 1040.516405 \n",
      "\n",
      "loss: 1144.329224  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 495.618578 \n",
      "\n",
      "loss: 359.706696  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 212.954995 \n",
      "\n",
      "loss: 205.054092  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 127.485784 \n",
      "\n",
      "loss: 139.746796  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 92.978674 \n",
      "\n",
      "loss: 130.657425  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 49.823930 \n",
      "\n",
      "loss: 39.680744  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 58.569920 \n",
      "\n",
      "loss: 53.818371  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 36.235755 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [233, 47], 'alpha': 0.3695934900677929, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.16799018355699483}\n",
      "loss: 1.091376  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 0.874907 \n",
      "\n",
      "loss: 0.900420  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.813960 \n",
      "\n",
      "loss: 0.789030  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 7.2%, Avg loss: 1.273064 \n",
      "\n",
      "loss: 1.267450  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.021639 \n",
      "\n",
      "loss: 0.985633  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.848575 \n",
      "\n",
      "loss: 0.851099  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 0.882449 \n",
      "\n",
      "loss: 0.899052  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 1.123199 \n",
      "\n",
      "loss: 1.129383  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.105000 \n",
      "\n",
      "loss: 1.116076  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.062188 \n",
      "\n",
      "loss: 2.127616  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.432568 \n",
      "\n",
      "loss: 1.394856  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.858835 \n",
      "\n",
      "loss: 0.868058  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 1.130105 \n",
      "\n",
      "loss: 1.124774  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 14.7%, Avg loss: 2.828304 \n",
      "\n",
      "loss: 2.840080  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 0.946014 \n",
      "\n",
      "loss: 0.981284  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 1.060378 \n",
      "\n",
      "loss: 1.067381  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.839543 \n",
      "\n",
      "loss: 0.835721  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 0.829486 \n",
      "\n",
      "loss: 0.850870  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.867374 \n",
      "\n",
      "loss: 0.878109  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.804793 \n",
      "\n",
      "loss: 0.807930  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.869872 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [49, 78, 85], 'alpha': 0.001433201977224923, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.03220800247186765}\n",
      "loss: 1.129328  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.618990 \n",
      "\n",
      "loss: 0.632173  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.563147 \n",
      "\n",
      "loss: 0.560917  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.542672 \n",
      "\n",
      "loss: 0.562023  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.522076 \n",
      "\n",
      "loss: 0.476172  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.502720 \n",
      "\n",
      "loss: 0.537277  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.506653 \n",
      "\n",
      "loss: 0.461991  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.512498 \n",
      "\n",
      "loss: 0.506079  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.517817 \n",
      "\n",
      "loss: 0.523144  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.507473 \n",
      "\n",
      "loss: 0.544201  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.498769 \n",
      "\n",
      "loss: 0.515643  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.490376 \n",
      "\n",
      "loss: 0.556546  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.497971 \n",
      "\n",
      "loss: 0.506060  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.488819 \n",
      "\n",
      "loss: 0.501357  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.496008 \n",
      "\n",
      "loss: 0.531913  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.489257 \n",
      "\n",
      "loss: 0.459707  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.485340 \n",
      "\n",
      "loss: 0.506944  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.502326 \n",
      "\n",
      "loss: 0.526000  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.484403 \n",
      "\n",
      "loss: 0.481337  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.497041 \n",
      "\n",
      "loss: 0.527008  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.475970 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [177, 285], 'alpha': 0.02136099688835174, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.41646229555330977}\n",
      "loss: 13.491343  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.844188 \n",
      "\n",
      "loss: 0.841958  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.836839 \n",
      "\n",
      "loss: 0.821943  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.822613 \n",
      "\n",
      "loss: 0.808970  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.828672 \n",
      "\n",
      "loss: 0.816662  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.815527 \n",
      "\n",
      "loss: 0.850382  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.825067 \n",
      "\n",
      "loss: 0.823737  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.832592 \n",
      "\n",
      "loss: 0.825493  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.816154 \n",
      "\n",
      "loss: 0.818845  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.822146 \n",
      "\n",
      "loss: 0.809356  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.820494 \n",
      "\n",
      "loss: 0.823813  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.822824 \n",
      "\n",
      "loss: 0.825050  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.828893 \n",
      "\n",
      "loss: 0.807745  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.821337 \n",
      "\n",
      "loss: 0.808985  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.824428 \n",
      "\n",
      "loss: 0.831812  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.817991 \n",
      "\n",
      "loss: 0.819942  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.826650 \n",
      "\n",
      "loss: 0.821633  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.832858 \n",
      "\n",
      "loss: 0.847091  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.821469 \n",
      "\n",
      "loss: 0.824832  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.822255 \n",
      "\n",
      "loss: 0.834433  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.818236 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [471, 402], 'alpha': 0.08325245919987297, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.2385421683054227}\n",
      "loss: 11.399493  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 2537.130690 \n",
      "\n",
      "loss: 825.933167  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 414.967531 \n",
      "\n",
      "loss: 48.915634  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 6092.871609 \n",
      "\n",
      "loss: 3580.529053  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 177.627854 \n",
      "\n",
      "loss: 591.975403  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 2.288804 \n",
      "\n",
      "loss: 0.851511  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.912379 \n",
      "\n",
      "loss: 0.858782  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 6.231857 \n",
      "\n",
      "loss: 0.850654  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.881201 \n",
      "\n",
      "loss: 0.857522  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 6.275374 \n",
      "\n",
      "loss: 38.637688  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.850501 \n",
      "\n",
      "loss: 0.874311  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 0.868373 \n",
      "\n",
      "loss: 0.842236  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 0.869624 \n",
      "\n",
      "loss: 0.856603  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.154654 \n",
      "\n",
      "loss: 0.862157  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 10.346059 \n",
      "\n",
      "loss: 346.595886  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.844264 \n",
      "\n",
      "loss: 0.856552  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 0.853816 \n",
      "\n",
      "loss: 0.832021  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.2%, Avg loss: 0.854393 \n",
      "\n",
      "loss: 0.834602  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.857569 \n",
      "\n",
      "loss: 0.862566  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 0.846287 \n",
      "\n",
      "loss: 0.846539  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.850732 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [160, 393], 'alpha': 0.008911139457051997, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.2441659158273702}\n",
      "loss: 1.128675  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.852230 \n",
      "\n",
      "loss: 0.890305  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 0.779543 \n",
      "\n",
      "loss: 0.783066  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.696052 \n",
      "\n",
      "loss: 0.709454  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.553625 \n",
      "\n",
      "loss: 0.542437  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.598691 \n",
      "\n",
      "loss: 0.556293  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.544508 \n",
      "\n",
      "loss: 0.553305  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.600174 \n",
      "\n",
      "loss: 0.590030  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.547889 \n",
      "\n",
      "loss: 0.526659  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.653278 \n",
      "\n",
      "loss: 0.621605  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.528000 \n",
      "\n",
      "loss: 0.512534  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.488198 \n",
      "\n",
      "loss: 0.524849  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.661367 \n",
      "\n",
      "loss: 0.674971  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.494466 \n",
      "\n",
      "loss: 0.551754  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.495978 \n",
      "\n",
      "loss: 0.507517  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.596923 \n",
      "\n",
      "loss: 0.588371  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.655701 \n",
      "\n",
      "loss: 0.653533  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.652070 \n",
      "\n",
      "loss: 0.631415  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 1.039902 \n",
      "\n",
      "loss: 1.120012  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.529291 \n",
      "\n",
      "loss: 0.550161  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.488303 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [231, 256], 'alpha': 0.00013967674306039884, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.36211827067588165}\n",
      "loss: 5.632543  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 124391.715061 \n",
      "\n",
      "loss: 40391.605469  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 42361.529622 \n",
      "\n",
      "loss: 28311.859375  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 15148.878526 \n",
      "\n",
      "loss: 17012.671875  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 14918.242513 \n",
      "\n",
      "loss: 23021.535156  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 29815.052951 \n",
      "\n",
      "loss: 42395.281250  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 6530.030192 \n",
      "\n",
      "loss: 3426.144775  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 2980.142334 \n",
      "\n",
      "loss: 2367.069580  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 8569.186361 \n",
      "\n",
      "loss: 9915.486328  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 2530.568441 \n",
      "\n",
      "loss: 1837.512695  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2821.602458 \n",
      "\n",
      "loss: 2423.982422  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 1980.365913 \n",
      "\n",
      "loss: 1825.607422  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 3041.156901 \n",
      "\n",
      "loss: 1659.980225  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 1146.110229 \n",
      "\n",
      "loss: 1040.460693  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 3290.031033 \n",
      "\n",
      "loss: 3292.264648  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 2196.032132 \n",
      "\n",
      "loss: 1952.582520  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 3765.525119 \n",
      "\n",
      "loss: 2299.168457  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 3534.249132 \n",
      "\n",
      "loss: 4704.612793  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 2971.986681 \n",
      "\n",
      "loss: 4231.448730  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 8462.782688 \n",
      "\n",
      "loss: 6027.395020  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 10587.048069 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [478, 73], 'alpha': 8.009582657649434, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.29598719853639266}\n",
      "loss: 4.034791  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.149770 \n",
      "\n",
      "loss: 1.070204  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.559274 \n",
      "\n",
      "loss: 5.534043  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.189691 \n",
      "\n",
      "loss: 1.228494  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.9%, Avg loss: 1.142037 \n",
      "\n",
      "loss: 1.122821  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.651440 \n",
      "\n",
      "loss: 3.214242  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 568.337701 \n",
      "\n",
      "loss: 546.676208  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 169.272951 \n",
      "\n",
      "loss: 9.338017  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.476194 \n",
      "\n",
      "loss: 1.310841  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.366376 \n",
      "\n",
      "loss: 1.370307  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 1.093539 \n",
      "\n",
      "loss: 1.093395  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 5.057656 \n",
      "\n",
      "loss: 5.482380  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 1.100542 \n",
      "\n",
      "loss: 1.095659  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 1.083517 \n",
      "\n",
      "loss: 1.083061  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 3.240335 \n",
      "\n",
      "loss: 3.920736  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.1%, Avg loss: 2.043183 \n",
      "\n",
      "loss: 2.026865  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 11.347040 \n",
      "\n",
      "loss: 10.110004  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.078682 \n",
      "\n",
      "loss: 1.075867  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.105647 \n",
      "\n",
      "loss: 1.078308  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 1.082550 \n",
      "\n",
      "loss: 1.082554  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 2.073182 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [21, 50, 337], 'alpha': 0.013901569917285509, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.02523728828096491}\n",
      "loss: 1.373411  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.813188 \n",
      "\n",
      "loss: 0.839481  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.826379 \n",
      "\n",
      "loss: 0.784068  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.810864 \n",
      "\n",
      "loss: 0.801357  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816304 \n",
      "\n",
      "loss: 0.794829  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.811843 \n",
      "\n",
      "loss: 0.825201  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.813969 \n",
      "\n",
      "loss: 0.809668  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.821041 \n",
      "\n",
      "loss: 0.820083  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.812398 \n",
      "\n",
      "loss: 0.832019  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.803599 \n",
      "\n",
      "loss: 0.850049  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.799077 \n",
      "\n",
      "loss: 0.815484  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.815008 \n",
      "\n",
      "loss: 0.815772  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.815648 \n",
      "\n",
      "loss: 0.814554  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.805941 \n",
      "\n",
      "loss: 0.809404  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.800738 \n",
      "\n",
      "loss: 0.798188  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.794560 \n",
      "\n",
      "loss: 0.823052  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.799786 \n",
      "\n",
      "loss: 0.804699  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.812140 \n",
      "\n",
      "loss: 0.807091  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.793734 \n",
      "\n",
      "loss: 0.802278  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.794941 \n",
      "\n",
      "loss: 0.788386  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.788632 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [175, 116], 'alpha': 0.01877139830999145, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.13920048215783912}\n",
      "loss: 8.989509  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 246.411481 \n",
      "\n",
      "loss: 161.078598  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 240.418393 \n",
      "\n",
      "loss: 209.688446  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 1392.642293 \n",
      "\n",
      "loss: 360.867371  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 121.912894 \n",
      "\n",
      "loss: 144.650528  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 661.961733 \n",
      "\n",
      "loss: 386.377472  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 31.698164 \n",
      "\n",
      "loss: 29.785343  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 113.601598 \n",
      "\n",
      "loss: 53.309658  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 215.279899 \n",
      "\n",
      "loss: 254.575134  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 89.186441 \n",
      "\n",
      "loss: 63.102612  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 593.486056 \n",
      "\n",
      "loss: 264.160126  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 45.050464 \n",
      "\n",
      "loss: 23.137756  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 29.772391 \n",
      "\n",
      "loss: 17.031521  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 30.247802 \n",
      "\n",
      "loss: 31.596138  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 16.342517 \n",
      "\n",
      "loss: 16.423868  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 9.017598 \n",
      "\n",
      "loss: 8.971433  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 11.631158 \n",
      "\n",
      "loss: 10.655459  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 36.521252 \n",
      "\n",
      "loss: 16.748085  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 23.215789 \n",
      "\n",
      "loss: 27.389410  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 11.382229 \n",
      "\n",
      "loss: 12.595349  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 19.875190 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [137, 435], 'alpha': 0.00025676224472019756, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.4999539234717537}\n",
      "loss: 14.191613  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 622996.124040 \n",
      "\n",
      "loss: 0.811619  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.828166 \n",
      "\n",
      "loss: 0.813618  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.822219 \n",
      "\n",
      "loss: 0.820795  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816212 \n",
      "\n",
      "loss: 0.806481  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.809311 \n",
      "\n",
      "loss: 0.825909  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.819980 \n",
      "\n",
      "loss: 0.801128  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.828115 \n",
      "\n",
      "loss: 0.802592  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.817216 \n",
      "\n",
      "loss: 0.843949  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816888 \n",
      "\n",
      "loss: 0.830398  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.811895 \n",
      "\n",
      "loss: 0.816484  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.818748 \n",
      "\n",
      "loss: 0.810640  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.822445 \n",
      "\n",
      "loss: 0.829110  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.813407 \n",
      "\n",
      "loss: 0.840919  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.810420 \n",
      "\n",
      "loss: 0.824868  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.810529 \n",
      "\n",
      "loss: 0.819301  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.816369 \n",
      "\n",
      "loss: 0.815517  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.824917 \n",
      "\n",
      "loss: 0.844028  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.813463 \n",
      "\n",
      "loss: 0.835836  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.816187 \n",
      "\n",
      "loss: 0.804884  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.817935 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [122, 340], 'alpha': 0.00022033924261045178, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.47481090518472285}\n",
      "loss: 21.637896  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.814617 \n",
      "\n",
      "loss: 0.815114  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.822758 \n",
      "\n",
      "loss: 0.800480  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.807796 \n",
      "\n",
      "loss: 0.820134  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.820876 \n",
      "\n",
      "loss: 0.826192  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.807704 \n",
      "\n",
      "loss: 0.818326  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.814323 \n",
      "\n",
      "loss: 0.801228  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.824013 \n",
      "\n",
      "loss: 0.802503  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.812542 \n",
      "\n",
      "loss: 0.824845  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.813201 \n",
      "\n",
      "loss: 0.866742  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.812186 \n",
      "\n",
      "loss: 0.809016  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.815098 \n",
      "\n",
      "loss: 0.807479  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.833932 \n",
      "\n",
      "loss: 0.813832  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.815093 \n",
      "\n",
      "loss: 0.826678  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.810037 \n",
      "\n",
      "loss: 0.829573  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.806946 \n",
      "\n",
      "loss: 0.790072  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.817038 \n",
      "\n",
      "loss: 0.834403  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.818972 \n",
      "\n",
      "loss: 0.782959  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.828099 \n",
      "\n",
      "loss: 0.817336  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.818509 \n",
      "\n",
      "loss: 0.810385  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.809740 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [437], 'alpha': 0.00012644454168301897, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.0631978653396157}\n",
      "loss: 1.077569  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.534714 \n",
      "\n",
      "loss: 0.527756  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.592648 \n",
      "\n",
      "loss: 0.602317  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.599273 \n",
      "\n",
      "loss: 0.557485  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.786663 \n",
      "\n",
      "loss: 0.841976  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.433886 \n",
      "\n",
      "loss: 0.445471  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.450892 \n",
      "\n",
      "loss: 0.494649  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.536543 \n",
      "\n",
      "loss: 0.549519  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.508390 \n",
      "\n",
      "loss: 0.489057  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.568881 \n",
      "\n",
      "loss: 0.520289  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.422306 \n",
      "\n",
      "loss: 0.389893  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.438244 \n",
      "\n",
      "loss: 0.464208  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.433674 \n",
      "\n",
      "loss: 0.413422  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.551258 \n",
      "\n",
      "loss: 0.610406  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.640499 \n",
      "\n",
      "loss: 0.638640  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.439123 \n",
      "\n",
      "loss: 0.432872  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.503698 \n",
      "\n",
      "loss: 0.449938  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.466035 \n",
      "\n",
      "loss: 0.442645  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.597251 \n",
      "\n",
      "loss: 0.583150  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.420331 \n",
      "\n",
      "loss: 0.394178  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.503570 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [66, 296, 493], 'alpha': 0.001229766630975591, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.08397779276956435}\n",
      "loss: 5.212093  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 2210.526693 \n",
      "\n",
      "loss: 1335.015381  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 378.591203 \n",
      "\n",
      "loss: 582.201233  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 558.517258 \n",
      "\n",
      "loss: 260.643188  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.5%, Avg loss: 2852.193631 \n",
      "\n",
      "loss: 2345.614502  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 812.804036 \n",
      "\n",
      "loss: 422.424896  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 366.954795 \n",
      "\n",
      "loss: 154.925049  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 1291.374854 \n",
      "\n",
      "loss: 1759.388184  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 354.958713 \n",
      "\n",
      "loss: 447.210266  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 664.171048 \n",
      "\n",
      "loss: 533.218689  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 279.081678 \n",
      "\n",
      "loss: 218.136368  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 186.384385 \n",
      "\n",
      "loss: 87.906815  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 372.138573 \n",
      "\n",
      "loss: 189.288956  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 46.9%, Avg loss: 301.055978 \n",
      "\n",
      "loss: 247.461670  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 695.042589 \n",
      "\n",
      "loss: 610.419312  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 83.947344 \n",
      "\n",
      "loss: 51.747948  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 56.832946 \n",
      "\n",
      "loss: 159.850708  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 61.454995 \n",
      "\n",
      "loss: 18.881666  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 39.696398 \n",
      "\n",
      "loss: 19.143843  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 38.097093 \n",
      "\n",
      "loss: 19.932230  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 41.452085 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [501, 390, 375], 'alpha': 0.00037382156979471714, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.05413856495383753}\n",
      "loss: 1.141153  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 2.652202 \n",
      "\n",
      "loss: 2.602594  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 3.171237 \n",
      "\n",
      "loss: 2.548379  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.222935 \n",
      "\n",
      "loss: 1.243336  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.947500 \n",
      "\n",
      "loss: 0.916825  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.901425 \n",
      "\n",
      "loss: 0.906045  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.824108 \n",
      "\n",
      "loss: 0.790570  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.378577 \n",
      "\n",
      "loss: 1.334078  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.828874 \n",
      "\n",
      "loss: 0.827974  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 5.163568 \n",
      "\n",
      "loss: 5.210364  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.551933 \n",
      "\n",
      "loss: 1.605187  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.719001 \n",
      "\n",
      "loss: 2.434646  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 2.370716 \n",
      "\n",
      "loss: 2.420940  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 0.892971 \n",
      "\n",
      "loss: 0.895812  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 0.947571 \n",
      "\n",
      "loss: 0.953233  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.708648 \n",
      "\n",
      "loss: 1.766082  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.842141 \n",
      "\n",
      "loss: 0.858214  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 15.123334 \n",
      "\n",
      "loss: 14.535766  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 3.255136 \n",
      "\n",
      "loss: 3.514104  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 2.177743 \n",
      "\n",
      "loss: 2.274498  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.891116 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [483], 'alpha': 0.0036289937644712017, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.4707411522108007}\n",
      "loss: 1.137099  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 5.871236 \n",
      "\n",
      "loss: 6.362584  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 14.586875 \n",
      "\n",
      "loss: 14.053205  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 8.603171 \n",
      "\n",
      "loss: 7.636111  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 11.389991 \n",
      "\n",
      "loss: 10.914196  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 18.693459 \n",
      "\n",
      "loss: 22.103378  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 2.808680 \n",
      "\n",
      "loss: 2.699848  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 3.124747 \n",
      "\n",
      "loss: 3.228647  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 10.404675 \n",
      "\n",
      "loss: 9.494071  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 7.030248 \n",
      "\n",
      "loss: 7.078675  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 16.521631 \n",
      "\n",
      "loss: 18.120113  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 11.170082 \n",
      "\n",
      "loss: 11.114626  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 6.306576 \n",
      "\n",
      "loss: 5.968463  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 6.084634 \n",
      "\n",
      "loss: 6.340819  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 8.658434 \n",
      "\n",
      "loss: 7.186857  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 15.803132 \n",
      "\n",
      "loss: 15.438581  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 12.192607 \n",
      "\n",
      "loss: 13.181984  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 25.279982 \n",
      "\n",
      "loss: 24.547733  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 20.840546 \n",
      "\n",
      "loss: 19.490597  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 11.186202 \n",
      "\n",
      "loss: 11.096601  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 8.319992 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [46], 'alpha': 0.09130389530107734, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.3486267819476365}\n",
      "loss: 1.296885  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 1.896870 \n",
      "\n",
      "loss: 1.952747  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 4.238484 \n",
      "\n",
      "loss: 4.319705  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 2.449296 \n",
      "\n",
      "loss: 2.317035  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 5.132005 \n",
      "\n",
      "loss: 4.822339  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 2.892412 \n",
      "\n",
      "loss: 2.928013  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 2.889271 \n",
      "\n",
      "loss: 3.109680  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.781176 \n",
      "\n",
      "loss: 2.788502  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 4.074688 \n",
      "\n",
      "loss: 3.827831  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 2.908985 \n",
      "\n",
      "loss: 2.690743  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 4.950935 \n",
      "\n",
      "loss: 5.094323  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 2.825014 \n",
      "\n",
      "loss: 2.886810  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 3.822487 \n",
      "\n",
      "loss: 3.691153  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 2.294316 \n",
      "\n",
      "loss: 2.208099  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 5.364614 \n",
      "\n",
      "loss: 5.374784  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 1.985201 \n",
      "\n",
      "loss: 1.926387  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 5.342625 \n",
      "\n",
      "loss: 5.233747  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 2.210617 \n",
      "\n",
      "loss: 2.213304  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 4.804255 \n",
      "\n",
      "loss: 4.905762  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 3.086809 \n",
      "\n",
      "loss: 3.111624  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 4.986612 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [443, 358], 'alpha': 0.10047501370071465, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.17366716227769363}\n",
      "loss: 33.536053  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.913793 \n",
      "\n",
      "loss: 0.909982  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.874453 \n",
      "\n",
      "loss: 0.875760  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.865008 \n",
      "\n",
      "loss: 0.858409  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.864327 \n",
      "\n",
      "loss: 0.862203  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.860566 \n",
      "\n",
      "loss: 0.865592  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.861199 \n",
      "\n",
      "loss: 0.852930  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.864240 \n",
      "\n",
      "loss: 0.863876  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.863540 \n",
      "\n",
      "loss: 0.872148  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.864285 \n",
      "\n",
      "loss: 0.860550  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.863279 \n",
      "\n",
      "loss: 0.871983  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.865983 \n",
      "\n",
      "loss: 0.863952  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.870041 \n",
      "\n",
      "loss: 0.858612  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.863088 \n",
      "\n",
      "loss: 0.863557  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.859109 \n",
      "\n",
      "loss: 0.869796  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.860572 \n",
      "\n",
      "loss: 0.867175  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.865530 \n",
      "\n",
      "loss: 0.864982  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.870316 \n",
      "\n",
      "loss: 0.874298  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.861495 \n",
      "\n",
      "loss: 0.865198  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.864618 \n",
      "\n",
      "loss: 0.861978  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.860902 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [346, 197], 'alpha': 0.00022119926654584084, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.4248028559370887}\n",
      "loss: 8.829305  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [62, 229], 'alpha': 6.367043779645354, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.21780002378933774}\n",
      "loss: 1.037430  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 10.358434 \n",
      "\n",
      "loss: 10.008728  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 11.363656 \n",
      "\n",
      "loss: 11.362963  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 10.383883 \n",
      "\n",
      "loss: 9.879942  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 12.296351 \n",
      "\n",
      "loss: 12.258871  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 10.513135 \n",
      "\n",
      "loss: 10.461026  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 12.409527 \n",
      "\n",
      "loss: 12.160256  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 11.151938 \n",
      "\n",
      "loss: 10.938115  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 11.635777 \n",
      "\n",
      "loss: 11.693115  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 9.918900 \n",
      "\n",
      "loss: 10.262436  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 11.551130 \n",
      "\n",
      "loss: 11.519923  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 10.354563 \n",
      "\n",
      "loss: 10.570164  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 44.4%, Avg loss: 11.816061 \n",
      "\n",
      "loss: 12.427556  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 10.411434 \n",
      "\n",
      "loss: 10.069534  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 11.191610 \n",
      "\n",
      "loss: 11.500845  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Avg loss: 10.263340 \n",
      "\n",
      "loss: 10.073526  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 11.080028 \n",
      "\n",
      "loss: 10.808541  [ 1024/33273]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 10.396163 \n",
      "\n",
      "loss: 10.494566  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 11.518858 \n",
      "\n",
      "loss: 11.409532  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 10.535699 \n",
      "\n",
      "loss: 10.971428  [ 1024/33274]\n",
      "Test Error: \n",
      " Accuracy: 43.7%, Avg loss: 11.350568 \n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "random.seed(random_state)\n",
    "\n",
    "input_size=X_test_selected.shape[-1]\n",
    "output_size=3\n",
    "epochs = 20\n",
    "num_search = 100\n",
    "\n",
    "accuracy_list = []\n",
    "hp_list = []\n",
    "pm_list = []\n",
    "\n",
    "for search in range(num_search):\n",
    "    print('-'*20)\n",
    "    hp = get_hps()\n",
    "    print(hp)\n",
    "    model_rs = DNN_rs(input_size=input_size, hidden_sizes=hp['hl'], output_size=output_size, activition_layer=hp['activition']).to(device)\n",
    "    \n",
    "    if hp['optimizer'] == 'SGD':\n",
    "        optimizer_rs = torch.optim.SGD(model_rs.parameters(), lr=hp['lr'], weight_decay=hp['alpha'])\n",
    "    elif hp['optimizer'] == 'Adam':\n",
    "        optimizer_rs = torch.optim.Adam(model_rs.parameters(), lr=hp['lr'], weight_decay=hp['alpha'])\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        break\n",
    "        \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        train_dataloader = train_dataloader_list[i%5]\n",
    "        validation_dataloader = val_dataloader_list[i%5]\n",
    "        model_rs.train()\n",
    "        train(train_dataloader, model_rs, loss_fn, optimizer_rs)\n",
    "        test(validation_dataloader, model_rs, loss_fn)\n",
    "        \n",
    "        if (i+1)%5==0:\n",
    "            model_rs.eval()\n",
    "            pred = model_rs(val_dataloader_k.dataset[:][0]).detach().cpu().max(axis=1).indices.numpy()\n",
    "            true = val_dataloader_k.dataset[:][1].cpu().numpy()\n",
    "            accuracy_list.append(accuracy_score(true, pred))\n",
    "            hp.update({'epoch': i+1})\n",
    "            hp_list.append(hp.copy())\n",
    "            pm_list.append(model_rs.state_dict())\n",
    "    print('-'*20)\n",
    "    \n",
    "accuracy_list = np.array(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8483121754183497\n",
      "F1 score: 0.8483121754183497\n",
      "Recall score: 0.8483121754183497\n",
      "Precision score: 0.8483121754183497\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_idx = np.argmax(accuracy_list)\n",
    "hp = hp_list[best_idx]\n",
    "model_best = DNN_rs(input_size=input_size, hidden_sizes=hp['hl'], output_size=output_size, activition_layer=hp['activition']).to(device)\n",
    "model_best.load_state_dict(pm_list[best_idx])\n",
    "\n",
    "model_best.eval()\n",
    "pred = model_best(test_dataloader.dataset[:][0]).detach().cpu().max(axis=1).indices.numpy()\n",
    "true = test_dataloader.dataset[:][1].cpu().numpy()\n",
    "\n",
    "performance_eval(true, pred, matrix_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8814195037507213"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 score: 0.8814195037507213\n",
      "Recall score: 0.8814195037507213\n",
      "Precision score: 0.8814195037507213\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_idx = np.argmax(accuracy_list)\n",
    "hp = hp_list[best_idx]\n",
    "model_best = DNN_rs(input_size=input_size, hidden_sizes=hp['hl'], output_size=output_size, activition_layer=hp['activition']).to(device)\n",
    "model_best.load_state_dict(pm_list[best_idx])\n",
    "\n",
    "model_best.eval()\n",
    "pred = model_best(test_dataloader.dataset[:][0]).detach().cpu().max(axis=1).indices.numpy()\n",
    "true = test_dataloader.dataset[:][1].cpu().numpy()\n",
    "\n",
    "performance_eval(true, pred, matrix_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hl': [346, 197],\n",
       " 'alpha': 0.00022119926654584084,\n",
       " 'activition': 'LeakyReLU',\n",
       " 'optimizer': 'SGD',\n",
       " 'lr': 0.4248028559370887,\n",
       " 'epoch': 20}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
