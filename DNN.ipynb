{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, precision_recall_curve, PrecisionRecallDisplay, roc_auc_score, RocCurveDisplay, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>annotation</th>\n",
       "      <th>min|FP1-F7</th>\n",
       "      <th>min|F7-T3</th>\n",
       "      <th>min|T3-T5</th>\n",
       "      <th>min|T5-O1</th>\n",
       "      <th>min|FP2-F8</th>\n",
       "      <th>min|F8-T4</th>\n",
       "      <th>min|T4-T6</th>\n",
       "      <th>min|T6-O2</th>\n",
       "      <th>...</th>\n",
       "      <th>norm_power_HF|CZ-C4</th>\n",
       "      <th>norm_power_HF|C4-T4</th>\n",
       "      <th>norm_power_HF|FP1-F3</th>\n",
       "      <th>norm_power_HF|F3-C3</th>\n",
       "      <th>norm_power_HF|C3-P3</th>\n",
       "      <th>norm_power_HF|P3-O1</th>\n",
       "      <th>norm_power_HF|FP2-F4</th>\n",
       "      <th>norm_power_HF|F4-C4</th>\n",
       "      <th>norm_power_HF|C4-P4</th>\n",
       "      <th>norm_power_HF|P4-O2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>61</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016087</td>\n",
       "      <td>0.066920</td>\n",
       "      <td>0.102402</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.690787</td>\n",
       "      <td>0.154544</td>\n",
       "      <td>0.062533</td>\n",
       "      <td>0.046460</td>\n",
       "      <td>0.066575</td>\n",
       "      <td>0.086999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>46</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024006</td>\n",
       "      <td>0.064857</td>\n",
       "      <td>0.031791</td>\n",
       "      <td>0.225788</td>\n",
       "      <td>0.409987</td>\n",
       "      <td>0.184671</td>\n",
       "      <td>0.071133</td>\n",
       "      <td>0.022369</td>\n",
       "      <td>0.079494</td>\n",
       "      <td>0.047536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>45</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037326</td>\n",
       "      <td>0.100177</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.622584</td>\n",
       "      <td>0.394504</td>\n",
       "      <td>0.225516</td>\n",
       "      <td>0.050673</td>\n",
       "      <td>0.044906</td>\n",
       "      <td>0.102142</td>\n",
       "      <td>0.068105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027546</td>\n",
       "      <td>0.107883</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.359140</td>\n",
       "      <td>0.276964</td>\n",
       "      <td>0.104977</td>\n",
       "      <td>0.018042</td>\n",
       "      <td>0.079467</td>\n",
       "      <td>0.078255</td>\n",
       "      <td>0.089385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036820</td>\n",
       "      <td>0.182520</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.328354</td>\n",
       "      <td>0.156929</td>\n",
       "      <td>0.151952</td>\n",
       "      <td>0.047532</td>\n",
       "      <td>0.135071</td>\n",
       "      <td>0.098320</td>\n",
       "      <td>0.137701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55451</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244334</td>\n",
       "      <td>0.625396</td>\n",
       "      <td>0.023821</td>\n",
       "      <td>0.058277</td>\n",
       "      <td>0.083594</td>\n",
       "      <td>0.114426</td>\n",
       "      <td>0.119654</td>\n",
       "      <td>0.295364</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>0.199585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55452</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.743060</td>\n",
       "      <td>0.076294</td>\n",
       "      <td>0.332341</td>\n",
       "      <td>0.228458</td>\n",
       "      <td>0.170603</td>\n",
       "      <td>0.351418</td>\n",
       "      <td>0.638666</td>\n",
       "      <td>0.490806</td>\n",
       "      <td>0.307429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55453</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296041</td>\n",
       "      <td>0.770194</td>\n",
       "      <td>0.041190</td>\n",
       "      <td>0.090919</td>\n",
       "      <td>0.186074</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.231053</td>\n",
       "      <td>0.770637</td>\n",
       "      <td>0.285257</td>\n",
       "      <td>0.413382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55454</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>77</td>\n",
       "      <td>82</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440360</td>\n",
       "      <td>0.720855</td>\n",
       "      <td>0.026959</td>\n",
       "      <td>0.026340</td>\n",
       "      <td>0.077674</td>\n",
       "      <td>0.269610</td>\n",
       "      <td>0.186769</td>\n",
       "      <td>0.790173</td>\n",
       "      <td>0.473615</td>\n",
       "      <td>0.415771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55455</th>\n",
       "      <td>11580</td>\n",
       "      <td>-1</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "      <td>80</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019325</td>\n",
       "      <td>0.735140</td>\n",
       "      <td>0.030715</td>\n",
       "      <td>0.077191</td>\n",
       "      <td>0.095298</td>\n",
       "      <td>0.317765</td>\n",
       "      <td>0.271859</td>\n",
       "      <td>0.675646</td>\n",
       "      <td>0.506836</td>\n",
       "      <td>0.561740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55456 rows Ã— 362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Patient  annotation  min|FP1-F7  min|F7-T3  min|T3-T5  min|T5-O1  \\\n",
       "0          258           1          50         61         57         53   \n",
       "1          258           1          48         62         60         46   \n",
       "2          258           1          51         60         59         45   \n",
       "3          258           1          49         58         56         42   \n",
       "4          258           1          45         57         61         41   \n",
       "...        ...         ...         ...        ...        ...        ...   \n",
       "55451    11580          -1          75         73         81         80   \n",
       "55452    11580          -1          74         74         77         71   \n",
       "55453    11580          -1          72         76         72         73   \n",
       "55454    11580          -1          77         82         74         75   \n",
       "55455    11580          -1          71         79         74         78   \n",
       "\n",
       "       min|FP2-F8  min|F8-T4  min|T4-T6  min|T6-O2  ...  norm_power_HF|CZ-C4  \\\n",
       "0              39         35         39         35  ...             0.016087   \n",
       "1              38         35         39         33  ...             0.024006   \n",
       "2              38         36         40         36  ...             0.037326   \n",
       "3              36         36         41         37  ...             0.027546   \n",
       "4              35         37         41         37  ...             0.036820   \n",
       "...           ...        ...        ...        ...  ...                  ...   \n",
       "55451          66         80         77         75  ...             0.244334   \n",
       "55452          79         75         82         77  ...             0.588236   \n",
       "55453          74         76         80         76  ...             0.296041   \n",
       "55454          82         85         80         76  ...             0.440360   \n",
       "55455          80         85         81         75  ...             1.019325   \n",
       "\n",
       "       norm_power_HF|C4-T4  norm_power_HF|FP1-F3  norm_power_HF|F3-C3  \\\n",
       "0                 0.066920              0.102402             0.481384   \n",
       "1                 0.064857              0.031791             0.225788   \n",
       "2                 0.100177              0.050009             0.622584   \n",
       "3                 0.107883              0.014017             0.359140   \n",
       "4                 0.182520              0.031397             0.328354   \n",
       "...                    ...                   ...                  ...   \n",
       "55451             0.625396              0.023821             0.058277   \n",
       "55452             0.743060              0.076294             0.332341   \n",
       "55453             0.770194              0.041190             0.090919   \n",
       "55454             0.720855              0.026959             0.026340   \n",
       "55455             0.735140              0.030715             0.077191   \n",
       "\n",
       "       norm_power_HF|C3-P3  norm_power_HF|P3-O1  norm_power_HF|FP2-F4  \\\n",
       "0                 0.690787             0.154544              0.062533   \n",
       "1                 0.409987             0.184671              0.071133   \n",
       "2                 0.394504             0.225516              0.050673   \n",
       "3                 0.276964             0.104977              0.018042   \n",
       "4                 0.156929             0.151952              0.047532   \n",
       "...                    ...                  ...                   ...   \n",
       "55451             0.083594             0.114426              0.119654   \n",
       "55452             0.228458             0.170603              0.351418   \n",
       "55453             0.186074             0.216797              0.231053   \n",
       "55454             0.077674             0.269610              0.186769   \n",
       "55455             0.095298             0.317765              0.271859   \n",
       "\n",
       "       norm_power_HF|F4-C4  norm_power_HF|C4-P4  norm_power_HF|P4-O2  \n",
       "0                 0.046460             0.066575             0.086999  \n",
       "1                 0.022369             0.079494             0.047536  \n",
       "2                 0.044906             0.102142             0.068105  \n",
       "3                 0.079467             0.078255             0.089385  \n",
       "4                 0.135071             0.098320             0.137701  \n",
       "...                    ...                  ...                  ...  \n",
       "55451             0.295364             0.185930             0.199585  \n",
       "55452             0.638666             0.490806             0.307429  \n",
       "55453             0.770637             0.285257             0.413382  \n",
       "55454             0.790173             0.473615             0.415771  \n",
       "55455             0.675646             0.506836             0.561740  \n",
       "\n",
       "[55456 rows x 362 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #load data on Yanqi Hong's computer\n",
    "# data = pd.read_csv('E:\\DATA\\TUD\\Master\\TUD_Master_Y1\\Q1\\EE4C12 Machine Learning For Electrical Engineering\\CodeLab\\Project\\S&S_SZD (1)\\Data\\Project_Data_EE4C12_S&S_SZD.csv')\n",
    "# data\n",
    "\n",
    "# load data on Zhixuan's computer\n",
    "data = pd.read_csv('D:\\\\User\\Zhixuan Ge\\Onedrive TUDelft\\OneDrive - Delft University of Technology\\Courses\\ML for EE\\SZD\\S&S_SZD\\Project_Data_EE4C12_S&S_SZD.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:].values\n",
    "y = np.int32(data['annotation'].values)\n",
    "\n",
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original number of training feature is:  360\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current number of training feature after feature selection is:  360\n"
     ]
    }
   ],
   "source": [
    "print(\"The original number of training feature is: \", X_train_scaled.shape[1])\n",
    "clf_etc = ExtraTreesClassifier(random_state=random_state).fit(X_train_scaled, y_train) # fit the model\n",
    "feature_importances = clf_etc.feature_importances_  # get the feature importance\n",
    "\n",
    "plt.bar(range(len(feature_importances)), feature_importances)   # plot the feature importance\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Feature importance\")\n",
    "plt.title(\"Feature importance of all features\")\n",
    "plt.show()\n",
    "\n",
    "important_feature_indices=np.argsort(feature_importances)   # sort the feature importance  \n",
    "important_feature_indices_cut=important_feature_indices[:int(len(important_feature_indices)/1.5)]   # select the most important features  \n",
    "\n",
    "X_train_selected=np.delete(X_train_scaled,important_feature_indices_cut,1)    # delete the least important features\n",
    "X_test_selected=np.delete(X_test_scaled,important_feature_indices_cut,1)      # delete the least important features\n",
    "print(\"The current number of training feature after feature selection is: \", X_train_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sample number of 0 is 1469\n",
      "The original sample number of 1 is 18177\n",
      "The original sample number of -1 is 21946\n",
      "\n",
      "The current sample number of 0 is 21946\n",
      "The current sample number of 1 is 21946\n",
      "The current sample number of -1 is 21946\n"
     ]
    }
   ],
   "source": [
    "num_z = np.sum(y_train==0)\n",
    "num_p = np.sum(y_train==1)\n",
    "num_n = np.sum(y_train==-1)\n",
    "print('The original sample number of 0 is', num_z)\n",
    "print('The original sample number of 1 is', num_p)\n",
    "print('The original sample number of -1 is', num_n)\n",
    "\n",
    "# oversampling\n",
    "sm = SMOTE(random_state=random_state)\n",
    "X_train_os, y_train_os = sm.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "num_z = np.sum(y_train_os==0)\n",
    "num_p = np.sum(y_train_os==1)\n",
    "num_n = np.sum(y_train_os==-1)\n",
    "\n",
    "print('')\n",
    "print('The current sample number of 0 is', num_z)\n",
    "print('The current sample number of 1 is', num_p)\n",
    "print('The current sample number of -1 is', num_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_tensor(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        X = np.float32(X)\n",
    "        X = torch.from_numpy(X)\n",
    "        # In pytorch, labels start from 0\n",
    "        # shift required\n",
    "        y = np.longlong(y) - y.min()\n",
    "        y = torch.from_numpy(y)\n",
    "        \n",
    "        self.X = X.to(device)\n",
    "        self.y = y.to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN class for random search\n",
    "class DNN_rs(nn.Module):\n",
    "    # Available activation functions: ReLU, Sigmoid, Tanh, LeakyReLU, ELU, SELU, Softplus, Softsign, LogSigmoid, PReLU, Softmin, Softmax, if the input is not in the list, ReLU will be used\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activition_layer=nn.ReLU()):\n",
    "        super(DNN_rs, self).__init__()\n",
    "        depth=len(hidden_sizes)\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i])) \n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            if activition_layer==\"ReLU\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activition_layer==\"Sigmoid\":\n",
    "                layers.append(nn.Sigmoid())\n",
    "            elif activition_layer==\"Tanh\":\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activition_layer==\"LeakyReLU\":\n",
    "                layers.append(nn.LeakyReLU())\n",
    "            elif activition_layer==\"ELU\":\n",
    "                layers.append(nn.ELU())\n",
    "            elif activition_layer==\"SELU\":\n",
    "                layers.append(nn.SELU())\n",
    "            elif activition_layer==\"Softplus\":\n",
    "                layers.append(nn.Softplus())\n",
    "            elif activition_layer==\"Softsign\":\n",
    "                layers.append(nn.Softsign())\n",
    "            elif activition_layer==\"LogSigmoid\":\n",
    "                layers.append(nn.LogSigmoid())\n",
    "            elif activition_layer==\"PReLU\":\n",
    "                layers.append(nn.PReLU())\n",
    "            elif activition_layer==\"Softmin\":\n",
    "                layers.append(nn.Softmin())\n",
    "            elif activition_layer==\"Softmax\":\n",
    "                layers.append(nn.Softmax())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear( hidden_sizes[-1], output_size))\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN_rs(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=360, out_features=100, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test output for DNN_rs\n",
    "input_size = X_train_scaled.shape[1]\n",
    "output_size = 3\n",
    "model_rs = DNN_rs(input_size, hidden_sizes=[100], output_size=output_size, activition_layer=\"PReLU\")\n",
    "print(model_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test functions\n",
    "def train(dataloader, model, loss_fn, optimizer):  \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>5f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # CrossEntropyLoss for multi-classification\n",
    "optimizer_rs = torch.optim.Adam(model_rs.parameters(),weight_decay=0.005)   # Adam optimizer for random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k fold cross validation function\n",
    "def Kfold_split(X_train, y_train, Shuffle_state, k=5):   # Split the data into training and validation sets\n",
    "    #example : X_k_train, y_k_train, X_k_val, y_k_val = Kfold_split(X_train, y_train, Shuffle_state)\n",
    "    kf = KFold(n_splits=k, random_state=Shuffle_state, shuffle=True)    # 5-fold cross validation\n",
    "    kf.get_n_splits(X_train)    \n",
    "    X_k_train = []\n",
    "    y_k_train = []\n",
    "    X_k_val = []\n",
    "    y_k_val = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):  # Split the data into training and validation sets\n",
    "        X_k_train.append(X_train[train_index])\n",
    "        y_k_train.append(y_train[train_index])\n",
    "        X_k_val.append(X_train[val_index])\n",
    "        y_k_val.append(y_train[val_index])\n",
    "    \n",
    "    return X_k_train, y_k_train, X_k_val, y_k_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_eval(true, pred, score_display=True, matrix_display=False, result_return=False):\n",
    "    Accuracy = accuracy_score(true, pred)\n",
    "    F1 = accuracy_score(true, pred)\n",
    "    Precision = accuracy_score(true, pred)\n",
    "    Recall = accuracy_score(true, pred)\n",
    "    \n",
    "    if score_display==True:\n",
    "        print(\"Accuracy: \" + str(Accuracy))\n",
    "        print(\"F1 score: \" + str(F1))\n",
    "        print(\"Recall score: \" + str(Recall))\n",
    "        print(\"Precision score: \" + str(Precision))\n",
    "        \n",
    "    if matrix_display==True:\n",
    "        label = ['Non-seizure', 'Transition','Seizure']\n",
    "        cm = confusion_matrix(true, pred)\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=label).plot()\n",
    "        plt.show(cm_display)\n",
    "    \n",
    "    if result_return:\n",
    "        return Accuracy, F1, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_dataloader_list = []\n",
    "val_dataloader_list = []\n",
    "\n",
    "X_k_train_list, y_k_train_list, X_k_val_list, y_k_val_list = Kfold_split(X_train_os, y_train_os, random_state)   # K-fold cross validation for DNN\n",
    "for i in range(5):\n",
    "    trainset_gpu = Data_tensor(X_k_train_list[i], y_k_train_list[i])\n",
    "    valset_gpu = Data_tensor(X_k_val_list[i], y_k_val_list[i])\n",
    "    train_dataloader_list.append(DataLoader(trainset_gpu, batch_size=batch_size, shuffle=True))\n",
    "    val_dataloader_list.append(DataLoader(valset_gpu, batch_size=batch_size, shuffle=True))\n",
    "    \n",
    "valset_gpu_k = Data_tensor(X_train_os, y_train_os)\n",
    "val_dataloader_k = DataLoader(valset_gpu_k, batch_size=batch_size, shuffle=True)\n",
    "testset_gpu = Data_tensor(X_test_selected, y_test)\n",
    "test_dataloader = DataLoader(testset_gpu, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_width=8\n",
    "max_width=512\n",
    "min_hl=1\n",
    "max_hl=3\n",
    "activition_list = [\"ReLU\", \"Tanh\", \"LeakyReLU\",\"Sigmoid\"]\n",
    "\n",
    "optimizer_list = [\"SGD\", \"Adam\"]\n",
    "min_learning_rate=0.0001\n",
    "max_learning_rate=0.5\n",
    "\n",
    "def get_hps():\n",
    "    num_hl = random.randint(min_hl, max_hl)\n",
    "    hl = []\n",
    "    for i in range(num_hl):\n",
    "        hl.append(random.randint(min_width, max_width))\n",
    "    alpha = np.power(10, random.uniform(-4, 1))\n",
    "    activition = random.choice(activition_list)\n",
    "    optimizer = random.choice(optimizer_list)\n",
    "    lr = random.uniform(min_learning_rate, max_learning_rate)\n",
    "    \n",
    "    return {'hl': hl, 'alpha': alpha, 'activition': activition, 'optimizer': optimizer, 'lr': lr}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "{'hl': [65, 20, 387], 'alpha': 0.002372174274717322, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.3682619599605898}\n",
      "loss: 1.099086  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.826968 \n",
      "\n",
      "loss: 0.850253  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.743726 \n",
      "\n",
      "loss: 0.797839  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.665820 \n",
      "\n",
      "loss: 0.665461  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.645986 \n",
      "\n",
      "loss: 0.645503  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.677543 \n",
      "\n",
      "loss: 0.666769  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.640137 \n",
      "\n",
      "loss: 0.659689  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.595963 \n",
      "\n",
      "loss: 0.609745  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.604705 \n",
      "\n",
      "loss: 0.596009  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.601817 \n",
      "\n",
      "loss: 0.591754  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.572528 \n",
      "\n",
      "loss: 0.539264  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.614174 \n",
      "\n",
      "loss: 0.588582  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.615639 \n",
      "\n",
      "loss: 0.578966  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.534598 \n",
      "\n",
      "loss: 0.595178  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.540741 \n",
      "\n",
      "loss: 0.562468  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.525826 \n",
      "\n",
      "loss: 0.539997  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.557241 \n",
      "\n",
      "loss: 0.549418  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.544517 \n",
      "\n",
      "loss: 0.539499  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.522069 \n",
      "\n",
      "loss: 0.515768  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.526192 \n",
      "\n",
      "loss: 0.507268  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.536178 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [387, 464, 287], 'alpha': 0.00027207846149075325, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.014995629997091364}\n",
      "loss: 1.117795  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098896 \n",
      "\n",
      "loss: 1.099125  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099023 \n",
      "\n",
      "loss: 1.099614  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.099380 \n",
      "\n",
      "loss: 1.099151  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098282 \n",
      "\n",
      "loss: 1.097829  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098494 \n",
      "\n",
      "loss: 1.097606  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.097990 \n",
      "\n",
      "loss: 1.097358  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 1.097694 \n",
      "\n",
      "loss: 1.098475  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.099322 \n",
      "\n",
      "loss: 1.098681  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.097233 \n",
      "\n",
      "loss: 1.097640  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 1.097109 \n",
      "\n",
      "loss: 1.097887  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 37.8%, Avg loss: 1.097379 \n",
      "\n",
      "loss: 1.096946  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.097163 \n",
      "\n",
      "loss: 1.096981  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.099337 \n",
      "\n",
      "loss: 1.098042  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.096792 \n",
      "\n",
      "loss: 1.096100  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.097418 \n",
      "\n",
      "loss: 1.097356  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.096967 \n",
      "\n",
      "loss: 1.097106  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.096536 \n",
      "\n",
      "loss: 1.095696  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 1.096457 \n",
      "\n",
      "loss: 1.096323  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.095843 \n",
      "\n",
      "loss: 1.096256  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.096175 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [127], 'alpha': 0.03363383896644277, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.3580382044999095}\n",
      "loss: 1.139552  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.736439 \n",
      "\n",
      "loss: 0.725433  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.706986 \n",
      "\n",
      "loss: 0.700083  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.693169 \n",
      "\n",
      "loss: 0.679961  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.684863 \n",
      "\n",
      "loss: 0.687860  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.686816 \n",
      "\n",
      "loss: 0.727735  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.686832 \n",
      "\n",
      "loss: 0.689602  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.690564 \n",
      "\n",
      "loss: 0.698681  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.680819 \n",
      "\n",
      "loss: 0.688149  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.676213 \n",
      "\n",
      "loss: 0.659025  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.681534 \n",
      "\n",
      "loss: 0.668883  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.683371 \n",
      "\n",
      "loss: 0.691296  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.678171 \n",
      "\n",
      "loss: 0.705136  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.678060 \n",
      "\n",
      "loss: 0.665372  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.674702 \n",
      "\n",
      "loss: 0.694703  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.679046 \n",
      "\n",
      "loss: 0.702952  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.679304 \n",
      "\n",
      "loss: 0.646197  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.678821 \n",
      "\n",
      "loss: 0.677784  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.675428 \n",
      "\n",
      "loss: 0.686277  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.670735 \n",
      "\n",
      "loss: 0.691893  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.679559 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [287, 222, 120], 'alpha': 0.017621595643491885, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.3794278028281707}\n",
      "loss: 1.097848  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.761615 \n",
      "\n",
      "loss: 0.743847  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.706409 \n",
      "\n",
      "loss: 0.680321  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.663155 \n",
      "\n",
      "loss: 0.646811  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.627557 \n",
      "\n",
      "loss: 0.604060  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.647753 \n",
      "\n",
      "loss: 0.625684  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.614424 \n",
      "\n",
      "loss: 0.601725  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.587719 \n",
      "\n",
      "loss: 0.597539  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.589558 \n",
      "\n",
      "loss: 0.586979  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.590922 \n",
      "\n",
      "loss: 0.622355  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.648711 \n",
      "\n",
      "loss: 0.634207  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.609662 \n",
      "\n",
      "loss: 0.567835  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.593847 \n",
      "\n",
      "loss: 0.581743  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.664500 \n",
      "\n",
      "loss: 0.666023  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.624194 \n",
      "\n",
      "loss: 0.636080  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.579064 \n",
      "\n",
      "loss: 0.546120  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.595605 \n",
      "\n",
      "loss: 0.593515  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.756791 \n",
      "\n",
      "loss: 0.751455  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.590924 \n",
      "\n",
      "loss: 0.588559  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.588956 \n",
      "\n",
      "loss: 0.603188  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.612366 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [365], 'alpha': 0.01297394315557648, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.10773534967758364}\n",
      "loss: 1.134394  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.765591 \n",
      "\n",
      "loss: 0.763925  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.712447 \n",
      "\n",
      "loss: 0.729032  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.688753 \n",
      "\n",
      "loss: 0.687656  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.665162 \n",
      "\n",
      "loss: 0.654149  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.655453 \n",
      "\n",
      "loss: 0.640711  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.650372 \n",
      "\n",
      "loss: 0.649915  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.634488 \n",
      "\n",
      "loss: 0.649277  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.625537 \n",
      "\n",
      "loss: 0.622623  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.615374 \n",
      "\n",
      "loss: 0.596686  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.614884 \n",
      "\n",
      "loss: 0.595885  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.617669 \n",
      "\n",
      "loss: 0.592901  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.604076 \n",
      "\n",
      "loss: 0.608439  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.601837 \n",
      "\n",
      "loss: 0.622360  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.594390 \n",
      "\n",
      "loss: 0.590201  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.594787 \n",
      "\n",
      "loss: 0.622006  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.599229 \n",
      "\n",
      "loss: 0.579045  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.589171 \n",
      "\n",
      "loss: 0.595845  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.588976 \n",
      "\n",
      "loss: 0.576969  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.582201 \n",
      "\n",
      "loss: 0.595904  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.585498 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [60, 55], 'alpha': 0.007936636741682485, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.30190264308030884}\n",
      "loss: 1.111572  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.205078 \n",
      "\n",
      "loss: 1.208398  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.706628 \n",
      "\n",
      "loss: 0.695567  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.677808 \n",
      "\n",
      "loss: 0.706153  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.670768 \n",
      "\n",
      "loss: 0.693985  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.633477 \n",
      "\n",
      "loss: 0.632760  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.644784 \n",
      "\n",
      "loss: 0.613320  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.652082 \n",
      "\n",
      "loss: 0.657785  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.670398 \n",
      "\n",
      "loss: 0.648417  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.682023 \n",
      "\n",
      "loss: 0.693500  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.654940 \n",
      "\n",
      "loss: 0.659978  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.674500 \n",
      "\n",
      "loss: 0.663137  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.649163 \n",
      "\n",
      "loss: 0.649247  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.663709 \n",
      "\n",
      "loss: 0.654674  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.674543 \n",
      "\n",
      "loss: 0.654963  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.691019 \n",
      "\n",
      "loss: 0.683905  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.684953 \n",
      "\n",
      "loss: 0.663555  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.671215 \n",
      "\n",
      "loss: 0.691880  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.685311 \n",
      "\n",
      "loss: 0.663572  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.692432 \n",
      "\n",
      "loss: 0.715101  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.688788 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [381], 'alpha': 0.019820946913584596, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.039492219019421244}\n",
      "loss: 1.102829  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.726884 \n",
      "\n",
      "loss: 0.728939  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.710817 \n",
      "\n",
      "loss: 0.697011  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.718118 \n",
      "\n",
      "loss: 0.721977  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.705421 \n",
      "\n",
      "loss: 0.702224  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.683231 \n",
      "\n",
      "loss: 0.665263  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.710533 \n",
      "\n",
      "loss: 0.678317  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.689605 \n",
      "\n",
      "loss: 0.709754  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.690912 \n",
      "\n",
      "loss: 0.730750  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.718693 \n",
      "\n",
      "loss: 0.726044  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.684231 \n",
      "\n",
      "loss: 0.684591  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.691344 \n",
      "\n",
      "loss: 0.661260  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.687936 \n",
      "\n",
      "loss: 0.675064  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.705362 \n",
      "\n",
      "loss: 0.706592  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.688863 \n",
      "\n",
      "loss: 0.707236  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.695409 \n",
      "\n",
      "loss: 0.688392  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.699590 \n",
      "\n",
      "loss: 0.720558  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.700027 \n",
      "\n",
      "loss: 0.694700  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.682627 \n",
      "\n",
      "loss: 0.680441  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.678562 \n",
      "\n",
      "loss: 0.666973  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.679716 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [432, 329], 'alpha': 0.12376526094181511, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.35231546092384025}\n",
      "loss: 1.108553  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.889841 \n",
      "\n",
      "loss: 0.905298  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.879499 \n",
      "\n",
      "loss: 0.877302  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.884593 \n",
      "\n",
      "loss: 0.890486  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.874472 \n",
      "\n",
      "loss: 0.872738  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.883035 \n",
      "\n",
      "loss: 0.870384  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.882763 \n",
      "\n",
      "loss: 0.859748  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.878461 \n",
      "\n",
      "loss: 0.868983  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.886072 \n",
      "\n",
      "loss: 0.896572  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.879154 \n",
      "\n",
      "loss: 0.876568  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.879988 \n",
      "\n",
      "loss: 0.873198  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.885165 \n",
      "\n",
      "loss: 0.889714  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.883515 \n",
      "\n",
      "loss: 0.899212  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.879390 \n",
      "\n",
      "loss: 0.865620  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.877731 \n",
      "\n",
      "loss: 0.891279  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.882248 \n",
      "\n",
      "loss: 0.884995  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.887873 \n",
      "\n",
      "loss: 0.885753  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.877139 \n",
      "\n",
      "loss: 0.884517  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.882485 \n",
      "\n",
      "loss: 0.883489  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.877142 \n",
      "\n",
      "loss: 0.880377  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.887231 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [346], 'alpha': 0.0013787685809762544, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.4276733287354719}\n",
      "loss: 1.117752  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.679707 \n",
      "\n",
      "loss: 0.660930  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.617795 \n",
      "\n",
      "loss: 0.633733  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.566552 \n",
      "\n",
      "loss: 0.549600  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.608614 \n",
      "\n",
      "loss: 0.588623  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.523088 \n",
      "\n",
      "loss: 0.514478  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.530892 \n",
      "\n",
      "loss: 0.506162  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.500879 \n",
      "\n",
      "loss: 0.476405  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.552950 \n",
      "\n",
      "loss: 0.504116  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.477887 \n",
      "\n",
      "loss: 0.480158  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.469777 \n",
      "\n",
      "loss: 0.461400  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.484142 \n",
      "\n",
      "loss: 0.480044  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.452331 \n",
      "\n",
      "loss: 0.462225  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.465975 \n",
      "\n",
      "loss: 0.501307  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.479275 \n",
      "\n",
      "loss: 0.496847  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.434014 \n",
      "\n",
      "loss: 0.419870  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.451024 \n",
      "\n",
      "loss: 0.442425  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.458922 \n",
      "\n",
      "loss: 0.428588  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.441003 \n",
      "\n",
      "loss: 0.470692  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.413626 \n",
      "\n",
      "loss: 0.406144  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.451981 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [202], 'alpha': 0.0024539630296714055, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.18515346546172962}\n",
      "loss: 1.102155  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.743772 \n",
      "\n",
      "loss: 0.772362  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.678812 \n",
      "\n",
      "loss: 0.694231  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.648975 \n",
      "\n",
      "loss: 0.661777  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.614883 \n",
      "\n",
      "loss: 0.617025  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.603777 \n",
      "\n",
      "loss: 0.604542  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.590128 \n",
      "\n",
      "loss: 0.588983  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.567636 \n",
      "\n",
      "loss: 0.549572  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.562535 \n",
      "\n",
      "loss: 0.557368  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.540303 \n",
      "\n",
      "loss: 0.548948  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.539439 \n",
      "\n",
      "loss: 0.535018  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.528884 \n",
      "\n",
      "loss: 0.530134  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.509234 \n",
      "\n",
      "loss: 0.525175  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.529983 \n",
      "\n",
      "loss: 0.568577  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.495125 \n",
      "\n",
      "loss: 0.497602  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.484413 \n",
      "\n",
      "loss: 0.490550  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.528324 \n",
      "\n",
      "loss: 0.529874  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.485787 \n",
      "\n",
      "loss: 0.500217  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.502357 \n",
      "\n",
      "loss: 0.500535  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.465392 \n",
      "\n",
      "loss: 0.481145  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.457943 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [351], 'alpha': 0.0021621663806048173, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.267116524527151}\n",
      "loss: 1.109805  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.706867 \n",
      "\n",
      "loss: 0.697601  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.642990 \n",
      "\n",
      "loss: 0.643972  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.598386 \n",
      "\n",
      "loss: 0.629689  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.605424 \n",
      "\n",
      "loss: 0.607403  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.552598 \n",
      "\n",
      "loss: 0.536170  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.552936 \n",
      "\n",
      "loss: 0.525162  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.530595 \n",
      "\n",
      "loss: 0.507101  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.520120 \n",
      "\n",
      "loss: 0.475820  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.507626 \n",
      "\n",
      "loss: 0.535443  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.511966 \n",
      "\n",
      "loss: 0.528610  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.483993 \n",
      "\n",
      "loss: 0.474034  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.500344 \n",
      "\n",
      "loss: 0.482010  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.477137 \n",
      "\n",
      "loss: 0.471323  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.458502 \n",
      "\n",
      "loss: 0.428814  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.468648 \n",
      "\n",
      "loss: 0.473019  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.451069 \n",
      "\n",
      "loss: 0.428718  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.438691 \n",
      "\n",
      "loss: 0.445851  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.447318 \n",
      "\n",
      "loss: 0.451628  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.445827 \n",
      "\n",
      "loss: 0.426529  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.415457 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [91], 'alpha': 0.020478637537502738, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.3423386640698383}\n",
      "loss: 1.112485  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.731857 \n",
      "\n",
      "loss: 0.769056  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.682015 \n",
      "\n",
      "loss: 0.660156  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.667244 \n",
      "\n",
      "loss: 0.689661  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.657197 \n",
      "\n",
      "loss: 0.702237  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.652124 \n",
      "\n",
      "loss: 0.605861  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.651093 \n",
      "\n",
      "loss: 0.638654  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.640396 \n",
      "\n",
      "loss: 0.626895  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.638614 \n",
      "\n",
      "loss: 0.664301  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.636198 \n",
      "\n",
      "loss: 0.655975  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.652769 \n",
      "\n",
      "loss: 0.654769  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.645117 \n",
      "\n",
      "loss: 0.591695  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.638602 \n",
      "\n",
      "loss: 0.596971  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.629539 \n",
      "\n",
      "loss: 0.625395  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.635473 \n",
      "\n",
      "loss: 0.636051  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.641212 \n",
      "\n",
      "loss: 0.623765  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.642340 \n",
      "\n",
      "loss: 0.666164  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.633315 \n",
      "\n",
      "loss: 0.633462  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.652626 \n",
      "\n",
      "loss: 0.632330  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.631490 \n",
      "\n",
      "loss: 0.642164  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.630326 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [125], 'alpha': 1.2853250450978235, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.13394366390025378}\n",
      "loss: 1.117789  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.096914 \n",
      "\n",
      "loss: 1.096401  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098828 \n",
      "\n",
      "loss: 1.098928  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098534 \n",
      "\n",
      "loss: 1.098767  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098630 \n",
      "\n",
      "loss: 1.099308  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098574 \n",
      "\n",
      "loss: 1.098631  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098809 \n",
      "\n",
      "loss: 1.098485  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098829 \n",
      "\n",
      "loss: 1.099133  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098507 \n",
      "\n",
      "loss: 1.098496  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098606 \n",
      "\n",
      "loss: 1.098803  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098794 \n",
      "\n",
      "loss: 1.098483  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.099070 \n",
      "\n",
      "loss: 1.099214  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098695 \n",
      "\n",
      "loss: 1.099430  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098602 \n",
      "\n",
      "loss: 1.098910  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098907 \n",
      "\n",
      "loss: 1.098298  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098598 \n",
      "\n",
      "loss: 1.098446  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.100157 \n",
      "\n",
      "loss: 1.099177  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.100780 \n",
      "\n",
      "loss: 1.100783  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 13.7%, Avg loss: 1.109479 \n",
      "\n",
      "loss: 1.109725  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 16.2%, Avg loss: 1.109837 \n",
      "\n",
      "loss: 1.108704  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.9%, Avg loss: 2.809870 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [475], 'alpha': 5.182610512973461, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.32775378878091055}\n",
      "loss: 1.097520  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 19.3%, Avg loss: 533696834638.769226 \n",
      "\n",
      "loss: 519063207936.000000  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 19.3%, Avg loss: 6051140133584143374090240.000000 \n",
      "\n",
      "loss: 5707961607209138970951680.000000  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 18.3%, Avg loss:      inf \n",
      "\n",
      "loss:   inf  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [460, 476], 'alpha': 0.16377942837573226, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.06990119673080061}\n",
      "loss: 1.149574  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.955057 \n",
      "\n",
      "loss: 0.970260  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.891569 \n",
      "\n",
      "loss: 0.909848  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 0.890388 \n",
      "\n",
      "loss: 0.888852  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.937532 \n",
      "\n",
      "loss: 0.937542  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.914483 \n",
      "\n",
      "loss: 0.915635  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.892409 \n",
      "\n",
      "loss: 0.876578  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 7.760459 \n",
      "\n",
      "loss: 7.636234  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 35.5%, Avg loss: 5.109662 \n",
      "\n",
      "loss: 5.136890  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 2.631387 \n",
      "\n",
      "loss: 2.488924  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 40.9%, Avg loss: 4.057770 \n",
      "\n",
      "loss: 4.072617  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 3.179940 \n",
      "\n",
      "loss: 3.098140  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 22.9%, Avg loss: 1.412669 \n",
      "\n",
      "loss: 1.443065  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.895727 \n",
      "\n",
      "loss: 0.928352  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.884297 \n",
      "\n",
      "loss: 0.886927  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.902306 \n",
      "\n",
      "loss: 0.889361  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 0.900403 \n",
      "\n",
      "loss: 0.872170  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.898516 \n",
      "\n",
      "loss: 0.893659  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.879954 \n",
      "\n",
      "loss: 0.894126  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.894061 \n",
      "\n",
      "loss: 0.913694  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.903905 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [295, 283, 142], 'alpha': 0.5433367219666622, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.18106202221243411}\n",
      "loss: 1.164990  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.104302 \n",
      "\n",
      "loss: 1.108645  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.113237 \n",
      "\n",
      "loss: 1.115504  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.137590 \n",
      "\n",
      "loss: 1.148496  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.154289 \n",
      "\n",
      "loss: 1.143209  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.126257 \n",
      "\n",
      "loss: 1.123752  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.208654 \n",
      "\n",
      "loss: 1.215389  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.223133 \n",
      "\n",
      "loss: 1.226141  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.159870 \n",
      "\n",
      "loss: 1.183107  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 3.806085 \n",
      "\n",
      "loss: 3.925254  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 2.545610 \n",
      "\n",
      "loss: 2.485091  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.170086 \n",
      "\n",
      "loss: 1.171427  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.113762 \n",
      "\n",
      "loss: 1.112094  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.101398 \n",
      "\n",
      "loss: 1.103781  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.101509 \n",
      "\n",
      "loss: 1.102255  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.115860 \n",
      "\n",
      "loss: 1.127641  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.118790 \n",
      "\n",
      "loss: 1.116217  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.166840 \n",
      "\n",
      "loss: 1.171502  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.102574 \n",
      "\n",
      "loss: 1.101171  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.130704 \n",
      "\n",
      "loss: 1.109801  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.100349 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [268], 'alpha': 0.029348044873745035, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.0765053755421957}\n",
      "loss: 1.142640  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.809738 \n",
      "\n",
      "loss: 0.789354  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.760207 \n",
      "\n",
      "loss: 0.780109  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.737037 \n",
      "\n",
      "loss: 0.738607  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.714650 \n",
      "\n",
      "loss: 0.706461  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.712001 \n",
      "\n",
      "loss: 0.702659  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.707200 \n",
      "\n",
      "loss: 0.718276  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.693691 \n",
      "\n",
      "loss: 0.675395  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.688884 \n",
      "\n",
      "loss: 0.726780  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.678542 \n",
      "\n",
      "loss: 0.672649  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.682046 \n",
      "\n",
      "loss: 0.686262  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.683613 \n",
      "\n",
      "loss: 0.666094  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.674985 \n",
      "\n",
      "loss: 0.660851  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.674050 \n",
      "\n",
      "loss: 0.670702  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.666278 \n",
      "\n",
      "loss: 0.680934  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.670728 \n",
      "\n",
      "loss: 0.663517  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.674917 \n",
      "\n",
      "loss: 0.673660  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.666735 \n",
      "\n",
      "loss: 0.669081  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.667846 \n",
      "\n",
      "loss: 0.657436  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.660093 \n",
      "\n",
      "loss: 0.658161  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.666160 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [413], 'alpha': 0.25258413428440013, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.19087148132461773}\n",
      "loss: 1.115357  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.855944 \n",
      "\n",
      "loss: 0.858414  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.911049 \n",
      "\n",
      "loss: 0.902978  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.903693 \n",
      "\n",
      "loss: 0.924843  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.926300 \n",
      "\n",
      "loss: 0.941329  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.929572 \n",
      "\n",
      "loss: 0.924500  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.948115 \n",
      "\n",
      "loss: 0.950259  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.904949 \n",
      "\n",
      "loss: 0.912178  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.928743 \n",
      "\n",
      "loss: 0.930004  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.908416 \n",
      "\n",
      "loss: 0.910039  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.912768 \n",
      "\n",
      "loss: 0.914499  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.912007 \n",
      "\n",
      "loss: 0.923674  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.923645 \n",
      "\n",
      "loss: 0.917608  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 0.944474 \n",
      "\n",
      "loss: 0.932091  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.920276 \n",
      "\n",
      "loss: 0.935952  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.915736 \n",
      "\n",
      "loss: 0.914623  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.924093 \n",
      "\n",
      "loss: 0.911162  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.911642 \n",
      "\n",
      "loss: 0.911576  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.916805 \n",
      "\n",
      "loss: 0.909148  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.911073 \n",
      "\n",
      "loss: 0.913392  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.924189 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [278, 136], 'alpha': 7.167899178925419, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.3408870134763847}\n",
      "loss: 1.099947  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [392, 144, 401], 'alpha': 0.16025404222065995, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.2174391488094856}\n",
      "loss: 1.095652  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 1.566560 \n",
      "\n",
      "loss: 1.507560  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.892009 \n",
      "\n",
      "loss: 0.923006  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.759762 \n",
      "\n",
      "loss: 0.750578  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.734986 \n",
      "\n",
      "loss: 0.757476  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.772682 \n",
      "\n",
      "loss: 0.761040  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.823979 \n",
      "\n",
      "loss: 0.827487  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.840210 \n",
      "\n",
      "loss: 0.846087  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.887817 \n",
      "\n",
      "loss: 0.903669  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.927812 \n",
      "\n",
      "loss: 0.927246  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.940001 \n",
      "\n",
      "loss: 0.946198  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.955061 \n",
      "\n",
      "loss: 0.951463  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.960245 \n",
      "\n",
      "loss: 0.951124  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.973814 \n",
      "\n",
      "loss: 0.964020  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 0.984018 \n",
      "\n",
      "loss: 0.964749  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 0.986172 \n",
      "\n",
      "loss: 0.978903  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 0.988860 \n",
      "\n",
      "loss: 0.968974  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 0.994468 \n",
      "\n",
      "loss: 1.006976  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 0.999450 \n",
      "\n",
      "loss: 0.997566  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 0.993529 \n",
      "\n",
      "loss: 0.997033  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 47.3%, Avg loss: 1.000068 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [9, 496], 'alpha': 0.4078122870250983, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.2538900691347022}\n",
      "loss: 1.089155  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098675 \n",
      "\n",
      "loss: 1.098490  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098754  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098801 \n",
      "\n",
      "loss: 1.098666  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098810 \n",
      "\n",
      "loss: 1.098138  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098573 \n",
      "\n",
      "loss: 1.098584  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098635 \n",
      "\n",
      "loss: 1.098545  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098655 \n",
      "\n",
      "loss: 1.098561  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098825 \n",
      "\n",
      "loss: 1.099042  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098562  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098633 \n",
      "\n",
      "loss: 1.098752  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098623 \n",
      "\n",
      "loss: 1.098685  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098604 \n",
      "\n",
      "loss: 1.098592  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098480 \n",
      "\n",
      "loss: 1.098552  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098685 \n",
      "\n",
      "loss: 1.098672  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098556 \n",
      "\n",
      "loss: 1.098506  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098629 \n",
      "\n",
      "loss: 1.098501  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098541  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098668 \n",
      "\n",
      "loss: 1.098665  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098543  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098616 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [453], 'alpha': 0.13381381972127276, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.18701953491031487}\n",
      "loss: 1.091939  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.820723 \n",
      "\n",
      "loss: 0.823691  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.816448 \n",
      "\n",
      "loss: 0.815724  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.820808 \n",
      "\n",
      "loss: 0.828321  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.813690 \n",
      "\n",
      "loss: 0.842211  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.823591 \n",
      "\n",
      "loss: 0.806082  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.825624 \n",
      "\n",
      "loss: 0.817410  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.816884 \n",
      "\n",
      "loss: 0.819406  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.817013 \n",
      "\n",
      "loss: 0.808482  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.812668 \n",
      "\n",
      "loss: 0.813513  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.824485 \n",
      "\n",
      "loss: 0.815684  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.825628 \n",
      "\n",
      "loss: 0.820747  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.816620 \n",
      "\n",
      "loss: 0.830132  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.817173 \n",
      "\n",
      "loss: 0.841049  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.813184 \n",
      "\n",
      "loss: 0.837746  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.822455 \n",
      "\n",
      "loss: 0.807229  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.825060 \n",
      "\n",
      "loss: 0.821472  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.814523 \n",
      "\n",
      "loss: 0.808765  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.817672 \n",
      "\n",
      "loss: 0.818377  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.810348 \n",
      "\n",
      "loss: 0.791087  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.821434 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [284], 'alpha': 5.854620354066272, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.24435399344367104}\n",
      "loss: 1.102260  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 40.9%, Avg loss: 1.092612 \n",
      "\n",
      "loss: 1.092014  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098590 \n",
      "\n",
      "loss: 1.098545  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098808 \n",
      "\n",
      "loss: 1.098939  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098594  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098681  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098633 \n",
      "\n",
      "loss: 1.098543  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098657 \n",
      "\n",
      "loss: 1.099263  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098496 \n",
      "\n",
      "loss: 1.098508  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098710 \n",
      "\n",
      "loss: 1.098391  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098619 \n",
      "\n",
      "loss: 1.099016  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098648 \n",
      "\n",
      "loss: 1.098722  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098656 \n",
      "\n",
      "loss: 1.098989  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098657 \n",
      "\n",
      "loss: 1.099013  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098687 \n",
      "\n",
      "loss: 1.098787  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098543 \n",
      "\n",
      "loss: 1.099194  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098712 \n",
      "\n",
      "loss: 1.098964  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098560  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098711 \n",
      "\n",
      "loss: 1.099042  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098826  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098776 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [483], 'alpha': 0.006529859485637705, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.029056790730444357}\n",
      "loss: 1.116648  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.843545 \n",
      "\n",
      "loss: 0.865444  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.791531 \n",
      "\n",
      "loss: 0.799284  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.767330 \n",
      "\n",
      "loss: 0.770779  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.742569 \n",
      "\n",
      "loss: 0.734511  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.738279 \n",
      "\n",
      "loss: 0.735272  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.732498 \n",
      "\n",
      "loss: 0.743667  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.711981 \n",
      "\n",
      "loss: 0.679131  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.701189 \n",
      "\n",
      "loss: 0.667563  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.686935 \n",
      "\n",
      "loss: 0.707269  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.687690 \n",
      "\n",
      "loss: 0.655043  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.687751 \n",
      "\n",
      "loss: 0.673630  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.672650 \n",
      "\n",
      "loss: 0.666037  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.664834 \n",
      "\n",
      "loss: 0.664592  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.653911 \n",
      "\n",
      "loss: 0.673328  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.654718 \n",
      "\n",
      "loss: 0.657226  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.658878 \n",
      "\n",
      "loss: 0.627057  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.645031 \n",
      "\n",
      "loss: 0.629638  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.639377 \n",
      "\n",
      "loss: 0.599700  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.630398 \n",
      "\n",
      "loss: 0.639021  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.630920 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [492, 48, 51], 'alpha': 0.45660217323948477, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.06428289310343142}\n",
      "loss: 1.101519  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098584 \n",
      "\n",
      "loss: 1.098644  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098622 \n",
      "\n",
      "loss: 1.098604  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098694 \n",
      "\n",
      "loss: 1.098668  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098608 \n",
      "\n",
      "loss: 1.098613  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098641 \n",
      "\n",
      "loss: 1.098542  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098644  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098636 \n",
      "\n",
      "loss: 1.098667  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098734 \n",
      "\n",
      "loss: 1.098591  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098625 \n",
      "\n",
      "loss: 1.098612  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098608 \n",
      "\n",
      "loss: 1.098722  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098636 \n",
      "\n",
      "loss: 1.098233  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098615  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098692 \n",
      "\n",
      "loss: 1.098566  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098624 \n",
      "\n",
      "loss: 1.098652  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098625 \n",
      "\n",
      "loss: 1.098615  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098636  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098629 \n",
      "\n",
      "loss: 1.098501  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098708 \n",
      "\n",
      "loss: 1.098618  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098619 \n",
      "\n",
      "loss: 1.098678  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098641 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [492, 289], 'alpha': 0.0006693215295907433, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.46446385515178823}\n",
      "loss: 1.114682  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 1.039188 \n",
      "\n",
      "loss: 1.029229  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 0.983025 \n",
      "\n",
      "loss: 0.970895  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.865907 \n",
      "\n",
      "loss: 0.833467  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.837746 \n",
      "\n",
      "loss: 0.842968  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.824208 \n",
      "\n",
      "loss: 0.828887  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.810237 \n",
      "\n",
      "loss: 0.810152  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.842954 \n",
      "\n",
      "loss: 0.812518  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.829567 \n",
      "\n",
      "loss: 0.827825  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.766272 \n",
      "\n",
      "loss: 0.777939  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.776279 \n",
      "\n",
      "loss: 0.780936  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.768216 \n",
      "\n",
      "loss: 0.797442  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.758239 \n",
      "\n",
      "loss: 0.736374  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.794973 \n",
      "\n",
      "loss: 0.786033  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.756122 \n",
      "\n",
      "loss: 0.767296  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.750352 \n",
      "\n",
      "loss: 0.717610  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.746599 \n",
      "\n",
      "loss: 0.738799  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.745874 \n",
      "\n",
      "loss: 0.752798  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.745941 \n",
      "\n",
      "loss: 0.781655  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.713248 \n",
      "\n",
      "loss: 0.698259  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.737486 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [361, 110, 373], 'alpha': 0.003618940614103234, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.4498170809130084}\n",
      "loss: 1.101215  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 1323.159724 \n",
      "\n",
      "loss: 1321.206543  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 324.906691 \n",
      "\n",
      "loss: 478.246002  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 213.940325 \n",
      "\n",
      "loss: 223.066315  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 54.160389 \n",
      "\n",
      "loss: 56.104958  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 813.286771 \n",
      "\n",
      "loss: 912.705261  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 106.312411 \n",
      "\n",
      "loss: 99.460007  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 89.031902 \n",
      "\n",
      "loss: 108.525597  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 16.378696 \n",
      "\n",
      "loss: 16.739450  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 13.185096 \n",
      "\n",
      "loss: 12.376169  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 8.092266 \n",
      "\n",
      "loss: 7.311952  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 9.488686 \n",
      "\n",
      "loss: 10.334570  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 21.413368 \n",
      "\n",
      "loss: 17.702854  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 12.548959 \n",
      "\n",
      "loss: 14.225386  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 3.383833 \n",
      "\n",
      "loss: 3.876972  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 2.947216 \n",
      "\n",
      "loss: 3.596187  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 4.026424 \n",
      "\n",
      "loss: 3.260917  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 3.235503 \n",
      "\n",
      "loss: 2.922856  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 13.999917 \n",
      "\n",
      "loss: 14.422481  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 1.353358 \n",
      "\n",
      "loss: 1.517571  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 8.264813 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [69, 134], 'alpha': 0.0013288828148723235, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.29419552835677093}\n",
      "loss: 1.113998  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.723965 \n",
      "\n",
      "loss: 0.702262  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.662880 \n",
      "\n",
      "loss: 0.670368  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.601510 \n",
      "\n",
      "loss: 0.604473  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.601638 \n",
      "\n",
      "loss: 0.547771  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.586952 \n",
      "\n",
      "loss: 0.557241  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.598830 \n",
      "\n",
      "loss: 0.620823  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.530594 \n",
      "\n",
      "loss: 0.555399  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.540391 \n",
      "\n",
      "loss: 0.549208  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.516523 \n",
      "\n",
      "loss: 0.481911  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.490257 \n",
      "\n",
      "loss: 0.527532  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.515680 \n",
      "\n",
      "loss: 0.499330  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.456475 \n",
      "\n",
      "loss: 0.440424  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.464564 \n",
      "\n",
      "loss: 0.484175  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.555869 \n",
      "\n",
      "loss: 0.536170  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.444598 \n",
      "\n",
      "loss: 0.430472  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.592384 \n",
      "\n",
      "loss: 0.550316  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.443775 \n",
      "\n",
      "loss: 0.427967  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.418249 \n",
      "\n",
      "loss: 0.479412  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.552119 \n",
      "\n",
      "loss: 0.531602  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.438724 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [309], 'alpha': 0.0012620801094670535, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.11454799772719608}\n",
      "loss: 1.124626  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.762548 \n",
      "\n",
      "loss: 0.769158  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.702988 \n",
      "\n",
      "loss: 0.668337  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.669079 \n",
      "\n",
      "loss: 0.632779  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.642211 \n",
      "\n",
      "loss: 0.638546  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.624942 \n",
      "\n",
      "loss: 0.626240  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.620260 \n",
      "\n",
      "loss: 0.628578  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.591109 \n",
      "\n",
      "loss: 0.554956  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.585884 \n",
      "\n",
      "loss: 0.612295  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.571079 \n",
      "\n",
      "loss: 0.565740  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.555007 \n",
      "\n",
      "loss: 0.559388  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.561639 \n",
      "\n",
      "loss: 0.548111  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.537021 \n",
      "\n",
      "loss: 0.587802  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.531916 \n",
      "\n",
      "loss: 0.563997  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.524085 \n",
      "\n",
      "loss: 0.526399  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.511103 \n",
      "\n",
      "loss: 0.521862  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.516948 \n",
      "\n",
      "loss: 0.523714  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.498097 \n",
      "\n",
      "loss: 0.465361  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.497967 \n",
      "\n",
      "loss: 0.488706  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.488364 \n",
      "\n",
      "loss: 0.462791  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.476368 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [448], 'alpha': 0.004488925992678658, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.33452199137031063}\n",
      "loss: 1.136418  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 5.543630 \n",
      "\n",
      "loss: 5.346434  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 7.322510 \n",
      "\n",
      "loss: 7.373460  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 3.513872 \n",
      "\n",
      "loss: 3.228498  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 53.3%, Avg loss: 6.294441 \n",
      "\n",
      "loss: 6.040623  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 6.463704 \n",
      "\n",
      "loss: 6.186821  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 5.256144 \n",
      "\n",
      "loss: 5.568411  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 3.660547 \n",
      "\n",
      "loss: 3.861160  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 3.604091 \n",
      "\n",
      "loss: 3.834311  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 6.627227 \n",
      "\n",
      "loss: 6.581809  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 38.4%, Avg loss: 7.864739 \n",
      "\n",
      "loss: 7.901932  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: 5.578797 \n",
      "\n",
      "loss: 5.893466  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 4.400130 \n",
      "\n",
      "loss: 4.444596  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 4.172209 \n",
      "\n",
      "loss: 3.888770  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 4.711891 \n",
      "\n",
      "loss: 4.714586  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 2.730424 \n",
      "\n",
      "loss: 2.802152  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 4.385572 \n",
      "\n",
      "loss: 4.273835  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 6.335853 \n",
      "\n",
      "loss: 6.295392  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 3.010322 \n",
      "\n",
      "loss: 2.866823  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 7.132702 \n",
      "\n",
      "loss: 7.131830  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 9.285473 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [284], 'alpha': 0.00045873221879225867, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.3923312502029476}\n",
      "loss: 1.088496  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 1.855296 \n",
      "\n",
      "loss: 1.785975  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.001493 \n",
      "\n",
      "loss: 1.034869  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 1.136391 \n",
      "\n",
      "loss: 1.157273  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.140160 \n",
      "\n",
      "loss: 1.273802  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.858021 \n",
      "\n",
      "loss: 0.825558  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.903979 \n",
      "\n",
      "loss: 0.963225  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.808766 \n",
      "\n",
      "loss: 1.605375  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 1.074741 \n",
      "\n",
      "loss: 1.072775  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.927548 \n",
      "\n",
      "loss: 0.878446  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.855771 \n",
      "\n",
      "loss: 0.811591  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.897344 \n",
      "\n",
      "loss: 0.946242  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.968415 \n",
      "\n",
      "loss: 0.963905  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.748830 \n",
      "\n",
      "loss: 0.770124  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.698641 \n",
      "\n",
      "loss: 0.676127  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.763182 \n",
      "\n",
      "loss: 0.762028  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.676839 \n",
      "\n",
      "loss: 0.701946  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.658239 \n",
      "\n",
      "loss: 0.740366  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.750859 \n",
      "\n",
      "loss: 0.744644  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.782433 \n",
      "\n",
      "loss: 0.744944  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.697503 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [105, 56], 'alpha': 0.0003052488743843354, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.21184695364765838}\n",
      "loss: 1.116258  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.728423 \n",
      "\n",
      "loss: 0.769011  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.727631 \n",
      "\n",
      "loss: 0.680669  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.687461 \n",
      "\n",
      "loss: 0.682214  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.691123 \n",
      "\n",
      "loss: 0.673746  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.700539 \n",
      "\n",
      "loss: 0.641027  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.767078 \n",
      "\n",
      "loss: 0.716232  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.714342 \n",
      "\n",
      "loss: 0.721064  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.691218 \n",
      "\n",
      "loss: 0.665227  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.688601 \n",
      "\n",
      "loss: 0.690992  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.753662 \n",
      "\n",
      "loss: 0.734687  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.767028 \n",
      "\n",
      "loss: 0.778914  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.700487 \n",
      "\n",
      "loss: 0.717756  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.711485 \n",
      "\n",
      "loss: 0.708436  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.765296 \n",
      "\n",
      "loss: 0.787027  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.855812 \n",
      "\n",
      "loss: 0.837889  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.995597 \n",
      "\n",
      "loss: 0.956674  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.821079 \n",
      "\n",
      "loss: 0.784676  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.670487 \n",
      "\n",
      "loss: 0.658787  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.976901 \n",
      "\n",
      "loss: 0.929253  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.744859 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [450, 381], 'alpha': 0.00018657606794409573, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.20137037892292414}\n",
      "loss: 1.095157  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.736707 \n",
      "\n",
      "loss: 0.707169  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.650291 \n",
      "\n",
      "loss: 0.618819  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.664435 \n",
      "\n",
      "loss: 0.716393  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.569791 \n",
      "\n",
      "loss: 0.610202  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.547179 \n",
      "\n",
      "loss: 0.518647  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.541826 \n",
      "\n",
      "loss: 0.547768  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.515497 \n",
      "\n",
      "loss: 0.506985  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.568724 \n",
      "\n",
      "loss: 0.530681  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.554299 \n",
      "\n",
      "loss: 0.539110  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.452062 \n",
      "\n",
      "loss: 0.484697  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.453545 \n",
      "\n",
      "loss: 0.435200  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.415379 \n",
      "\n",
      "loss: 0.387891  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.449539 \n",
      "\n",
      "loss: 0.445799  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.528392 \n",
      "\n",
      "loss: 0.534520  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.380305 \n",
      "\n",
      "loss: 0.366479  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.467660 \n",
      "\n",
      "loss: 0.441436  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.376321 \n",
      "\n",
      "loss: 0.393526  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.427344 \n",
      "\n",
      "loss: 0.460219  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.358029 \n",
      "\n",
      "loss: 0.372214  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.451152 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [417, 449], 'alpha': 0.00035183995885073016, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.26818924837978114}\n",
      "loss: 1.135284  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.744452 \n",
      "\n",
      "loss: 0.719078  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.718651 \n",
      "\n",
      "loss: 0.723960  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.681526 \n",
      "\n",
      "loss: 0.678105  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.679316 \n",
      "\n",
      "loss: 0.683264  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.658962 \n",
      "\n",
      "loss: 0.677013  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.680025 \n",
      "\n",
      "loss: 0.699565  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.651366 \n",
      "\n",
      "loss: 0.670693  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.619200 \n",
      "\n",
      "loss: 0.616302  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.632296 \n",
      "\n",
      "loss: 0.616594  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.611881 \n",
      "\n",
      "loss: 0.633956  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.627093 \n",
      "\n",
      "loss: 0.588997  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.591548 \n",
      "\n",
      "loss: 0.608678  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.609007 \n",
      "\n",
      "loss: 0.627647  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.593548 \n",
      "\n",
      "loss: 0.569052  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.638700 \n",
      "\n",
      "loss: 0.663926  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.563824 \n",
      "\n",
      "loss: 0.568866  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.615339 \n",
      "\n",
      "loss: 0.619367  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.546334 \n",
      "\n",
      "loss: 0.505168  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.505848 \n",
      "\n",
      "loss: 0.525307  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.531720 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [224], 'alpha': 0.0008268756043227057, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.4372555284398829}\n",
      "loss: 1.118606  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.937114 \n",
      "\n",
      "loss: 1.777893  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 2.505901 \n",
      "\n",
      "loss: 2.364402  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 46.5%, Avg loss: 1.414242 \n",
      "\n",
      "loss: 1.465762  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.171865 \n",
      "\n",
      "loss: 1.137243  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.930729 \n",
      "\n",
      "loss: 0.942208  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.806904 \n",
      "\n",
      "loss: 0.817998  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.894234 \n",
      "\n",
      "loss: 0.912576  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.826172 \n",
      "\n",
      "loss: 0.842105  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.722445 \n",
      "\n",
      "loss: 0.692647  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.812115 \n",
      "\n",
      "loss: 0.832042  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.709512 \n",
      "\n",
      "loss: 0.757997  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.803745 \n",
      "\n",
      "loss: 0.814587  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.696101 \n",
      "\n",
      "loss: 0.692186  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.778070 \n",
      "\n",
      "loss: 0.733586  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.724355 \n",
      "\n",
      "loss: 0.717650  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.722597 \n",
      "\n",
      "loss: 0.721379  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.673938 \n",
      "\n",
      "loss: 0.680820  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.659641 \n",
      "\n",
      "loss: 0.676122  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.673933 \n",
      "\n",
      "loss: 0.668996  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.815878 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [234], 'alpha': 1.096759042295366, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.3261072325098612}\n",
      "loss: 1.176097  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098715 \n",
      "\n",
      "loss: 1.098503  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098612 \n",
      "\n",
      "loss: 1.098555  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098445 \n",
      "\n",
      "loss: 1.099198  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098571 \n",
      "\n",
      "loss: 1.098655  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098575 \n",
      "\n",
      "loss: 1.098880  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098597 \n",
      "\n",
      "loss: 1.098927  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098687 \n",
      "\n",
      "loss: 1.098538  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098732 \n",
      "\n",
      "loss: 1.098626  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098692 \n",
      "\n",
      "loss: 1.098589  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098541 \n",
      "\n",
      "loss: 1.098512  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098600 \n",
      "\n",
      "loss: 1.098589  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098648  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098659 \n",
      "\n",
      "loss: 1.098578  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098614 \n",
      "\n",
      "loss: 1.098613  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098644 \n",
      "\n",
      "loss: 1.098900  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098627 \n",
      "\n",
      "loss: 1.098617  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098875  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098540 \n",
      "\n",
      "loss: 1.098585  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098691 \n",
      "\n",
      "loss: 1.098387  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098611 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [436, 15, 504], 'alpha': 0.00029262665363958537, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.20326638757044074}\n",
      "loss: 1.128710  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.757339 \n",
      "\n",
      "loss: 0.712239  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.724713 \n",
      "\n",
      "loss: 0.746323  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.792213 \n",
      "\n",
      "loss: 0.768642  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.655329 \n",
      "\n",
      "loss: 0.665567  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.646783 \n",
      "\n",
      "loss: 0.634876  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.691923 \n",
      "\n",
      "loss: 0.703315  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.588764 \n",
      "\n",
      "loss: 0.593182  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.585402 \n",
      "\n",
      "loss: 0.534617  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.572254 \n",
      "\n",
      "loss: 0.576060  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.553170 \n",
      "\n",
      "loss: 0.543157  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.552614 \n",
      "\n",
      "loss: 0.538069  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.557699 \n",
      "\n",
      "loss: 0.535555  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.552407 \n",
      "\n",
      "loss: 0.537047  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.530762 \n",
      "\n",
      "loss: 0.501187  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.524648 \n",
      "\n",
      "loss: 0.515132  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.517494 \n",
      "\n",
      "loss: 0.520583  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.545936 \n",
      "\n",
      "loss: 0.575705  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.503667 \n",
      "\n",
      "loss: 0.480068  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.522674 \n",
      "\n",
      "loss: 0.469692  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.493345 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [117, 450], 'alpha': 0.010120488197724748, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.18954866217656036}\n",
      "loss: 1.093028  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.762188 \n",
      "\n",
      "loss: 0.746642  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.692473 \n",
      "\n",
      "loss: 0.699210  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.647589 \n",
      "\n",
      "loss: 0.622508  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.616472 \n",
      "\n",
      "loss: 0.605800  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.604304 \n",
      "\n",
      "loss: 0.572179  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.598766 \n",
      "\n",
      "loss: 0.611406  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.584744 \n",
      "\n",
      "loss: 0.609214  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.573727 \n",
      "\n",
      "loss: 0.572504  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.556597 \n",
      "\n",
      "loss: 0.513616  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.557212 \n",
      "\n",
      "loss: 0.555493  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.549209 \n",
      "\n",
      "loss: 0.526350  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.547189 \n",
      "\n",
      "loss: 0.546022  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.541405 \n",
      "\n",
      "loss: 0.577899  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.526764 \n",
      "\n",
      "loss: 0.507406  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.519929 \n",
      "\n",
      "loss: 0.512160  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.528158 \n",
      "\n",
      "loss: 0.495469  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.529882 \n",
      "\n",
      "loss: 0.543028  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.506326 \n",
      "\n",
      "loss: 0.527392  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.507935 \n",
      "\n",
      "loss: 0.483927  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.514032 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [143, 482], 'alpha': 0.8324401326318135, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.21156144224648243}\n",
      "loss: 1.144019  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.141170 \n",
      "\n",
      "loss: 2.134257  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 5.093039 \n",
      "\n",
      "loss: 5.117720  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.748081 \n",
      "\n",
      "loss: 2.753681  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 5.043334 \n",
      "\n",
      "loss: 4.903658  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.615817 \n",
      "\n",
      "loss: 1.653213  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 3.398032 \n",
      "\n",
      "loss: 3.495467  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 9.910407 \n",
      "\n",
      "loss: 9.993049  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.714260 \n",
      "\n",
      "loss: 1.688434  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.349495 \n",
      "\n",
      "loss: 1.353865  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 5.658104 \n",
      "\n",
      "loss: 5.591352  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.818570 \n",
      "\n",
      "loss: 5.034249  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 15.695110 \n",
      "\n",
      "loss: 15.750702  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 5.590344 \n",
      "\n",
      "loss: 5.650031  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 2.105040 \n",
      "\n",
      "loss: 2.042433  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 9.535068 \n",
      "\n",
      "loss: 9.638505  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 7.238080 \n",
      "\n",
      "loss: 7.002159  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 6.073186 \n",
      "\n",
      "loss: 5.990690  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.381665 \n",
      "\n",
      "loss: 1.355424  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.257886 \n",
      "\n",
      "loss: 2.284286  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 3.383687 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [409, 292, 346], 'alpha': 0.39087813410005207, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.14842424196473264}\n",
      "loss: 1.108882  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098549 \n",
      "\n",
      "loss: 1.098488  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098627 \n",
      "\n",
      "loss: 1.098895  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098634 \n",
      "\n",
      "loss: 1.098632  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098589 \n",
      "\n",
      "loss: 1.098679  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098699 \n",
      "\n",
      "loss: 1.098909  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098604 \n",
      "\n",
      "loss: 1.098647  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098596 \n",
      "\n",
      "loss: 1.098673  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098638 \n",
      "\n",
      "loss: 1.098522  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098651 \n",
      "\n",
      "loss: 1.098599  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098612 \n",
      "\n",
      "loss: 1.098724  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098634 \n",
      "\n",
      "loss: 1.098396  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098640 \n",
      "\n",
      "loss: 1.098549  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098698 \n",
      "\n",
      "loss: 1.098628  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098683 \n",
      "\n",
      "loss: 1.098732  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098669 \n",
      "\n",
      "loss: 1.098817  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098630 \n",
      "\n",
      "loss: 1.098767  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098674 \n",
      "\n",
      "loss: 1.098637  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098754 \n",
      "\n",
      "loss: 1.098563  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098641 \n",
      "\n",
      "loss: 1.098569  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098637 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [304], 'alpha': 0.4771840226513163, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.02867691992645079}\n",
      "loss: 1.089328  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 0.990575 \n",
      "\n",
      "loss: 0.989509  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.995294 \n",
      "\n",
      "loss: 0.996248  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.994735 \n",
      "\n",
      "loss: 0.991570  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 1.011535 \n",
      "\n",
      "loss: 1.021346  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 1.017769 \n",
      "\n",
      "loss: 1.012411  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 0.990697 \n",
      "\n",
      "loss: 0.990246  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.999072 \n",
      "\n",
      "loss: 1.005682  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.005639 \n",
      "\n",
      "loss: 0.999359  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.013115 \n",
      "\n",
      "loss: 1.012333  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.993952 \n",
      "\n",
      "loss: 1.001370  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 1.012739 \n",
      "\n",
      "loss: 1.017166  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.007514 \n",
      "\n",
      "loss: 1.000828  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 1.004514 \n",
      "\n",
      "loss: 0.997504  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 1.003822 \n",
      "\n",
      "loss: 1.005636  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.012565 \n",
      "\n",
      "loss: 1.005003  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 1.012271 \n",
      "\n",
      "loss: 1.002998  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.000211 \n",
      "\n",
      "loss: 1.004527  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Avg loss: 0.994384 \n",
      "\n",
      "loss: 0.994332  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.008827 \n",
      "\n",
      "loss: 1.009578  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 1.009546 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [252, 265, 478], 'alpha': 1.834845327334873, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.4803933737468977}\n",
      "loss: 1.117840  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098699 \n",
      "\n",
      "loss: 1.097700  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098871 \n",
      "\n",
      "loss: 1.099045  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098819 \n",
      "\n",
      "loss: 1.098667  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098588 \n",
      "\n",
      "loss: 1.098530  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098643 \n",
      "\n",
      "loss: 1.098758  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098708 \n",
      "\n",
      "loss: 1.098682  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098660 \n",
      "\n",
      "loss: 1.098788  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098604 \n",
      "\n",
      "loss: 1.098544  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098707 \n",
      "\n",
      "loss: 1.098491  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.099022 \n",
      "\n",
      "loss: 1.098653  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098711  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098656 \n",
      "\n",
      "loss: 1.098871  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098934 \n",
      "\n",
      "loss: 1.098802  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098669 \n",
      "\n",
      "loss: 1.098798  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098600 \n",
      "\n",
      "loss: 1.098862  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098640 \n",
      "\n",
      "loss: 1.098312  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098718 \n",
      "\n",
      "loss: 1.099374  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098928 \n",
      "\n",
      "loss: 1.098481  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098582 \n",
      "\n",
      "loss: 1.098819  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098540 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [443], 'alpha': 0.0008494245424178485, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.20194739810400691}\n",
      "loss: 1.075316  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.720097 \n",
      "\n",
      "loss: 0.700943  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.658855 \n",
      "\n",
      "loss: 0.687505  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.616948 \n",
      "\n",
      "loss: 0.625295  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.588081 \n",
      "\n",
      "loss: 0.583338  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.567916 \n",
      "\n",
      "loss: 0.534803  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.563643 \n",
      "\n",
      "loss: 0.563068  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.537974 \n",
      "\n",
      "loss: 0.535089  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.522856 \n",
      "\n",
      "loss: 0.521178  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.500688 \n",
      "\n",
      "loss: 0.499018  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.492635 \n",
      "\n",
      "loss: 0.482735  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.523277 \n",
      "\n",
      "loss: 0.516901  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.497482 \n",
      "\n",
      "loss: 0.467051  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.464956 \n",
      "\n",
      "loss: 0.458786  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.459009 \n",
      "\n",
      "loss: 0.454896  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.505204 \n",
      "\n",
      "loss: 0.517785  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.552194 \n",
      "\n",
      "loss: 0.537817  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.438835 \n",
      "\n",
      "loss: 0.431626  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.466001 \n",
      "\n",
      "loss: 0.418415  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.444428 \n",
      "\n",
      "loss: 0.425237  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.408722 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [134, 304, 312], 'alpha': 0.00015801863700144815, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.32872084491981396}\n",
      "loss: 1.092964  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 35.9%, Avg loss: 1.567121 \n",
      "\n",
      "loss: 1.215484  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.190816 \n",
      "\n",
      "loss: 1.092190  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.104160 \n",
      "\n",
      "loss: 1.091615  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.110589 \n",
      "\n",
      "loss: 1.104411  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.120608 \n",
      "\n",
      "loss: 1.124718  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.104098 \n",
      "\n",
      "loss: 1.108562  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.102076 \n",
      "\n",
      "loss: 1.093632  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.104619 \n",
      "\n",
      "loss: 1.106271  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.149941 \n",
      "\n",
      "loss: 1.128703  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 1.100998 \n",
      "\n",
      "loss: 1.102211  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.103912 \n",
      "\n",
      "loss: 1.087062  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.092469 \n",
      "\n",
      "loss: 1.106521  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.092739 \n",
      "\n",
      "loss: 1.091534  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 34.3%, Avg loss: 1.102559 \n",
      "\n",
      "loss: 1.081910  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.090951 \n",
      "\n",
      "loss: 1.091593  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.089367 \n",
      "\n",
      "loss: 1.086000  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 1.090785 \n",
      "\n",
      "loss: 1.092030  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.084539 \n",
      "\n",
      "loss: 1.091106  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.091668 \n",
      "\n",
      "loss: 1.078450  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.086988 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [275, 169, 486], 'alpha': 0.0020129290986662433, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.13288069481442377}\n",
      "loss: 1.101044  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.827340 \n",
      "\n",
      "loss: 0.832561  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.724517 \n",
      "\n",
      "loss: 0.741099  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.666165 \n",
      "\n",
      "loss: 0.670325  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.616142 \n",
      "\n",
      "loss: 0.591206  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.606874 \n",
      "\n",
      "loss: 0.626144  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.571509 \n",
      "\n",
      "loss: 0.553743  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.568603 \n",
      "\n",
      "loss: 0.566539  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.523307 \n",
      "\n",
      "loss: 0.516577  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.499884 \n",
      "\n",
      "loss: 0.478182  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.488041 \n",
      "\n",
      "loss: 0.470945  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.491467 \n",
      "\n",
      "loss: 0.475861  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.467046 \n",
      "\n",
      "loss: 0.467366  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.575594 \n",
      "\n",
      "loss: 0.555519  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.485598 \n",
      "\n",
      "loss: 0.520009  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.436077 \n",
      "\n",
      "loss: 0.417466  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.451839 \n",
      "\n",
      "loss: 0.431976  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.474404 \n",
      "\n",
      "loss: 0.484720  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.408809 \n",
      "\n",
      "loss: 0.434057  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.399552 \n",
      "\n",
      "loss: 0.424377  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.524552 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [351], 'alpha': 0.16885111941871558, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.4645162640195865}\n",
      "loss: 1.096308  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 2.529583 \n",
      "\n",
      "loss: 2.434814  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 3.817306 \n",
      "\n",
      "loss: 3.892947  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 44.9%, Avg loss: 1.690671 \n",
      "\n",
      "loss: 1.740244  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 41.0%, Avg loss: 5.608557 \n",
      "\n",
      "loss: 5.670079  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 3.057219 \n",
      "\n",
      "loss: 3.014821  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 4.696225 \n",
      "\n",
      "loss: 4.925827  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 10.159493 \n",
      "\n",
      "loss: 10.573489  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 4.217871 \n",
      "\n",
      "loss: 4.094740  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 22.5%, Avg loss: 7.130813 \n",
      "\n",
      "loss: 7.297345  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 4.749686 \n",
      "\n",
      "loss: 4.793533  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 2.850776 \n",
      "\n",
      "loss: 2.803076  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 8.012299 \n",
      "\n",
      "loss: 7.761384  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 22.2%, Avg loss: 4.429903 \n",
      "\n",
      "loss: 4.089062  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 34.6%, Avg loss: 2.869742 \n",
      "\n",
      "loss: 2.735357  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 36.0%, Avg loss: 6.492146 \n",
      "\n",
      "loss: 6.391399  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 10.218098 \n",
      "\n",
      "loss: 10.533094  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.755706 \n",
      "\n",
      "loss: 1.722798  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 10.929305 \n",
      "\n",
      "loss: 11.210026  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 31.5%, Avg loss: 4.834298 \n",
      "\n",
      "loss: 4.848553  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 5.987725 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [12], 'alpha': 0.01956264676406525, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.2688634386104035}\n",
      "loss: 1.123799  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.772769 \n",
      "\n",
      "loss: 0.776937  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.715311 \n",
      "\n",
      "loss: 0.699039  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.695594 \n",
      "\n",
      "loss: 0.683061  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.672038 \n",
      "\n",
      "loss: 0.670927  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.672609 \n",
      "\n",
      "loss: 0.682220  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.665681 \n",
      "\n",
      "loss: 0.656781  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.657358 \n",
      "\n",
      "loss: 0.638055  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.657018 \n",
      "\n",
      "loss: 0.665266  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.647889 \n",
      "\n",
      "loss: 0.696840  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.648902 \n",
      "\n",
      "loss: 0.642215  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.656465 \n",
      "\n",
      "loss: 0.656612  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.650251 \n",
      "\n",
      "loss: 0.663229  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.648984 \n",
      "\n",
      "loss: 0.619549  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.641437 \n",
      "\n",
      "loss: 0.633280  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.644257 \n",
      "\n",
      "loss: 0.676127  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.649825 \n",
      "\n",
      "loss: 0.654143  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.643310 \n",
      "\n",
      "loss: 0.653019  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.644758 \n",
      "\n",
      "loss: 0.652505  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.635248 \n",
      "\n",
      "loss: 0.613485  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.652846 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [143, 75, 485], 'alpha': 0.005559608046637538, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.18482659166056808}\n",
      "loss: 1.102139  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.801731 \n",
      "\n",
      "loss: 0.776744  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.693171 \n",
      "\n",
      "loss: 0.651185  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.635723 \n",
      "\n",
      "loss: 0.638748  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.601097 \n",
      "\n",
      "loss: 0.599759  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.588631 \n",
      "\n",
      "loss: 0.584560  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.575981 \n",
      "\n",
      "loss: 0.593720  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.567882 \n",
      "\n",
      "loss: 0.583717  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.527608 \n",
      "\n",
      "loss: 0.550091  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.508246 \n",
      "\n",
      "loss: 0.502752  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.494979 \n",
      "\n",
      "loss: 0.475102  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.491694 \n",
      "\n",
      "loss: 0.473294  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.501535 \n",
      "\n",
      "loss: 0.488950  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.562584 \n",
      "\n",
      "loss: 0.620477  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.491810 \n",
      "\n",
      "loss: 0.488552  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.518440 \n",
      "\n",
      "loss: 0.490000  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.455015 \n",
      "\n",
      "loss: 0.464239  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.455870 \n",
      "\n",
      "loss: 0.443383  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.472251 \n",
      "\n",
      "loss: 0.504118  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.440272 \n",
      "\n",
      "loss: 0.463466  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.414947 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [232], 'alpha': 1.4747716254292624, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.3339648155359205}\n",
      "loss: 1.142057  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098593  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098643 \n",
      "\n",
      "loss: 1.098385  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098837 \n",
      "\n",
      "loss: 1.098517  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098808 \n",
      "\n",
      "loss: 1.098218  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098718 \n",
      "\n",
      "loss: 1.098714  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098637 \n",
      "\n",
      "loss: 1.098694  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098610 \n",
      "\n",
      "loss: 1.098740  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098517 \n",
      "\n",
      "loss: 1.098453  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098736 \n",
      "\n",
      "loss: 1.098791  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098555 \n",
      "\n",
      "loss: 1.098214  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098674 \n",
      "\n",
      "loss: 1.098960  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098609 \n",
      "\n",
      "loss: 1.098429  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098762 \n",
      "\n",
      "loss: 1.099068  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098650 \n",
      "\n",
      "loss: 1.098671  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098607 \n",
      "\n",
      "loss: 1.098729  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098877  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098735 \n",
      "\n",
      "loss: 1.098208  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098598 \n",
      "\n",
      "loss: 1.098520  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098583 \n",
      "\n",
      "loss: 1.098077  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098589 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [161, 485, 347], 'alpha': 0.00032955849401379, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.0578027923425092}\n",
      "loss: 1.104584  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.884800 \n",
      "\n",
      "loss: 0.882059  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.873804 \n",
      "\n",
      "loss: 0.853451  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.813816 \n",
      "\n",
      "loss: 0.768279  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.653346 \n",
      "\n",
      "loss: 0.630734  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.623218 \n",
      "\n",
      "loss: 0.632561  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.588949 \n",
      "\n",
      "loss: 0.548882  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.637649 \n",
      "\n",
      "loss: 0.640268  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.607777 \n",
      "\n",
      "loss: 0.614000  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.628440 \n",
      "\n",
      "loss: 0.618227  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.660064 \n",
      "\n",
      "loss: 0.665228  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.584195 \n",
      "\n",
      "loss: 0.511644  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.604336 \n",
      "\n",
      "loss: 0.611283  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.542384 \n",
      "\n",
      "loss: 0.521099  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.581927 \n",
      "\n",
      "loss: 0.586407  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.620146 \n",
      "\n",
      "loss: 0.615624  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.553945 \n",
      "\n",
      "loss: 0.557009  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.588544 \n",
      "\n",
      "loss: 0.553133  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.867965 \n",
      "\n",
      "loss: 0.849766  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.671770 \n",
      "\n",
      "loss: 0.617986  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.569210 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [388], 'alpha': 0.05836039254922183, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.30245443053240895}\n",
      "loss: 1.080725  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 2.149906 \n",
      "\n",
      "loss: 2.177402  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.836533 \n",
      "\n",
      "loss: 0.884267  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.783418 \n",
      "\n",
      "loss: 0.752398  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.794953 \n",
      "\n",
      "loss: 0.803848  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.808939 \n",
      "\n",
      "loss: 0.798358  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.810922 \n",
      "\n",
      "loss: 0.787269  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.799401 \n",
      "\n",
      "loss: 0.789825  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.814313 \n",
      "\n",
      "loss: 0.808810  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.815934 \n",
      "\n",
      "loss: 0.851397  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.810450 \n",
      "\n",
      "loss: 0.817077  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.830111 \n",
      "\n",
      "loss: 0.849617  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 3.201333 \n",
      "\n",
      "loss: 3.429329  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 145.725281 \n",
      "\n",
      "loss: 145.791718  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 183.243342 \n",
      "\n",
      "loss: 189.290070  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 6.347580 \n",
      "\n",
      "loss: 6.281875  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.796165 \n",
      "\n",
      "loss: 0.764683  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.764062 \n",
      "\n",
      "loss: 0.770236  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.756036 \n",
      "\n",
      "loss: 0.759231  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.751534 \n",
      "\n",
      "loss: 0.766171  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.769036 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [183, 112, 359], 'alpha': 0.14831661135493762, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.12563551390974007}\n",
      "loss: 1.098784  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.5%, Avg loss: 1.047503 \n",
      "\n",
      "loss: 1.051254  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.965538 \n",
      "\n",
      "loss: 0.960107  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 0.979297 \n",
      "\n",
      "loss: 0.971454  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 0.991337 \n",
      "\n",
      "loss: 1.006593  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 0.989354 \n",
      "\n",
      "loss: 0.979873  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 0.995422 \n",
      "\n",
      "loss: 1.000236  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.991546 \n",
      "\n",
      "loss: 0.980522  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 52.7%, Avg loss: 0.991418 \n",
      "\n",
      "loss: 1.002492  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 0.989840 \n",
      "\n",
      "loss: 0.991534  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.994799 \n",
      "\n",
      "loss: 0.993374  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.986683 \n",
      "\n",
      "loss: 0.966530  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 0.980984 \n",
      "\n",
      "loss: 0.980599  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.984254 \n",
      "\n",
      "loss: 0.985784  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.994244 \n",
      "\n",
      "loss: 0.996725  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 1.003478 \n",
      "\n",
      "loss: 0.998221  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 46.9%, Avg loss: 0.994151 \n",
      "\n",
      "loss: 0.999125  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 0.993083 \n",
      "\n",
      "loss: 0.993877  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.990649 \n",
      "\n",
      "loss: 0.994132  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 1.003504 \n",
      "\n",
      "loss: 0.992478  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.987749 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [55], 'alpha': 0.14839601428674118, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.001872489974982372}\n",
      "loss: 1.140193  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 30.9%, Avg loss: 1.109056 \n",
      "\n",
      "loss: 1.108867  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 37.3%, Avg loss: 1.089917 \n",
      "\n",
      "loss: 1.082072  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 1.075601 \n",
      "\n",
      "loss: 1.078870  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 1.053582 \n",
      "\n",
      "loss: 1.060404  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.042973 \n",
      "\n",
      "loss: 1.043094  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.6%, Avg loss: 1.032339 \n",
      "\n",
      "loss: 1.017367  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 1.021424 \n",
      "\n",
      "loss: 1.030374  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 1.015782 \n",
      "\n",
      "loss: 1.000486  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 1.000811 \n",
      "\n",
      "loss: 1.010979  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 0.997226 \n",
      "\n",
      "loss: 0.998864  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 0.991349 \n",
      "\n",
      "loss: 0.980608  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.982572 \n",
      "\n",
      "loss: 0.973170  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.979526 \n",
      "\n",
      "loss: 0.988359  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.968031 \n",
      "\n",
      "loss: 0.976033  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.967653 \n",
      "\n",
      "loss: 0.969018  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.964113 \n",
      "\n",
      "loss: 0.978843  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.955849 \n",
      "\n",
      "loss: 0.945158  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.954386 \n",
      "\n",
      "loss: 0.944012  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.944801 \n",
      "\n",
      "loss: 0.954242  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.946136 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [334], 'alpha': 7.924826614167679, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.275885042521083}\n",
      "loss: 1.077268  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.090925 \n",
      "\n",
      "loss: 1.090258  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098560 \n",
      "\n",
      "loss: 1.098404  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098606 \n",
      "\n",
      "loss: 1.098559  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098641 \n",
      "\n",
      "loss: 1.098627  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098639 \n",
      "\n",
      "loss: 1.098624  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098640 \n",
      "\n",
      "loss: 1.098638  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098614 \n",
      "\n",
      "loss: 1.098653  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098564 \n",
      "\n",
      "loss: 1.098521  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098640 \n",
      "\n",
      "loss: 1.098596  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098568 \n",
      "\n",
      "loss: 1.098541  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098637 \n",
      "\n",
      "loss: 1.098681  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098713 \n",
      "\n",
      "loss: 1.098963  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098532 \n",
      "\n",
      "loss: 1.098866  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098737 \n",
      "\n",
      "loss: 1.099177  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098589 \n",
      "\n",
      "loss: 1.098717  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098598 \n",
      "\n",
      "loss: 1.098554  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098616 \n",
      "\n",
      "loss: 1.098466  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098927 \n",
      "\n",
      "loss: 1.098047  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098600 \n",
      "\n",
      "loss: 1.098620  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098661 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [295, 12], 'alpha': 0.00036255124676536245, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.41731405044101977}\n",
      "loss: 1.089990  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.749856 \n",
      "\n",
      "loss: 0.774727  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.691309 \n",
      "\n",
      "loss: 0.696556  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.652352 \n",
      "\n",
      "loss: 0.663854  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.692137 \n",
      "\n",
      "loss: 0.682981  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.618342 \n",
      "\n",
      "loss: 0.647695  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.603807 \n",
      "\n",
      "loss: 0.585324  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.574757 \n",
      "\n",
      "loss: 0.546096  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.578914 \n",
      "\n",
      "loss: 0.590888  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.551473 \n",
      "\n",
      "loss: 0.527155  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.558873 \n",
      "\n",
      "loss: 0.577627  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.544998 \n",
      "\n",
      "loss: 0.597374  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.514640 \n",
      "\n",
      "loss: 0.529381  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.527978 \n",
      "\n",
      "loss: 0.513896  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.503953 \n",
      "\n",
      "loss: 0.538637  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.498626 \n",
      "\n",
      "loss: 0.474663  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.514932 \n",
      "\n",
      "loss: 0.507848  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.468766 \n",
      "\n",
      "loss: 0.486827  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.461648 \n",
      "\n",
      "loss: 0.455222  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.474567 \n",
      "\n",
      "loss: 0.452481  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.469435 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [290, 83, 228], 'alpha': 0.00043373812368277954, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.44950084622242076}\n",
      "loss: 1.104469  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 872.216778 \n",
      "\n",
      "loss: 815.783813  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 156.003314 \n",
      "\n",
      "loss: 154.167252  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 114.393135 \n",
      "\n",
      "loss: 98.858261  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 288.483598 \n",
      "\n",
      "loss: 300.065948  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 97.225297 \n",
      "\n",
      "loss: 98.929268  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 17.972413 \n",
      "\n",
      "loss: 15.702512  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 19.864755 \n",
      "\n",
      "loss: 21.599726  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 21.051544 \n",
      "\n",
      "loss: 24.613213  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 30.725998 \n",
      "\n",
      "loss: 28.231176  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 13.242406 \n",
      "\n",
      "loss: 11.734313  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 10.030298 \n",
      "\n",
      "loss: 10.209547  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 9.960145 \n",
      "\n",
      "loss: 10.615602  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 5.544414 \n",
      "\n",
      "loss: 5.815068  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 6.384235 \n",
      "\n",
      "loss: 5.912253  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 16.734949 \n",
      "\n",
      "loss: 15.741686  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 9.876636 \n",
      "\n",
      "loss: 8.163864  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 4.800062 \n",
      "\n",
      "loss: 4.784711  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 6.971005 \n",
      "\n",
      "loss: 8.531075  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 5.105868 \n",
      "\n",
      "loss: 5.087404  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 2.298659 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [468], 'alpha': 0.006152422634646263, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.1769039494438477}\n",
      "loss: 1.150621  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.761688 \n",
      "\n",
      "loss: 0.741238  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.735010 \n",
      "\n",
      "loss: 0.745557  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.716766 \n",
      "\n",
      "loss: 0.727962  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.699676 \n",
      "\n",
      "loss: 0.723886  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.717383 \n",
      "\n",
      "loss: 0.740358  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.705435 \n",
      "\n",
      "loss: 0.664191  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.706655 \n",
      "\n",
      "loss: 0.720670  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.707306 \n",
      "\n",
      "loss: 0.713563  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.668296 \n",
      "\n",
      "loss: 0.647867  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.682407 \n",
      "\n",
      "loss: 0.680828  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.672221 \n",
      "\n",
      "loss: 0.666400  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.664438 \n",
      "\n",
      "loss: 0.702534  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.666364 \n",
      "\n",
      "loss: 0.649943  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.648530 \n",
      "\n",
      "loss: 0.638893  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.660504 \n",
      "\n",
      "loss: 0.665730  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.667739 \n",
      "\n",
      "loss: 0.680421  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.660297 \n",
      "\n",
      "loss: 0.632225  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.648628 \n",
      "\n",
      "loss: 0.678441  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.626528 \n",
      "\n",
      "loss: 0.601662  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.646782 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [460, 455, 216], 'alpha': 7.399205907653401, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.43231638754036}\n",
      "loss: 1.095974  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 16.6%, Avg loss: 7443623647321703424.000000 \n",
      "\n",
      "loss: 7253110971554594816.000000  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 16.3%, Avg loss:      inf \n",
      "\n",
      "loss:   inf  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss:      nan \n",
      "\n",
      "loss:   nan  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss:      nan \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [459], 'alpha': 0.011520519192355904, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.39121500942420107}\n",
      "loss: 1.106882  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 8.473634 \n",
      "\n",
      "loss: 8.078879  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 4.050999 \n",
      "\n",
      "loss: 3.916604  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 4.298725 \n",
      "\n",
      "loss: 4.502933  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 5.114579 \n",
      "\n",
      "loss: 4.854368  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 9.798657 \n",
      "\n",
      "loss: 9.656698  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 5.463319 \n",
      "\n",
      "loss: 5.461026  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 3.844783 \n",
      "\n",
      "loss: 4.018431  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 5.067802 \n",
      "\n",
      "loss: 5.110171  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 5.207276 \n",
      "\n",
      "loss: 5.205713  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 16.040344 \n",
      "\n",
      "loss: 16.023422  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: 10.160363 \n",
      "\n",
      "loss: 10.278349  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 14.051926 \n",
      "\n",
      "loss: 13.542389  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 8.298872 \n",
      "\n",
      "loss: 8.228457  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 6.949670 \n",
      "\n",
      "loss: 7.543151  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 2.867176 \n",
      "\n",
      "loss: 2.924045  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 5.381190 \n",
      "\n",
      "loss: 5.602122  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 14.639297 \n",
      "\n",
      "loss: 14.858371  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 4.229578 \n",
      "\n",
      "loss: 4.312088  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 5.530513 \n",
      "\n",
      "loss: 5.213440  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 10.315517 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [418, 350], 'alpha': 2.090836831467693, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.07968300170606755}\n",
      "loss: 1.142029  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098622 \n",
      "\n",
      "loss: 1.098310  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098608 \n",
      "\n",
      "loss: 1.098562  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098766 \n",
      "\n",
      "loss: 1.098368  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098566 \n",
      "\n",
      "loss: 1.098548  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098548 \n",
      "\n",
      "loss: 1.098927  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098620 \n",
      "\n",
      "loss: 1.098642  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098654 \n",
      "\n",
      "loss: 1.099404  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098657 \n",
      "\n",
      "loss: 1.098612  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098630 \n",
      "\n",
      "loss: 1.098367  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098747 \n",
      "\n",
      "loss: 1.098400  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098687 \n",
      "\n",
      "loss: 1.098409  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098439  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098746 \n",
      "\n",
      "loss: 1.098734  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098887 \n",
      "\n",
      "loss: 1.098120  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098691 \n",
      "\n",
      "loss: 1.099748  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098605 \n",
      "\n",
      "loss: 1.098472  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098706 \n",
      "\n",
      "loss: 1.098585  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098524 \n",
      "\n",
      "loss: 1.099002  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098745 \n",
      "\n",
      "loss: 1.098282  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098608 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [63, 203, 454], 'alpha': 0.00015617914405316642, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.09986645772987338}\n",
      "loss: 1.128610  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.529749 \n",
      "\n",
      "loss: 1.544385  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.167023 \n",
      "\n",
      "loss: 1.175046  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.178950 \n",
      "\n",
      "loss: 1.180235  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.279913 \n",
      "\n",
      "loss: 1.273634  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.107287 \n",
      "\n",
      "loss: 1.106407  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.124115 \n",
      "\n",
      "loss: 1.131711  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.105307 \n",
      "\n",
      "loss: 1.102417  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.120400 \n",
      "\n",
      "loss: 1.113732  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.100523 \n",
      "\n",
      "loss: 1.093104  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098428 \n",
      "\n",
      "loss: 1.093379  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.112008 \n",
      "\n",
      "loss: 1.109224  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.7%, Avg loss: 1.091388 \n",
      "\n",
      "loss: 1.090523  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 1.089849 \n",
      "\n",
      "loss: 1.089169  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 1.084120 \n",
      "\n",
      "loss: 1.084729  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 42.4%, Avg loss: 1.080457 \n",
      "\n",
      "loss: 1.077607  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 1.069047 \n",
      "\n",
      "loss: 1.069761  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 40.4%, Avg loss: 1.087472 \n",
      "\n",
      "loss: 1.101256  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 46.3%, Avg loss: 1.039971 \n",
      "\n",
      "loss: 1.043400  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.991702 \n",
      "\n",
      "loss: 1.003875  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 0.967108 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [187, 164], 'alpha': 1.26672372543513, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.011929850907255615}\n",
      "loss: 1.135608  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 1.074971 \n",
      "\n",
      "loss: 1.076515  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 52.1%, Avg loss: 1.093655 \n",
      "\n",
      "loss: 1.093844  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.098009 \n",
      "\n",
      "loss: 1.097747  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 1.098539 \n",
      "\n",
      "loss: 1.098528  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098612 \n",
      "\n",
      "loss: 1.098644  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098624  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098616 \n",
      "\n",
      "loss: 1.098575  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098645 \n",
      "\n",
      "loss: 1.098543  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098612 \n",
      "\n",
      "loss: 1.098598  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098630  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098612 \n",
      "\n",
      "loss: 1.098610  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098619  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098634 \n",
      "\n",
      "loss: 1.098626  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098644  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098619 \n",
      "\n",
      "loss: 1.098600  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098616  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098614 \n",
      "\n",
      "loss: 1.098619  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098635 \n",
      "\n",
      "loss: 1.098627  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098614 \n",
      "\n",
      "loss: 1.098608  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098617 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [212], 'alpha': 0.004378342745277836, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.17563794992846782}\n",
      "loss: 1.138600  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.390964 \n",
      "\n",
      "loss: 1.301634  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.695542 \n",
      "\n",
      "loss: 0.713560  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.626271 \n",
      "\n",
      "loss: 0.592489  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.641661 \n",
      "\n",
      "loss: 0.668901  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.664843 \n",
      "\n",
      "loss: 0.644791  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.677931 \n",
      "\n",
      "loss: 0.634858  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.724265 \n",
      "\n",
      "loss: 0.721190  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.664592 \n",
      "\n",
      "loss: 0.690901  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.651814 \n",
      "\n",
      "loss: 0.645748  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.710823 \n",
      "\n",
      "loss: 0.756903  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.686140 \n",
      "\n",
      "loss: 0.674656  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.692705 \n",
      "\n",
      "loss: 0.683459  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.664694 \n",
      "\n",
      "loss: 0.647325  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.683204 \n",
      "\n",
      "loss: 0.680273  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.778378 \n",
      "\n",
      "loss: 0.770430  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.893285 \n",
      "\n",
      "loss: 0.865606  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 64.490964 \n",
      "\n",
      "loss: 61.709072  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 25.187554 \n",
      "\n",
      "loss: 22.878115  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 3.598249 \n",
      "\n",
      "loss: 3.603777  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.704041 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [212, 355, 510], 'alpha': 1.6531254668439463, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.05775934175052865}\n",
      "loss: 1.098464  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098609 \n",
      "\n",
      "loss: 1.098602  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098607 \n",
      "\n",
      "loss: 1.098643  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098685 \n",
      "\n",
      "loss: 1.098651  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098605 \n",
      "\n",
      "loss: 1.098628  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098639 \n",
      "\n",
      "loss: 1.098675  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098607 \n",
      "\n",
      "loss: 1.098645  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098617  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098635 \n",
      "\n",
      "loss: 1.098620  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098609  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098622 \n",
      "\n",
      "loss: 1.098616  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098612  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098610 \n",
      "\n",
      "loss: 1.098566  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098655 \n",
      "\n",
      "loss: 1.098650  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098604 \n",
      "\n",
      "loss: 1.098588  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098623 \n",
      "\n",
      "loss: 1.098628  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098626  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098608  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098649 \n",
      "\n",
      "loss: 1.098577  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098597  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098641 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [99, 305], 'alpha': 6.4963823658346636, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.05429043335263408}\n",
      "loss: 1.112441  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098609 \n",
      "\n",
      "loss: 1.098587  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098611 \n",
      "\n",
      "loss: 1.098619  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098605 \n",
      "\n",
      "loss: 1.098610  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098625 \n",
      "\n",
      "loss: 1.098608  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098622 \n",
      "\n",
      "loss: 1.098624  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098600 \n",
      "\n",
      "loss: 1.098649  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098607 \n",
      "\n",
      "loss: 1.098605  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098640 \n",
      "\n",
      "loss: 1.098624  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098598 \n",
      "\n",
      "loss: 1.098644  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098612  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098613 \n",
      "\n",
      "loss: 1.098622  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098609 \n",
      "\n",
      "loss: 1.098645  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098627 \n",
      "\n",
      "loss: 1.098570  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098623 \n",
      "\n",
      "loss: 1.098596  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098609  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098604 \n",
      "\n",
      "loss: 1.098637  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098606 \n",
      "\n",
      "loss: 1.098638  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098630 \n",
      "\n",
      "loss: 1.098650  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098625 \n",
      "\n",
      "loss: 1.098598  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098632 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [184, 381], 'alpha': 0.855344153120213, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.1926591971290291}\n",
      "loss: 1.160953  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.435608 \n",
      "\n",
      "loss: 4.547991  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 4.673715 \n",
      "\n",
      "loss: 4.375773  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 4.396115 \n",
      "\n",
      "loss: 4.382527  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 4.041130 \n",
      "\n",
      "loss: 4.127670  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 4.365222 \n",
      "\n",
      "loss: 4.338063  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 4.082091 \n",
      "\n",
      "loss: 4.097481  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.171735 \n",
      "\n",
      "loss: 4.119487  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 4.239920 \n",
      "\n",
      "loss: 4.283851  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 4.711717 \n",
      "\n",
      "loss: 4.614911  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 4.675186 \n",
      "\n",
      "loss: 4.642172  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 3.951899 \n",
      "\n",
      "loss: 3.856760  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 4.465182 \n",
      "\n",
      "loss: 4.506910  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 3.707929 \n",
      "\n",
      "loss: 3.681544  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 4.236133 \n",
      "\n",
      "loss: 4.076639  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 4.098959 \n",
      "\n",
      "loss: 4.350851  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 4.206864 \n",
      "\n",
      "loss: 4.155090  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 4.211773 \n",
      "\n",
      "loss: 4.401249  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 4.123549 \n",
      "\n",
      "loss: 3.965685  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 4.218578 \n",
      "\n",
      "loss: 4.082839  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 4.278553 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [105, 138, 30], 'alpha': 0.3498858821987227, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.18217431374345397}\n",
      "loss: 1.100830  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098636 \n",
      "\n",
      "loss: 1.098497  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098561  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098819 \n",
      "\n",
      "loss: 1.098669  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098675 \n",
      "\n",
      "loss: 1.098594  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098587 \n",
      "\n",
      "loss: 1.098557  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098703 \n",
      "\n",
      "loss: 1.098669  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098594 \n",
      "\n",
      "loss: 1.098383  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098766 \n",
      "\n",
      "loss: 1.098495  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098587 \n",
      "\n",
      "loss: 1.098706  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098773 \n",
      "\n",
      "loss: 1.098786  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098628 \n",
      "\n",
      "loss: 1.098862  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098652 \n",
      "\n",
      "loss: 1.098688  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098597 \n",
      "\n",
      "loss: 1.098588  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098660  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098759 \n",
      "\n",
      "loss: 1.098593  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098649 \n",
      "\n",
      "loss: 1.098444  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098630 \n",
      "\n",
      "loss: 1.098609  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098860 \n",
      "\n",
      "loss: 1.098660  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098595 \n",
      "\n",
      "loss: 1.098715  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098773 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [493], 'alpha': 0.20950212120905315, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.33172739938514556}\n",
      "loss: 1.161431  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 44.5%, Avg loss: 1.975265 \n",
      "\n",
      "loss: 2.026393  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.891267 \n",
      "\n",
      "loss: 0.899621  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.916694 \n",
      "\n",
      "loss: 0.890175  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.875438 \n",
      "\n",
      "loss: 0.893855  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.888154 \n",
      "\n",
      "loss: 0.888743  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.905908 \n",
      "\n",
      "loss: 0.882415  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.892804 \n",
      "\n",
      "loss: 0.888677  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: 0.883422 \n",
      "\n",
      "loss: 0.904372  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.879590 \n",
      "\n",
      "loss: 0.920279  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.884916 \n",
      "\n",
      "loss: 0.863797  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.911588 \n",
      "\n",
      "loss: 0.913523  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.896425 \n",
      "\n",
      "loss: 0.915847  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.904814 \n",
      "\n",
      "loss: 0.920144  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.878133 \n",
      "\n",
      "loss: 0.895134  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.910428 \n",
      "\n",
      "loss: 0.933523  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.907170 \n",
      "\n",
      "loss: 0.917156  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.879745 \n",
      "\n",
      "loss: 0.908679  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.890179 \n",
      "\n",
      "loss: 0.891816  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.889150 \n",
      "\n",
      "loss: 0.888166  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.933266 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [376], 'alpha': 3.163983248479132, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.1631588341382142}\n",
      "loss: 1.125920  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.9%, Avg loss: 1.095349 \n",
      "\n",
      "loss: 1.095356  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098580 \n",
      "\n",
      "loss: 1.098406  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098632 \n",
      "\n",
      "loss: 1.098448  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098688 \n",
      "\n",
      "loss: 1.098751  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098747 \n",
      "\n",
      "loss: 1.099160  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098686 \n",
      "\n",
      "loss: 1.098518  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098615 \n",
      "\n",
      "loss: 1.098636  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098754 \n",
      "\n",
      "loss: 1.098551  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098851 \n",
      "\n",
      "loss: 1.099722  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098659 \n",
      "\n",
      "loss: 1.098783  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098786 \n",
      "\n",
      "loss: 1.097952  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099108 \n",
      "\n",
      "loss: 1.100605  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098669 \n",
      "\n",
      "loss: 1.099183  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098593 \n",
      "\n",
      "loss: 1.098582  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098870 \n",
      "\n",
      "loss: 1.098292  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.099509 \n",
      "\n",
      "loss: 1.099027  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099818 \n",
      "\n",
      "loss: 1.099703  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.099343 \n",
      "\n",
      "loss: 1.099498  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 1.099592 \n",
      "\n",
      "loss: 1.099689  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.099984 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [159, 291, 73], 'alpha': 0.0009103333742719545, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.3077959064840951}\n",
      "loss: 1.131151  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.107769 \n",
      "\n",
      "loss: 1.111354  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.107947 \n",
      "\n",
      "loss: 1.107975  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.101299 \n",
      "\n",
      "loss: 1.100574  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.094649 \n",
      "\n",
      "loss: 1.093968  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.095065 \n",
      "\n",
      "loss: 1.092628  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 1.059496 \n",
      "\n",
      "loss: 1.062230  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 43.4%, Avg loss: 1.029267 \n",
      "\n",
      "loss: 1.024558  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.912367 \n",
      "\n",
      "loss: 0.921283  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.864366 \n",
      "\n",
      "loss: 0.842740  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.861012 \n",
      "\n",
      "loss: 0.876204  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.854568 \n",
      "\n",
      "loss: 0.845381  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.838370 \n",
      "\n",
      "loss: 0.825906  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.844129 \n",
      "\n",
      "loss: 0.835684  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.830754 \n",
      "\n",
      "loss: 0.860926  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.830124 \n",
      "\n",
      "loss: 0.815767  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.823757 \n",
      "\n",
      "loss: 0.816082  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.880892 \n",
      "\n",
      "loss: 0.883204  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.835930 \n",
      "\n",
      "loss: 0.815138  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.810148 \n",
      "\n",
      "loss: 0.810902  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.810478 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [215, 288], 'alpha': 1.4744357297415664, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.1051703241728327}\n",
      "loss: 1.105810  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098582 \n",
      "\n",
      "loss: 1.098583  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098807 \n",
      "\n",
      "loss: 1.099680  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098566 \n",
      "\n",
      "loss: 1.098766  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099238 \n",
      "\n",
      "loss: 1.099037  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098605 \n",
      "\n",
      "loss: 1.098747  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098624 \n",
      "\n",
      "loss: 1.098510  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098676 \n",
      "\n",
      "loss: 1.099001  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098805 \n",
      "\n",
      "loss: 1.099009  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098868  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098781 \n",
      "\n",
      "loss: 1.098550  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098628 \n",
      "\n",
      "loss: 1.099169  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098619 \n",
      "\n",
      "loss: 1.098390  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098387 \n",
      "\n",
      "loss: 1.097170  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098576 \n",
      "\n",
      "loss: 1.098573  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.099043 \n",
      "\n",
      "loss: 1.099162  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098644 \n",
      "\n",
      "loss: 1.098807  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.099574 \n",
      "\n",
      "loss: 1.098664  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098767 \n",
      "\n",
      "loss: 1.099177  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098805 \n",
      "\n",
      "loss: 1.098814  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.099001 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [318, 343, 172], 'alpha': 0.02113201699244151, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.2556358810376061}\n",
      "loss: 1.107398  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.104182 \n",
      "\n",
      "loss: 1.107819  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.161004 \n",
      "\n",
      "loss: 1.170053  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.107656 \n",
      "\n",
      "loss: 1.106238  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.108022 \n",
      "\n",
      "loss: 1.106653  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.108794 \n",
      "\n",
      "loss: 1.107406  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.122328 \n",
      "\n",
      "loss: 1.123064  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.163399 \n",
      "\n",
      "loss: 1.164556  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.100686 \n",
      "\n",
      "loss: 1.106527  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.161446 \n",
      "\n",
      "loss: 1.180241  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.106416 \n",
      "\n",
      "loss: 1.102462  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098853 \n",
      "\n",
      "loss: 1.099120  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.174302 \n",
      "\n",
      "loss: 1.184045  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.109699 \n",
      "\n",
      "loss: 1.102383  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.101887 \n",
      "\n",
      "loss: 1.104897  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.099335 \n",
      "\n",
      "loss: 1.100605  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.138378 \n",
      "\n",
      "loss: 1.124298  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.106852 \n",
      "\n",
      "loss: 1.111919  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098437 \n",
      "\n",
      "loss: 1.098826  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.170818 \n",
      "\n",
      "loss: 1.173159  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.105141 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [94, 345, 51], 'alpha': 0.00262380783729638, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.4092298899218785}\n",
      "loss: 1.101813  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.723604 \n",
      "\n",
      "loss: 0.698999  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.663478 \n",
      "\n",
      "loss: 0.647041  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.625869 \n",
      "\n",
      "loss: 0.624013  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.580829 \n",
      "\n",
      "loss: 0.534748  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.545399 \n",
      "\n",
      "loss: 0.563455  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.568722 \n",
      "\n",
      "loss: 0.581907  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.546390 \n",
      "\n",
      "loss: 0.568562  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.600940 \n",
      "\n",
      "loss: 0.573948  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.497135 \n",
      "\n",
      "loss: 0.509389  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.524743 \n",
      "\n",
      "loss: 0.505390  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.474052 \n",
      "\n",
      "loss: 0.437413  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.450883 \n",
      "\n",
      "loss: 0.468423  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.517478 \n",
      "\n",
      "loss: 0.522144  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.547844 \n",
      "\n",
      "loss: 0.565538  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.467410 \n",
      "\n",
      "loss: 0.479514  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.592008 \n",
      "\n",
      "loss: 0.603148  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.439498 \n",
      "\n",
      "loss: 0.393455  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.437321 \n",
      "\n",
      "loss: 0.423554  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.490708 \n",
      "\n",
      "loss: 0.476240  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.407799 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [352], 'alpha': 0.003565310198594198, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.012310251324640488}\n",
      "loss: 1.115579  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.869249 \n",
      "\n",
      "loss: 0.855721  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.826243 \n",
      "\n",
      "loss: 0.845421  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 0.809455 \n",
      "\n",
      "loss: 0.800524  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.791281 \n",
      "\n",
      "loss: 0.791575  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.794420 \n",
      "\n",
      "loss: 0.773750  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.788431 \n",
      "\n",
      "loss: 0.788952  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.775974 \n",
      "\n",
      "loss: 0.784084  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.771757 \n",
      "\n",
      "loss: 0.806053  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.760807 \n",
      "\n",
      "loss: 0.747361  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.769703 \n",
      "\n",
      "loss: 0.787773  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.765801 \n",
      "\n",
      "loss: 0.771483  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.757665 \n",
      "\n",
      "loss: 0.750112  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.754510 \n",
      "\n",
      "loss: 0.809838  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.745237 \n",
      "\n",
      "loss: 0.753343  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.755072 \n",
      "\n",
      "loss: 0.728140  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.752640 \n",
      "\n",
      "loss: 0.738860  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.744689 \n",
      "\n",
      "loss: 0.724234  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.743042 \n",
      "\n",
      "loss: 0.770181  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.734150 \n",
      "\n",
      "loss: 0.735012  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.744702 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [511], 'alpha': 0.023750990234495464, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.2072790613977599}\n",
      "loss: 1.101510  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 2.691475 \n",
      "\n",
      "loss: 2.653454  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.762643 \n",
      "\n",
      "loss: 0.718795  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.729603 \n",
      "\n",
      "loss: 0.736395  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.746186 \n",
      "\n",
      "loss: 0.722106  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.729248 \n",
      "\n",
      "loss: 0.739655  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.744408 \n",
      "\n",
      "loss: 0.728859  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.744330 \n",
      "\n",
      "loss: 0.738419  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.723194 \n",
      "\n",
      "loss: 0.730427  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.753009 \n",
      "\n",
      "loss: 0.772958  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.761907 \n",
      "\n",
      "loss: 0.781615  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.768175 \n",
      "\n",
      "loss: 0.779019  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.749260 \n",
      "\n",
      "loss: 0.741134  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.772996 \n",
      "\n",
      "loss: 0.792605  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 14.548313 \n",
      "\n",
      "loss: 14.827662  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 107.316562 \n",
      "\n",
      "loss: 109.789375  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 72.049932 \n",
      "\n",
      "loss: 71.195190  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 26.230900 \n",
      "\n",
      "loss: 25.119854  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 4.575181 \n",
      "\n",
      "loss: 4.720551  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.741905 \n",
      "\n",
      "loss: 0.711351  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.701482 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [302, 107, 375], 'alpha': 0.3032293622030006, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.12206782134526159}\n",
      "loss: 1.143683  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.344711 \n",
      "\n",
      "loss: 1.369137  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.450044 \n",
      "\n",
      "loss: 1.426012  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.270673 \n",
      "\n",
      "loss: 1.290912  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.805296 \n",
      "\n",
      "loss: 1.789584  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.021615 \n",
      "\n",
      "loss: 1.981458  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.177629 \n",
      "\n",
      "loss: 1.170961  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.313508 \n",
      "\n",
      "loss: 1.335158  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.575642 \n",
      "\n",
      "loss: 1.584508  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 3.631201 \n",
      "\n",
      "loss: 3.648154  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 2.496495 \n",
      "\n",
      "loss: 2.527381  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.205286 \n",
      "\n",
      "loss: 1.200033  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.132967 \n",
      "\n",
      "loss: 1.149131  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.225308 \n",
      "\n",
      "loss: 1.215342  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.194395 \n",
      "\n",
      "loss: 1.196986  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.284079 \n",
      "\n",
      "loss: 1.307777  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 2.072596 \n",
      "\n",
      "loss: 2.140580  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.279652 \n",
      "\n",
      "loss: 1.289366  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.777018 \n",
      "\n",
      "loss: 1.750945  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 7.424707 \n",
      "\n",
      "loss: 7.102547  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.464929 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [360, 10, 465], 'alpha': 0.5686203300525338, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.10949868429247848}\n",
      "loss: 1.094964  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.099115 \n",
      "\n",
      "loss: 1.097821  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098655 \n",
      "\n",
      "loss: 1.098794  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.099055 \n",
      "\n",
      "loss: 1.099915  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098939 \n",
      "\n",
      "loss: 1.098322  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.099079 \n",
      "\n",
      "loss: 1.099778  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098918 \n",
      "\n",
      "loss: 1.097525  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099366 \n",
      "\n",
      "loss: 1.098734  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.099160 \n",
      "\n",
      "loss: 1.099892  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.099707 \n",
      "\n",
      "loss: 1.100782  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098605 \n",
      "\n",
      "loss: 1.098471  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098803 \n",
      "\n",
      "loss: 1.099502  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099270 \n",
      "\n",
      "loss: 1.099248  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098378 \n",
      "\n",
      "loss: 1.099294  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098680 \n",
      "\n",
      "loss: 1.099060  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.099017 \n",
      "\n",
      "loss: 1.097617  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098943 \n",
      "\n",
      "loss: 1.098749  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098738 \n",
      "\n",
      "loss: 1.099046  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098878 \n",
      "\n",
      "loss: 1.101551  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098969 \n",
      "\n",
      "loss: 1.098654  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.100256 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [273, 245, 33], 'alpha': 0.06123912407005947, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.06675621886795874}\n",
      "loss: 1.112465  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.903994 \n",
      "\n",
      "loss: 0.912337  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.834155 \n",
      "\n",
      "loss: 0.830376  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.846640 \n",
      "\n",
      "loss: 0.857187  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.841600 \n",
      "\n",
      "loss: 0.857258  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 5.439184 \n",
      "\n",
      "loss: 5.884378  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.125105 \n",
      "\n",
      "loss: 1.122185  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.119256 \n",
      "\n",
      "loss: 1.123627  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.821111 \n",
      "\n",
      "loss: 0.817303  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.808842 \n",
      "\n",
      "loss: 0.846512  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.815210 \n",
      "\n",
      "loss: 0.795667  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.816766 \n",
      "\n",
      "loss: 0.800795  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.814772 \n",
      "\n",
      "loss: 0.816583  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.825355 \n",
      "\n",
      "loss: 0.829754  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.820942 \n",
      "\n",
      "loss: 0.834599  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.816128 \n",
      "\n",
      "loss: 0.822697  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.817697 \n",
      "\n",
      "loss: 0.839705  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.824361 \n",
      "\n",
      "loss: 0.824175  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.827075 \n",
      "\n",
      "loss: 0.818943  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.813251 \n",
      "\n",
      "loss: 0.801006  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.819507 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [349, 279], 'alpha': 8.698858477633713, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.30636489614860524}\n",
      "loss: 1.113865  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 1.098187 \n",
      "\n",
      "loss: 1.098087  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098609 \n",
      "\n",
      "loss: 1.098724  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098622  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098656 \n",
      "\n",
      "loss: 1.098607  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098620 \n",
      "\n",
      "loss: 1.098445  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098661 \n",
      "\n",
      "loss: 1.098429  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098644 \n",
      "\n",
      "loss: 1.098566  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098598 \n",
      "\n",
      "loss: 1.098614  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098669 \n",
      "\n",
      "loss: 1.099024  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098670 \n",
      "\n",
      "loss: 1.098836  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098628 \n",
      "\n",
      "loss: 1.098690  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098610 \n",
      "\n",
      "loss: 1.098661  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098577 \n",
      "\n",
      "loss: 1.098597  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098581 \n",
      "\n",
      "loss: 1.098875  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098566 \n",
      "\n",
      "loss: 1.098274  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098610 \n",
      "\n",
      "loss: 1.098639  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098639  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098749 \n",
      "\n",
      "loss: 1.098689  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098603 \n",
      "\n",
      "loss: 1.098719  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098591 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [464, 266, 226], 'alpha': 1.4218240258013333, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.37185335553557647}\n",
      "loss: 1.134160  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 4.662240 \n",
      "\n",
      "loss: 4.636354  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.502409 \n",
      "\n",
      "loss: 1.500972  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 3.778167 \n",
      "\n",
      "loss: 3.766222  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 3.447818 \n",
      "\n",
      "loss: 3.316404  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 3.704014 \n",
      "\n",
      "loss: 3.638209  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.720026 \n",
      "\n",
      "loss: 2.682418  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 2.246393 \n",
      "\n",
      "loss: 2.209625  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 3.631647 \n",
      "\n",
      "loss: 3.407164  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 3.923303 \n",
      "\n",
      "loss: 3.981169  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.805614 \n",
      "\n",
      "loss: 2.923044  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 3.709179 \n",
      "\n",
      "loss: 3.554944  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 2.499852 \n",
      "\n",
      "loss: 2.567729  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 3.251985 \n",
      "\n",
      "loss: 3.370564  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 3.797887 \n",
      "\n",
      "loss: 3.726609  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 4.979617 \n",
      "\n",
      "loss: 5.115814  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.990953 \n",
      "\n",
      "loss: 3.054337  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 3.512355 \n",
      "\n",
      "loss: 3.396856  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.579181 \n",
      "\n",
      "loss: 2.539474  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 4.133314 \n",
      "\n",
      "loss: 4.090403  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 3.189580 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [238, 140], 'alpha': 0.5739630470474949, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.3134115436471925}\n",
      "loss: 1.111860  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.6%, Avg loss: 45.821659 \n",
      "\n",
      "loss: 41.180248  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 29.0%, Avg loss: 38.020979 \n",
      "\n",
      "loss: 38.250771  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.061452 \n",
      "\n",
      "loss: 1.058292  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098622 \n",
      "\n",
      "loss: 1.098414  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098559 \n",
      "\n",
      "loss: 1.098803  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098640 \n",
      "\n",
      "loss: 1.098836  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098760 \n",
      "\n",
      "loss: 1.097993  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098804 \n",
      "\n",
      "loss: 1.098675  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098911 \n",
      "\n",
      "loss: 1.098501  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098602 \n",
      "\n",
      "loss: 1.098706  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098608 \n",
      "\n",
      "loss: 1.098720  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098666 \n",
      "\n",
      "loss: 1.098965  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098288 \n",
      "\n",
      "loss: 1.098779  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099224 \n",
      "\n",
      "loss: 1.099599  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098558 \n",
      "\n",
      "loss: 1.099018  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098601 \n",
      "\n",
      "loss: 1.098975  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098665 \n",
      "\n",
      "loss: 1.098626  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.100225 \n",
      "\n",
      "loss: 1.099528  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098774 \n",
      "\n",
      "loss: 1.098747  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098873 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [233, 47], 'alpha': 0.3695934900677929, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.16799018355699483}\n",
      "loss: 1.120642  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.021288 \n",
      "\n",
      "loss: 1.020591  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 1.077577 \n",
      "\n",
      "loss: 1.079885  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.100458 \n",
      "\n",
      "loss: 1.099756  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.099188 \n",
      "\n",
      "loss: 1.099760  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098794 \n",
      "\n",
      "loss: 1.098505  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.101673 \n",
      "\n",
      "loss: 1.103631  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099818 \n",
      "\n",
      "loss: 1.101538  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.099069 \n",
      "\n",
      "loss: 1.098333  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098586 \n",
      "\n",
      "loss: 1.098320  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098706 \n",
      "\n",
      "loss: 1.100168  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098626 \n",
      "\n",
      "loss: 1.098710  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099534 \n",
      "\n",
      "loss: 1.097458  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.098933 \n",
      "\n",
      "loss: 1.098477  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.099017 \n",
      "\n",
      "loss: 1.098841  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098803 \n",
      "\n",
      "loss: 1.099694  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.100055 \n",
      "\n",
      "loss: 1.101173  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098656 \n",
      "\n",
      "loss: 1.098764  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 1.100713 \n",
      "\n",
      "loss: 1.100088  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.102172 \n",
      "\n",
      "loss: 1.099947  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098714 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [49, 78, 85], 'alpha': 0.001433201977224923, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.03220800247186765}\n",
      "loss: 1.099342  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.947463 \n",
      "\n",
      "loss: 0.966550  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.865931 \n",
      "\n",
      "loss: 0.864761  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.836785 \n",
      "\n",
      "loss: 0.833680  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.809992 \n",
      "\n",
      "loss: 0.806905  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.807236 \n",
      "\n",
      "loss: 0.769375  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.794211 \n",
      "\n",
      "loss: 0.773501  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.778220 \n",
      "\n",
      "loss: 0.776086  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.765098 \n",
      "\n",
      "loss: 0.747117  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.750398 \n",
      "\n",
      "loss: 0.741788  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.750579 \n",
      "\n",
      "loss: 0.750324  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.742963 \n",
      "\n",
      "loss: 0.747684  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.732107 \n",
      "\n",
      "loss: 0.763412  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.723505 \n",
      "\n",
      "loss: 0.722971  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.710982 \n",
      "\n",
      "loss: 0.743511  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.711362 \n",
      "\n",
      "loss: 0.723495  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.708075 \n",
      "\n",
      "loss: 0.718163  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.698651 \n",
      "\n",
      "loss: 0.690974  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.691137 \n",
      "\n",
      "loss: 0.681431  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.679772 \n",
      "\n",
      "loss: 0.686182  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.679890 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [177, 285], 'alpha': 0.02136099688835174, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.41646229555330977}\n",
      "loss: 1.105375  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.742015 \n",
      "\n",
      "loss: 0.688281  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.665412 \n",
      "\n",
      "loss: 0.700501  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.650666 \n",
      "\n",
      "loss: 0.659172  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.623517 \n",
      "\n",
      "loss: 0.602106  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.655160 \n",
      "\n",
      "loss: 0.666020  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.641878 \n",
      "\n",
      "loss: 0.663473  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.623045 \n",
      "\n",
      "loss: 0.640800  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.628177 \n",
      "\n",
      "loss: 0.633582  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.646892 \n",
      "\n",
      "loss: 0.642734  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.615489 \n",
      "\n",
      "loss: 0.607056  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.626654 \n",
      "\n",
      "loss: 0.613667  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.622574 \n",
      "\n",
      "loss: 0.627736  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.624986 \n",
      "\n",
      "loss: 0.646191  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.610295 \n",
      "\n",
      "loss: 0.611004  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.623691 \n",
      "\n",
      "loss: 0.597798  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.633044 \n",
      "\n",
      "loss: 0.572619  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.612833 \n",
      "\n",
      "loss: 0.605373  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.648102 \n",
      "\n",
      "loss: 0.640898  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.676252 \n",
      "\n",
      "loss: 0.661025  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.624390 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [471, 402], 'alpha': 0.08325245919987297, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.2385421683054227}\n",
      "loss: 1.107444  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 49.087761 \n",
      "\n",
      "loss: 44.876457  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 39.711213 \n",
      "\n",
      "loss: 43.052647  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Avg loss: 51.310847 \n",
      "\n",
      "loss: 48.673904  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 334.065268 \n",
      "\n",
      "loss: 312.235748  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 3.821936 \n",
      "\n",
      "loss: 3.741156  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 44.5%, Avg loss: 149.338326 \n",
      "\n",
      "loss: 137.639923  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 50.9%, Avg loss: 2.251336 \n",
      "\n",
      "loss: 2.218525  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.790582 \n",
      "\n",
      "loss: 0.784855  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.723578 \n",
      "\n",
      "loss: 0.702289  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.749561 \n",
      "\n",
      "loss: 0.731268  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.698879 \n",
      "\n",
      "loss: 0.689047  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.728245 \n",
      "\n",
      "loss: 0.737669  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.739165 \n",
      "\n",
      "loss: 0.737956  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.699114 \n",
      "\n",
      "loss: 0.746711  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.739078 \n",
      "\n",
      "loss: 0.723887  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.726293 \n",
      "\n",
      "loss: 0.708784  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.718448 \n",
      "\n",
      "loss: 0.716482  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.725556 \n",
      "\n",
      "loss: 0.734095  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.743142 \n",
      "\n",
      "loss: 0.729515  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.743213 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [160, 393], 'alpha': 0.008911139457051997, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.2441659158273702}\n",
      "loss: 1.149857  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.181757 \n",
      "\n",
      "loss: 1.191866  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 1.096315 \n",
      "\n",
      "loss: 1.095126  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 0.989088 \n",
      "\n",
      "loss: 1.003747  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 0.939777 \n",
      "\n",
      "loss: 0.921081  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 0.937755 \n",
      "\n",
      "loss: 0.944581  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 0.949367 \n",
      "\n",
      "loss: 0.966171  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 43.3%, Avg loss: 1.036808 \n",
      "\n",
      "loss: 1.015841  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 0.944210 \n",
      "\n",
      "loss: 0.952486  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.836457 \n",
      "\n",
      "loss: 0.806177  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.888433 \n",
      "\n",
      "loss: 0.859072  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.917460 \n",
      "\n",
      "loss: 0.907550  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.915769 \n",
      "\n",
      "loss: 0.949353  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.845070 \n",
      "\n",
      "loss: 0.862535  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.965701 \n",
      "\n",
      "loss: 0.927106  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.905833 \n",
      "\n",
      "loss: 0.938673  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.878762 \n",
      "\n",
      "loss: 0.882622  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.999186 \n",
      "\n",
      "loss: 0.999743  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 0.899689 \n",
      "\n",
      "loss: 0.886820  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.879178 \n",
      "\n",
      "loss: 0.853543  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 0.924255 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [231, 256], 'alpha': 0.00013967674306039884, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.36211827067588165}\n",
      "loss: 1.103909  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 152.522737 \n",
      "\n",
      "loss: 145.772339  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 15.444453 \n",
      "\n",
      "loss: 16.936430  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 16.850333 \n",
      "\n",
      "loss: 16.445230  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 10.061621 \n",
      "\n",
      "loss: 12.564665  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 2.412060 \n",
      "\n",
      "loss: 2.820518  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 15.828134 \n",
      "\n",
      "loss: 15.391372  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 1.177315 \n",
      "\n",
      "loss: 1.241950  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.944894 \n",
      "\n",
      "loss: 0.961873  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.856190 \n",
      "\n",
      "loss: 0.819386  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.713842 \n",
      "\n",
      "loss: 0.681198  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.680958 \n",
      "\n",
      "loss: 0.615858  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.555019 \n",
      "\n",
      "loss: 0.533017  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.617170 \n",
      "\n",
      "loss: 0.604062  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.512559 \n",
      "\n",
      "loss: 0.432376  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.733814 \n",
      "\n",
      "loss: 0.623856  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.581234 \n",
      "\n",
      "loss: 0.439092  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.369737 \n",
      "\n",
      "loss: 0.389209  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.410793 \n",
      "\n",
      "loss: 0.436913  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.390030 \n",
      "\n",
      "loss: 0.351739  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.396223 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [478, 73], 'alpha': 8.009582657649434, 'activition': 'ReLU', 'optimizer': 'Adam', 'lr': 0.29598719853639266}\n",
      "loss: 1.114169  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098714 \n",
      "\n",
      "loss: 1.098319  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098609 \n",
      "\n",
      "loss: 1.098712  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098614 \n",
      "\n",
      "loss: 1.098653  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098692 \n",
      "\n",
      "loss: 1.098679  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098582 \n",
      "\n",
      "loss: 1.098529  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098624 \n",
      "\n",
      "loss: 1.098596  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098621 \n",
      "\n",
      "loss: 1.098920  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098610 \n",
      "\n",
      "loss: 1.098602  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098617 \n",
      "\n",
      "loss: 1.098782  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098662 \n",
      "\n",
      "loss: 1.098761  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098630 \n",
      "\n",
      "loss: 1.098476  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.098680 \n",
      "\n",
      "loss: 1.099231  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098602 \n",
      "\n",
      "loss: 1.098468  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098567 \n",
      "\n",
      "loss: 1.098670  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098608 \n",
      "\n",
      "loss: 1.098773  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098626 \n",
      "\n",
      "loss: 1.098205  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098607 \n",
      "\n",
      "loss: 1.098544  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098490 \n",
      "\n",
      "loss: 1.098236  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 1.098585 \n",
      "\n",
      "loss: 1.099643  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098905 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [21, 50, 337], 'alpha': 0.013901569917285509, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.02523728828096491}\n",
      "loss: 1.167423  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.099190 \n",
      "\n",
      "loss: 1.102075  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.099434 \n",
      "\n",
      "loss: 1.098438  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.097946 \n",
      "\n",
      "loss: 1.098397  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.098851 \n",
      "\n",
      "loss: 1.097474  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098272 \n",
      "\n",
      "loss: 1.098152  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098298 \n",
      "\n",
      "loss: 1.099194  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.104035 \n",
      "\n",
      "loss: 1.108483  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 1.098353 \n",
      "\n",
      "loss: 1.096825  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.0%, Avg loss: 1.100705 \n",
      "\n",
      "loss: 1.099004  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098595 \n",
      "\n",
      "loss: 1.098992  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.099107 \n",
      "\n",
      "loss: 1.098794  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.6%, Avg loss: 1.098837 \n",
      "\n",
      "loss: 1.098852  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098684 \n",
      "\n",
      "loss: 1.098379  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098209 \n",
      "\n",
      "loss: 1.097997  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 1.098721 \n",
      "\n",
      "loss: 1.099145  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098880 \n",
      "\n",
      "loss: 1.098297  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 1.098187 \n",
      "\n",
      "loss: 1.098277  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 42.5%, Avg loss: 1.098385 \n",
      "\n",
      "loss: 1.098527  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.098262 \n",
      "\n",
      "loss: 1.098337  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 32.7%, Avg loss: 1.098605 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [175, 116], 'alpha': 0.01877139830999145, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.13920048215783912}\n",
      "loss: 1.092377  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.792584 \n",
      "\n",
      "loss: 0.755938  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.976068 \n",
      "\n",
      "loss: 0.992355  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.767622 \n",
      "\n",
      "loss: 0.770431  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.949158 \n",
      "\n",
      "loss: 0.938459  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 2.570074 \n",
      "\n",
      "loss: 2.565253  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Avg loss: 1.606304 \n",
      "\n",
      "loss: 1.664068  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 46.3%, Avg loss: 3.782203 \n",
      "\n",
      "loss: 3.801085  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 4.134410 \n",
      "\n",
      "loss: 4.333878  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 30.8%, Avg loss: 22.698734 \n",
      "\n",
      "loss: 24.265720  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.809669 \n",
      "\n",
      "loss: 0.763912  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 39.0%, Avg loss: 3.011073 \n",
      "\n",
      "loss: 2.932477  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.918198 \n",
      "\n",
      "loss: 0.885195  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.762982 \n",
      "\n",
      "loss: 0.763608  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 1.730018 \n",
      "\n",
      "loss: 1.631125  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 2.345908 \n",
      "\n",
      "loss: 2.290895  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 1.239911 \n",
      "\n",
      "loss: 1.311750  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.266051 \n",
      "\n",
      "loss: 1.314705  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 8.974618 \n",
      "\n",
      "loss: 8.883739  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 8.703900 \n",
      "\n",
      "loss: 8.768147  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 17.384170 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [137, 435], 'alpha': 0.00025676224472019756, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.4999539234717537}\n",
      "loss: 1.088576  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.670791 \n",
      "\n",
      "loss: 0.659307  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.641867 \n",
      "\n",
      "loss: 0.651557  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.552767 \n",
      "\n",
      "loss: 0.544292  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.542312 \n",
      "\n",
      "loss: 0.521180  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.544982 \n",
      "\n",
      "loss: 0.531808  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.528755 \n",
      "\n",
      "loss: 0.533177  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.625545 \n",
      "\n",
      "loss: 0.654067  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.453229 \n",
      "\n",
      "loss: 0.468152  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.432609 \n",
      "\n",
      "loss: 0.444343  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.502307 \n",
      "\n",
      "loss: 0.524869  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.412528 \n",
      "\n",
      "loss: 0.421844  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.418636 \n",
      "\n",
      "loss: 0.421181  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.369953 \n",
      "\n",
      "loss: 0.389369  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.460211 \n",
      "\n",
      "loss: 0.507298  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.374652 \n",
      "\n",
      "loss: 0.346678  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.415298 \n",
      "\n",
      "loss: 0.403858  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.344292 \n",
      "\n",
      "loss: 0.342794  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.743508 \n",
      "\n",
      "loss: 0.628659  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.313133 \n",
      "\n",
      "loss: 0.314585  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.308022 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [122, 340], 'alpha': 0.00022033924261045178, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.47481090518472285}\n",
      "loss: 1.113467  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.728307 \n",
      "\n",
      "loss: 0.741765  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.596995 \n",
      "\n",
      "loss: 0.611959  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.575747 \n",
      "\n",
      "loss: 0.561278  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.558644 \n",
      "\n",
      "loss: 0.578042  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.508743 \n",
      "\n",
      "loss: 0.509556  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.515958 \n",
      "\n",
      "loss: 0.474545  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.521582 \n",
      "\n",
      "loss: 0.513328  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.483632 \n",
      "\n",
      "loss: 0.504755  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.498220 \n",
      "\n",
      "loss: 0.495838  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.439820 \n",
      "\n",
      "loss: 0.483290  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.496199 \n",
      "\n",
      "loss: 0.513728  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.405205 \n",
      "\n",
      "loss: 0.428592  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.399577 \n",
      "\n",
      "loss: 0.400042  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.380971 \n",
      "\n",
      "loss: 0.361406  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.391197 \n",
      "\n",
      "loss: 0.382565  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.380796 \n",
      "\n",
      "loss: 0.413462  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.380417 \n",
      "\n",
      "loss: 0.394383  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.326175 \n",
      "\n",
      "loss: 0.316233  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.336761 \n",
      "\n",
      "loss: 0.318027  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.351556 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [437], 'alpha': 0.00012644454168301897, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.0631978653396157}\n",
      "loss: 1.165431  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.792589 \n",
      "\n",
      "loss: 0.790618  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.758625 \n",
      "\n",
      "loss: 0.764227  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.743574 \n",
      "\n",
      "loss: 0.744509  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.723767 \n",
      "\n",
      "loss: 0.733708  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.727328 \n",
      "\n",
      "loss: 0.730669  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.719296 \n",
      "\n",
      "loss: 0.706934  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.707780 \n",
      "\n",
      "loss: 0.716812  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.700923 \n",
      "\n",
      "loss: 0.717960  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.688075 \n",
      "\n",
      "loss: 0.725958  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.690245 \n",
      "\n",
      "loss: 0.701332  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.688073 \n",
      "\n",
      "loss: 0.689345  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.678312 \n",
      "\n",
      "loss: 0.639896  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.672633 \n",
      "\n",
      "loss: 0.667634  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.664109 \n",
      "\n",
      "loss: 0.633886  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.663305 \n",
      "\n",
      "loss: 0.614668  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.663886 \n",
      "\n",
      "loss: 0.657027  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.655339 \n",
      "\n",
      "loss: 0.616865  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.648422 \n",
      "\n",
      "loss: 0.620610  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.641532 \n",
      "\n",
      "loss: 0.630007  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.640330 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [66, 296, 493], 'alpha': 0.001229766630975591, 'activition': 'LeakyReLU', 'optimizer': 'Adam', 'lr': 0.08397779276956435}\n",
      "loss: 1.109373  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 1.184412 \n",
      "\n",
      "loss: 1.216118  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.774797 \n",
      "\n",
      "loss: 0.780978  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.759096 \n",
      "\n",
      "loss: 0.776267  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.687532 \n",
      "\n",
      "loss: 0.731526  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.671421 \n",
      "\n",
      "loss: 0.664671  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.637698 \n",
      "\n",
      "loss: 0.670181  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.615141 \n",
      "\n",
      "loss: 0.597556  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.588156 \n",
      "\n",
      "loss: 0.554604  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.562743 \n",
      "\n",
      "loss: 0.522568  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.564653 \n",
      "\n",
      "loss: 0.515376  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.561604 \n",
      "\n",
      "loss: 0.531693  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.548268 \n",
      "\n",
      "loss: 0.545578  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.529915 \n",
      "\n",
      "loss: 0.507511  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.513570 \n",
      "\n",
      "loss: 0.483134  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.519066 \n",
      "\n",
      "loss: 0.488427  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.490761 \n",
      "\n",
      "loss: 0.486157  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.471429 \n",
      "\n",
      "loss: 0.495222  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.458144 \n",
      "\n",
      "loss: 0.458411  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.479090 \n",
      "\n",
      "loss: 0.444503  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.447949 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [501, 390, 375], 'alpha': 0.00037382156979471714, 'activition': 'Tanh', 'optimizer': 'Adam', 'lr': 0.05413856495383753}\n",
      "loss: 1.101633  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.965490 \n",
      "\n",
      "loss: 0.966254  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.880894 \n",
      "\n",
      "loss: 0.781553  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.783498 \n",
      "\n",
      "loss: 0.767130  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.679007 \n",
      "\n",
      "loss: 0.705516  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.685595 \n",
      "\n",
      "loss: 0.673742  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.654232 \n",
      "\n",
      "loss: 0.632972  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.844812 \n",
      "\n",
      "loss: 0.793529  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.738899 \n",
      "\n",
      "loss: 0.707879  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.030561 \n",
      "\n",
      "loss: 1.013341  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.699041 \n",
      "\n",
      "loss: 0.671076  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.724484 \n",
      "\n",
      "loss: 0.736519  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.611625 \n",
      "\n",
      "loss: 0.556255  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.771547 \n",
      "\n",
      "loss: 0.750758  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.777875 \n",
      "\n",
      "loss: 0.790028  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.650200 \n",
      "\n",
      "loss: 0.664396  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.746958 \n",
      "\n",
      "loss: 0.774587  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.799958 \n",
      "\n",
      "loss: 0.741717  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.813070 \n",
      "\n",
      "loss: 0.766369  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.778635 \n",
      "\n",
      "loss: 0.789543  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.707153 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [483], 'alpha': 0.0036289937644712017, 'activition': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.4707411522108007}\n",
      "loss: 1.120307  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 4.053180 \n",
      "\n",
      "loss: 3.873049  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 4.332131 \n",
      "\n",
      "loss: 4.183843  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 4.925288 \n",
      "\n",
      "loss: 4.984393  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 6.436033 \n",
      "\n",
      "loss: 6.655790  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 37.2%, Avg loss: 5.641656 \n",
      "\n",
      "loss: 5.863642  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 39.6%, Avg loss: 9.238642 \n",
      "\n",
      "loss: 9.346972  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 4.734699 \n",
      "\n",
      "loss: 4.720479  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 4.634117 \n",
      "\n",
      "loss: 4.513202  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 5.847422 \n",
      "\n",
      "loss: 5.992840  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 5.438513 \n",
      "\n",
      "loss: 5.439520  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 4.741283 \n",
      "\n",
      "loss: 4.787715  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 6.938299 \n",
      "\n",
      "loss: 6.764464  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 6.918794 \n",
      "\n",
      "loss: 6.988511  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 4.839942 \n",
      "\n",
      "loss: 5.075693  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 43.4%, Avg loss: 11.697501 \n",
      "\n",
      "loss: 11.472285  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 3.809923 \n",
      "\n",
      "loss: 3.742742  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 10.382726 \n",
      "\n",
      "loss: 9.871565  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 4.821651 \n",
      "\n",
      "loss: 4.549525  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 39.3%, Avg loss: 3.605012 \n",
      "\n",
      "loss: 3.630932  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 5.344692 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [46], 'alpha': 0.09130389530107734, 'activition': 'Tanh', 'optimizer': 'SGD', 'lr': 0.3486267819476365}\n",
      "loss: 1.103469  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.814800 \n",
      "\n",
      "loss: 0.789555  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.799257 \n",
      "\n",
      "loss: 0.794665  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.796738 \n",
      "\n",
      "loss: 0.804088  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.790876 \n",
      "\n",
      "loss: 0.781058  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.799546 \n",
      "\n",
      "loss: 0.810667  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.802913 \n",
      "\n",
      "loss: 0.792457  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.793651 \n",
      "\n",
      "loss: 0.798254  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.797929 \n",
      "\n",
      "loss: 0.810771  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.794967 \n",
      "\n",
      "loss: 0.805740  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.799173 \n",
      "\n",
      "loss: 0.802321  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.801331 \n",
      "\n",
      "loss: 0.774339  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.796653 \n",
      "\n",
      "loss: 0.780825  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.796777 \n",
      "\n",
      "loss: 0.828725  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.791200 \n",
      "\n",
      "loss: 0.775231  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.803698 \n",
      "\n",
      "loss: 0.809304  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.801448 \n",
      "\n",
      "loss: 0.790128  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.797795 \n",
      "\n",
      "loss: 0.808549  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.795628 \n",
      "\n",
      "loss: 0.777624  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.790083 \n",
      "\n",
      "loss: 0.777768  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.799155 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [443, 358], 'alpha': 0.10047501370071465, 'activition': 'ReLU', 'optimizer': 'SGD', 'lr': 0.17366716227769363}\n",
      "loss: 1.103012  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.869154 \n",
      "\n",
      "loss: 0.856847  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.855524 \n",
      "\n",
      "loss: 0.857765  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.856044 \n",
      "\n",
      "loss: 0.865134  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.849478 \n",
      "\n",
      "loss: 0.861806  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.852125 \n",
      "\n",
      "loss: 0.833113  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.855648 \n",
      "\n",
      "loss: 0.843548  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.848699 \n",
      "\n",
      "loss: 0.849413  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.851882 \n",
      "\n",
      "loss: 0.862953  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.848493 \n",
      "\n",
      "loss: 0.861901  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.850850 \n",
      "\n",
      "loss: 0.815915  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.854124 \n",
      "\n",
      "loss: 0.845518  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.846583 \n",
      "\n",
      "loss: 0.841531  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.851209 \n",
      "\n",
      "loss: 0.865221  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.848336 \n",
      "\n",
      "loss: 0.849028  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.849945 \n",
      "\n",
      "loss: 0.862117  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.852295 \n",
      "\n",
      "loss: 0.850247  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 0.847646 \n",
      "\n",
      "loss: 0.844547  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.851761 \n",
      "\n",
      "loss: 0.859664  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.844141 \n",
      "\n",
      "loss: 0.849082  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.852297 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [346, 197], 'alpha': 0.00022119926654584084, 'activition': 'LeakyReLU', 'optimizer': 'SGD', 'lr': 0.4248028559370887}\n",
      "loss: 1.115899  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.711596 \n",
      "\n",
      "loss: 0.668609  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.611485 \n",
      "\n",
      "loss: 0.588565  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.569233 \n",
      "\n",
      "loss: 0.611086  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.528257 \n",
      "\n",
      "loss: 0.531300  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.515488 \n",
      "\n",
      "loss: 0.528432  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.478175 \n",
      "\n",
      "loss: 0.474655  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.576901 \n",
      "\n",
      "loss: 0.561291  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.435538 \n",
      "\n",
      "loss: 0.438603  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.461398 \n",
      "\n",
      "loss: 0.451213  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.409117 \n",
      "\n",
      "loss: 0.384892  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.438602 \n",
      "\n",
      "loss: 0.422266  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.455633 \n",
      "\n",
      "loss: 0.466057  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.377073 \n",
      "\n",
      "loss: 0.384980  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.357916 \n",
      "\n",
      "loss: 0.373144  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.430352 \n",
      "\n",
      "loss: 0.426509  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.402818 \n",
      "\n",
      "loss: 0.409137  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.361417 \n",
      "\n",
      "loss: 0.357184  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.318386 \n",
      "\n",
      "loss: 0.307610  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.297330 \n",
      "\n",
      "loss: 0.281242  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.308674 \n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "{'hl': [62, 229], 'alpha': 6.367043779645354, 'activition': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.21780002378933774}\n",
      "loss: 1.110788  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 8.017573 \n",
      "\n",
      "loss: 7.885864  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 8.713110 \n",
      "\n",
      "loss: 8.708373  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 7.941734 \n",
      "\n",
      "loss: 7.602213  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 8.019643 \n",
      "\n",
      "loss: 8.380331  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 8.056742 \n",
      "\n",
      "loss: 8.462216  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 8.294110 \n",
      "\n",
      "loss: 8.487379  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 8.093991 \n",
      "\n",
      "loss: 8.106357  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 8.280574 \n",
      "\n",
      "loss: 8.047204  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 8.370403 \n",
      "\n",
      "loss: 8.354364  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 8.032455 \n",
      "\n",
      "loss: 8.541882  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 8.471947 \n",
      "\n",
      "loss: 8.320797  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 8.330726 \n",
      "\n",
      "loss: 7.962720  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 8.478794 \n",
      "\n",
      "loss: 7.835433  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 8.402229 \n",
      "\n",
      "loss: 8.279370  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 8.718636 \n",
      "\n",
      "loss: 8.966988  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 8.551804 \n",
      "\n",
      "loss: 8.590426  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.1%, Avg loss: 7.702832 \n",
      "\n",
      "loss: 7.484894  [ 1024/52670]\n",
      "Test Error: \n",
      " Accuracy: 33.5%, Avg loss: 8.359818 \n",
      "\n",
      "loss: 8.076406  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 8.784668 \n",
      "\n",
      "loss: 8.794286  [ 1024/52671]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 8.079502 \n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "random.seed(random_state)\n",
    "\n",
    "input_size=X_test_selected.shape[-1]\n",
    "output_size=3\n",
    "epochs = 20\n",
    "num_search = 100\n",
    "\n",
    "accuracy_list = []\n",
    "hp_list = []\n",
    "pm_list = []\n",
    "\n",
    "for search in range(num_search):\n",
    "    print('-'*20)\n",
    "    hp = get_hps()\n",
    "    print(hp)\n",
    "    model_rs = DNN_rs(input_size=input_size, hidden_sizes=hp['hl'], output_size=output_size, activition_layer=hp['activition']).to(device)\n",
    "    \n",
    "    if hp['optimizer'] == 'SGD':\n",
    "        optimizer_rs = torch.optim.SGD(model_rs.parameters(), lr=hp['lr'], weight_decay=hp['alpha'])\n",
    "    elif hp['optimizer'] == 'Adam':\n",
    "        optimizer_rs = torch.optim.Adam(model_rs.parameters(), lr=hp['lr'], weight_decay=hp['alpha'])\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        break\n",
    "        \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        train_dataloader = train_dataloader_list[i%5]\n",
    "        validation_dataloader = val_dataloader_list[i%5]\n",
    "        model_rs.train()\n",
    "        train(train_dataloader, model_rs, loss_fn, optimizer_rs)\n",
    "        test(validation_dataloader, model_rs, loss_fn)\n",
    "        \n",
    "        if (i+1)%5==0:\n",
    "            model_rs.eval()\n",
    "            pred = model_rs(val_dataloader_k.dataset[:][0]).detach().cpu().max(axis=1).indices.numpy()\n",
    "            true = val_dataloader_k.dataset[:][1].cpu().numpy()\n",
    "            accuracy_list.append(accuracy_score(true, pred))\n",
    "            hp.update({'epoch': i+1})\n",
    "            hp_list.append(hp.copy())\n",
    "            pm_list.append(model_rs.state_dict())\n",
    "    print('-'*20)\n",
    "    \n",
    "accuracy_list = np.array(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8387189844200807\n",
      "F1 score: 0.8387189844200807\n",
      "Recall score: 0.8387189844200807\n",
      "Precision score: 0.8387189844200807\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_idx = np.argmax(accuracy_list)\n",
    "hp = hp_list[best_idx]\n",
    "model_best = DNN_rs(input_size=input_size, hidden_sizes=hp['hl'], output_size=output_size, activition_layer=hp['activition']).to(device)\n",
    "model_best.load_state_dict(pm_list[best_idx])\n",
    "\n",
    "model_best.eval()\n",
    "pred = model_best(test_dataloader.dataset[:][0]).detach().cpu().max(axis=1).indices.numpy()\n",
    "true = test_dataloader.dataset[:][1].cpu().numpy()\n",
    "\n",
    "performance_eval(true, pred, matrix_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hl': [137, 435],\n",
       " 'alpha': 0.00025676224472019756,\n",
       " 'activition': 'ReLU',\n",
       " 'optimizer': 'SGD',\n",
       " 'lr': 0.4999539234717537,\n",
       " 'epoch': 20}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
